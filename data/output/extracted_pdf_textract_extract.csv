Sentence,Source
"b'.\ns\nr\ne\nh\ns\ni\nl\nb\nu\np\n\nd\ne\ni\nl\nl\na\n\ns\nt\ni\n\nf\no\n\ne\nn\no\n\nr\no\n\nn\no\ni\nt\na\ni\nc\no\ns\ns\nA\n\nl\na\nc\ni\ng\no\nl\no\nh\nc\ny\ns\nP\n\nn\na\nc\ni\nr\ne\nm\nA\ne\nh\nt\n\ny\nb\n\nd\ne\nt\nh\ng\ni\nr\ny\np\no\nc\n\ns\ni\n\nt\nn\ne\nm\nu\nc\no\nd\n\ns\ni\nh\nT\n\n.\n\nn\no\ni\nt\na\ni\nc\no\ns\ns\nA\n\nl\na\nc\ni\ng\no\nl\no\nh\nc\ny\ns\nP\n\nn\na\nc\ni\nr\ne\nm\nA\ne\nh\nt\n\nh\ng\nu\no\nr\nh\nt\n\no\ng\n\nt\ns\nu\nm\ne\nl\no\nh\nw\n\nr\no\n\nt\nr\na\np\n\nn\ni\n\nt\nn\ne\nt\nn\no\nc\n\ns\ni\nh\nt\n\ne\ns\nu\ne\nr\n\no\nt\n\ns\nt\ns\ne\nu\nq\ne\nr\n\ny\nn\na\n\nt\nu\nb\n\n,\nt\ns\no\nc\n\no\nn\n\nt\na\n\nd\ne\nr\na\nh\ns\n\ne\nb\n\ny\na\nm\n\nt\nn\ne\nt\nn\no\nC\n\n\xc2\xa9 2022 American Psychological Association\nISSN: 1064-1297\n\n2022, Vol. 30, No. 4, 379\xe2\x80\x93380\nhttps://doi.org/10.1037/pha0000582\n\nExperimental and Clinical Psychopharmacology\n\nINTRODUCTION\n\nCrowdsourcing Methods in Addiction Science:\n\nEmerging Research and Best Practices\n\nJustin C. Strickland1, Michael Amlung2, 3, and Derek D. Reed2, 3\n\n1 Department of Psychiatry and Behavioral Sciences, Johns Hopkins University School of Medicine\n\n2 Department of Applied Behavioral Science, University of Kansas\n\n3 Cofrin Logan Center for Addiction Research and Treatment, Lawrence, Kansas, United States\n\nCrowdsourcing platforms such as Amazon Mechanical Turk, Proli\xef\xac\x81c, and Qualtrics Panels have become a\ndominant form of sampling in recent years. Crowdsourcing enables researchers to effectively and\nef\xef\xac\x81ciently sample research participants with greater geographic variability, access to hard-to-reach\npopulations, and reduced costs. These methods have been increasingly used across varied areas of\npsychological science and essential for research during the COVID-19 pandemic due to their facilitation\nof remote research. Recent work documents methods for improving data quality, emerging crowdsourcing\nplatforms, and how crowdsourcing data \xef\xac\x81t within broader research programs. Addiction scientists will bene\xef\xac\x81t\nfrom the adoption of best practice guidelines in crowdsourcing as well as developing novel approaches,\nvenues, and applications to advance the \xef\xac\x81eld.\n\nPublic Health Signi\xef\xac\x81cance\nThe following set of articles in this special issue describes best practice methods and novel applications\nof crowdsourcing in addiction and psychological science. These articles advance the \xef\xac\x81eld and present\npractical guidelines and open-source resources for researchers using crowdsourcing in future work.\n\nKeywords: crowdsourcing, methods, mTurk, Proli\xef\xac\x81c, validity\n\nWith the changing landscape of work amidst the global COVID-19\npandemic and increasing costs associated with collecting data from\nlarge participant samples, researchers are turning to alternative meth-\nods for recruiting participants and collecting data. Accordingly,\ncrowdsourcing platforms such as Amazon Mechanical Turk, Proli\xef\xac\x81c,\nand Qualtrics Panels have become a dominant form of sampling\nin recent years (Strickland & Stoops, 2019). These crowdsourcing\nplatforms enable researchers to continue to collect data from large\nsamples of human participants when face-to-face visits are challeng-\ning, cost-prohibitive, or infeasible due to other barriers (e.g., research\nsites at logistically prohibitive distances; social distancing require-\nments during COVID-19). Alongside optimism about the practical\nbene\xef\xac\x81ts that crowdsourcing may provide are uncertainties about the\nvalidity of these approaches and how they can (and cannot) be used.\nThis special issue includes a collection of articles on best practices and\nemerging research using crowdsourcing for addictions research.\n\nArticles included in this special issue emphasize that data quality\nand control methods are critical in crowdsourcing platforms to\nensure data are reliable and valid. Jones et al. (2022) summarize\nthe importance of this work using a meta-analysis of careless\nresponding in crowdsourced alcohol use research and \xef\xac\x81nd that\napproximately 12% of participants are classi\xef\xac\x81ed as careless respon-\nders. They also provide practical recommendations to address these\nissues to include the use of both overt and covert \xef\xac\x81delity measures.\nBelliveau and Yakovenko (2022) complement these \xef\xac\x81ndings by\nproviding a practical implementation guide with step-by-step in-\nstructions for screening for speeding, straight-lining (i.e., tendency\nto make the same response in a group of questions), inconsistent\nresponding, nonsensical responding, and missing data. Open-source\ncode for conducting these procedures is provided for those looking\nto adopt these methods in their own work. Conceptually related\nbehavioral economic research shows the practical implications of\n\nhttps://orcid.org/0000-0003-1077-0394\n\nJustin C. Strickland\nJustin C. Strickland\xe2\x80\x99s contribution was partially supported by National\nInstitute on Drug Abuse (NIDA; Grant R03DA054098). Michael Amlung\xe2\x80\x99s\ncontribution was partially supported by National Institute on Alcohol Abuse\nand Alcoholism (NIAAA; Grant R01AA027255) and the Cofrin Logan Center\nfor Addiction Research and Treatment at the University of Kansas. The authors\nhave no \xef\xac\x81nancial con\xef\xac\x82icts of interest in regard to this editorial introduction.\n\nJustin C. Strickland played lead role in writing of original draft and equal\nrole in conceptualization. Michael Amlung played equal role in conceptuali-\nzation and writing of review and editing. Derek D. Reed played equal role in\nconceptualization and writing of review and editing.\n\nCorrespondence concerning this article should be addressed to Justin C.\nStrickland, Department of Psychiatry and Behavioral Sciences, Johns\nHopkins University School of Medicine, 5510 Nathan Shock Drive,\nBaltimore, MD 21224, United States. Email: jstric14@jhmi.edu\n\n379\n\n\x0c380\n\nSTRICKLAND, AMLUNG, AND REED\n\nthese quality checks. Craft et al. (2022) \xef\xac\x81nd that delay-discounting\ndata from participants failing systematicity checks did not differ from\nrandomly generated data highlighting the importance of screening\nand removing data with a priori validity checks. Freitas-Lemos et al.\n(2022) describe how a novel check based on instructional under-\nstanding differentiated participants on consistency of cigarette use\nreporting, responding on a cigarette demand task, and the relationship\nbetween use behavior and demand data.\n\nAs the methods to screen for quality data have improved, so have\nthe venues in which crowdsourcing has been applied. Historically,\nAmazon Mechanical Turk was the primary crowdsourcing outlet for\npsychological and addiction science research. Belliveau et al. (2022)\ndescribe the use of the more recently developed Qualtrics Panels\nresource to study behavioral addictions (video gaming and gaming\ndisorder). They \xef\xac\x81nd that Qualtrics data offered a participant pool\nsimilar in demographics to a community-recruited population sup-\nporting feasibility and potential usefulness of the resource. Stanton\net al. (2022) show how another novel platform, Proli\xef\xac\x81c, can facilitate\nrepeated measures data in two protocols: a 5-day daily diary protocol\nand a test\xe2\x80\x93retest protocol. They explain across these two independent\nstudies how Proli\xef\xac\x81c-recruited participants provided valid data\nconsistent with theoretical expectations and afforded ef\xef\xac\x81cient\ncollection of longitudinal outcomes. Beyond online platforms like\nAmazon Mechanical Turk, Qualtrics Panels, and Proli\xef\xac\x81c, Pennington\net al. (2022) describe how big team science may ef\xef\xac\x81ciently crowd-\nsource researchers with the goal of producing reproducible projects\nconducted across varied institutions. A review of existing work using\nbig team, crowdsourced approaches (e.g., ManyLabs, Psychological\nScience Accelerator) as well as a novel approach used by the study\nteam are described.\n\nHow crowdsourcing \xef\xac\x81ts within a broader research program is\nultimately varied and may include pilot projects, methods develop-\nment, intervention deployment, and more. Rzeszutek et al. (2022)\nprovide one example of how crowdsourcing may be used to evaluate\nnovel behavioral outcomes by studying cross-drug withdrawal\neffects for cigarette and opioids. They use a behavioral economic\nframework to show that opioid withdrawal may increase cigarette\nvaluation, thereby providing a pathway for future treatment devel-\nopment work to build upon. Borodovsky (2022) integrates the above\nwork to describe differences between generalizability and represen-\ntativeness. A clear and concise review of these concepts is provided\nalong with how such differences may inform the boundary condi-\ntions under which internet-based research may (or may not) advance\nthe literature.\n\nGiven the rapid emergence and evolution of crowdsourcing\nresearch platforms, it is safe to assume this is a method that is here to\nstay. This special issue highlights best practices for using crowdsour-\ncing in addiction science while also drawing attention to important\nmethodological and conceptual limitations. We hope that addiction\n\nscientists continue to adhere to these guidelines while pushing the\nboundaries of possible work within crowdsourcing science.\n\nReferences\n\nBelliveau, J., Soucy, K. I., & Yakovenko, I. (2022). The validity of qualtrics\npanel data for research on video gaming and gaming disorder. Experi-\nmental and Clinical Psychopharmacology, 30(4), 424\xe2\x80\x93431. https://\ndoi.org/10.1037/pha0000575\n\nBelliveau, J., & Yakovenko, I. (2022). Evaluating and improving the quality\nof survey data from panel and crowd-sourced samples: A practical guide\nfor psychological research. Experimental and Clinical Psychopharmacol-\nogy, 30(4), 400\xe2\x80\x93408. https://doi.org/10.1037/pha0000564\n\nBorodovsky, J. T. (2022). Generalizability and representativeness: Consid-\nerations for internet-based research on substance use behaviors. Experi-\nmental and Clinical Psychopharmacology, 30(4), 466\xe2\x80\x93477. https://\ndoi.org/10.1037/pha0000581\n\nCraft, W. H., Tegge, A. N., Freitas-Lemos, R., Tomlinson, D. C., & Bickel,\nW. K. (2022). Are poor quality data just random responses?: A crowd-\nsourced study of delay discounting in alcohol use disorder. Experimental\nand Clinical Psychopharmacology, 30(4), 409\xe2\x80\x93414. https://doi.org/10\n.1037/pha0000549\n\nFreitas-Lemos, R., Tegge, A. N., Craft, W. H., Tomlinson, D. C., Stein, J. S., &\nBickel, W. K. (2022). Understanding data quality: Instructional comprehen-\nsion as a practical metric in crowdsourced investigations of behavioral\neconomic cigarette demand. Experimental and Clinical Psychopharmacol-\nogy, 30(4), 415\xe2\x80\x93423. https://doi.org/10.1037/pha0000579\n\nJones, A., Earnest, J., Adam, M., Clarke, R., Yates, J., & Pennington, C. R.\n(2022). Careless responding in crowdsourced alcohol research: A system-\natic review and meta-analysis of practices and prevalence. Experimental\nand Clinical Psychopharmacology, 30(4), 381\xe2\x80\x93399. https://doi.org/10\n.1037/pha0000546\n\nPennington, C. R., Jones, A. J., Tzavella, L., Chambers, C. D., & Button,\nK. S. (2022). Beyond online participant crowdsourcing: The bene\xef\xac\x81ts\nand opportunities of big team addiction science. Experimental and Clini-\ncal Psychopharmacology,\nhttps://doi.org/10.10\n37/pha0000541\n\n444\xe2\x80\x93451.\n\n30(4),\n\nRzeszutek, M. J., Gipson-Reichardt, C. D., Kaplan, B. A., & Koffarnus,\nM. N. (2022). Using crowdsourcing to study the differential effects of\ncross-drug withdrawal for cigarettes and opioids in a behavioral economic\ndemand framework. Experimental and Clinical Psychopharmacology,\n30(4), 452\xe2\x80\x93465. https://doi.org/10.1037/pha0000558\n\nStanton, K., Carpenter, R. W., Nance, M., Sturgeon, T., & Villalongo\nAndino, M. (2022). A multisample demonstration of using the proli\xef\xac\x81c\nplatform for repeated assessment and psychometric substance use research.\nExperimental and Clinical Psychopharmacology, 30(4), 432\xe2\x80\x93443. https://\ndoi.org/10.1037/pha0000545\n\nStrickland, J. C., & Stoops, W. W. (2019). The use of crowdsourcing in\naddiction science research: Amazon Mechanical Turk. Experimental and\nClinical Psychopharmacology, 27(1), 1\xe2\x80\x9318. https://doi.org/10.1037/pha\n0000235\n\nReceived May 10, 2022\nAccepted May 10, 2022 \xe2\x96\xaa\n\n.\ns\nr\ne\nh\ns\ni\nl\nb\nu\np\n\nd\ne\ni\nl\nl\na\n\ns\nt\ni\n\nf\no\n\ne\nn\no\n\nr\no\n\nn\no\ni\nt\na\ni\nc\no\ns\ns\nA\n\nl\na\nc\ni\ng\no\nl\no\nh\nc\ny\ns\nP\n\nn\na\nc\ni\nr\ne\nm\nA\ne\nh\nt\n\ny\nb\n\nd\ne\nt\nh\ng\ni\nr\ny\np\no\nc\n\ns\ni\n\nt\nn\ne\nm\nu\nc\no\nd\n\ns\ni\nh\nT\n\n.\n\nn\no\ni\nt\na\ni\nc\no\ns\ns\nA\n\nl\na\nc\ni\ng\no\nl\no\nh\nc\ny\ns\nP\n\nn\na\nc\ni\nr\ne\nm\nA\ne\nh\nt\n\nh\ng\nu\no\nr\nh\nt\n\no\ng\n\nt\ns\nu\nm\ne\nl\no\nh\nw\n\nr\no\n\nt\nr\na\np\n\nn\ni\n\nt\nn\ne\nt\nn\no\nc\n\ns\ni\nh\nt\n\ne\ns\nu\ne\nr\n\no\nt\n\ns\nt\ns\ne\nu\nq\ne\nr\n\ny\nn\na\n\nt\nu\nb\n\n,\nt\ns\no\nc\n\no\nn\n\nt\na\n\nd\ne\nr\na\nh\ns\n\ne\nb\n\ny\na\nm\n\nt\nn\ne\nt\nn\no\nC\n\n\x0c'",ContentServer.asp.pdf
"b""1 \n\nThe Benefits, Barriers, and Risks of Big Team Science \n\nAuthors: Patrick S. Forscher1,2, Eric-Jan Wagenmakers3, Nicholas A. Coles4,  \n\nMiguel Alejandro Silan5,6, Nat\xc3\xa1lia Dutra7, Dana Basnight-Brown8, & Hans IJzerman2,9 \n\n \nAffiliations: \n1Busara Center for Behavioral Economics \n2Universit\xc3\xa9 Grenoble Alpes \n3University of Amsterdam \n4Stanford University \n5Universit\xc3\xa9 Lumi\xc3\xa8re Lyon 2 \n6Annecy Behavioral Science Lab \n7Universidade Federal do Par\xc3\xa1 \n8United States International University-Africa \n9Institut Universitaire de France \n \nNotes: PSF and HIJ wrote the first draft; all authors provided critical comments and revisions. \nConflict of Interest Statement: PSF, NAC, MAS, ND, DB, and HIJ are members of the \nPsychological Science Accelerator, a network of psychology research labs that conducts big \nteam science projects.  \n \nAbstract: Progress in psychology has been frustrated by challenges concerning replicability, \ngeneralizability, strategy selection, inferential reproducibility, and computational reproducibility. \nAlthough often discussed separately, these five challenges may share a common cause: \ninsufficient investment of intellectual and non-intellectual resources into the typical psychology \nstudy. We suggest that the emerging emphasis on big team science can help address these \nchallenges by allowing researchers to pool their resources together to increase the amount \navailable for a single study. However, the current incentives, infrastructure, and institutions in \nacademic science have all developed under the assumption that science is conducted by solo \nPrincipal Investigators and their dependent trainees, an assumption that creates barriers to \nsustainable big team science. We also anticipate that big team science carries unique risks, such \nas the potential for big team science organizations to be co-opted by unaccountable leaders, \nbecome overly conservative, and make mistakes at a grand scale. Big team science organizations \nmust also acquire personnel who are properly compensated and have clear roles, raising risks \nrelated to mismanagement and a lack of financial sustainability. If researchers can manage its \nunique barriers and risks, big team science has the potential to spur great progress in psychology \nand beyond. \n\n \n\n \n\n \n\n\x0c2 \n\nDespite decades of investment, progress in psychology has been slower than many \n\nscholars would like (Fanelli, 2010; Meehl, 1978; Newell, 1973). The lack of progress is \nempirically supported by the results of replication studies. Whether with the same population, \nsetting, and materials (the replicability challenge; Klein et al., 2014; Open Science \nCollaboration, 2015) or after a change to one or more of these features (the generalizability \nchallenge; Henrich et al., 2010; Tiokhin et al., 2019; Yarkoni, 2019), replicated results often \ndiffer meaningfully from original results. Meaningful differences also occur in other forms of \nreplication, such as: when separate teams develop research strategies to address the same \nresearch question (the strategy selection challenge; Landy et al., 2020) when separate teams \ndevelop analysis plans for the same dataset (the inferential reproducibility challenge; Botvinik-\nNezer et al., 2019; Silberzahn et al., 2018), and even when separate teams write code to execute \nthe same analysis (the computational reproducibility challenge; Donoho et al., 2008; Hardwicke \net al., 2018; Obels et al., 2019).  \n\nThese five challenges have complex proximal causes. Yet they may share a common \n\nultimate cause: insufficient resource investment in the typical psychology study. Insofar as this \npremise is true, a particular method of collaboration, big team science, may help address this \nultimate cause by efficiently scaling up the resources that can be invested in any given study. We \ndefine big team science as a method involving a relatively large number of collaborators who \nmay be dispersed across labs, institutions, disciplines, cultures, and continents. We contrast this \nsystem of science with small team science, which is usually organized around a single Principal \nInvestigator and their dependent trainees. If the unique risks and challenges of big team science \nare properly understood and managed, this method of collaboration may have great potential to \nimprove the efficiency and information value of psychological science. \n\nThe challenges in psychology share a common cause \n\nDiscussion of the slow progress in psychology has a long history. Scholars have argued \npersuasively that each of the five challenges facing psychology has a pernicious and potentially \ndestructive influence on scientific progress (Donoho et al., 2008; Landy et al., 2020; Meehl, \n1978; Sears, 1986; Silberzahn et al., 2018). These scholars have also presented a dizzying array \nof remedies for these challenges, ranging from increasing study sample sizes (Button et al., \n2013), which ought to improve replicability, to implementing version control (Vuorre & Curley, \n2018), which ought to improve computational reproducibility. Although varied, these remedies \nshare a common feature: they ask researchers to incur additional costs to improve a particular \naspect of study rigor (Finkel et al., 2017; LeBel et al., 2017; see Table 1 for a list of proposed \nremedies and their attendant costs). \n\nBecause the individual researcher incurs costs for each remedy, discussions of problems \n\nin psychology tend to assume that implementations of these remedies are zero-sum in the \nfollowing sense. The scientific resources that are available to a given researcher, such as time, \nlabor, and money, are limited. This means that, as long as a solution to improve, say, replicability \ndoes not also improve all the other aspects of study rigor, resources devoted to implementing that \nsolution to replicability must necessarily take away resources that could be spent on solutions to \nother aspects of rigor. Replicability, generalizability, inferential reproducibility, strategy \nselection, and computational reproducibility thus become qualities that must be traded off each \n\n \n\n\x0c3 \n\nother because devoting resources to improve one of these five qualities means sacrificing \nresources that could be invested in improving the others. \n\nChallenge \n\nSuggested remedy \n\nCosts of implementation \n\nIncrease sample size \n\nLabor and money for extra recruitment \n\nWho bears costs? \n\nResearchers; \nparticipants \n\nReplicability \n\nPreregister analysis plans \n\nLabor creating preregistration and making it \naccessible on a platform \n\nResearchers;  \nplatform maintainers \n\nImprove documentation of \nmaterials \n\nLabor creating and archiving documentation \nmaterials \n\nResearchers;  \nplatform maintainers \n\nUse special platforms to recruit \nculturally different participants \n\nLabor developing, maintaining, and learning to \nuse new platforms; money using and \nmaintaining the platforms \n\nResearchers; \nplatform maintainers \n\nGeneralizability \n\nCollaborate with colleagues from \ndifferent settings & backgrounds \n\nLabor developing relationships with new \ncolleagues \n\nImprove generality of materials \n\nLabor and money developing and validating \nimproved stimuli and measures \n\nExamine many construct \noperationalizations \n\nLabor and money examining different \noperationalizations \n\nResearchers \n\nResearchers; \nparticipants \n\nResearchers; \nparticipants \n\nStrategy selection \n\nConsensus design \n\nMachine-readable hypothesis tests \n\nInferential \nreproducibility \n\nMany analyst design \n\nLabor developing relationships with relevant \nexperts; labor implementing consensus process \n\nResearchers \n\nLabor learning and implementing machine-\nreadable systems; money developing and \nmaintaining platforms \n\nLabor making connections with relevant \nanalysts \n\nResearchers;  \nplatform maintainers \n\nResearchers \n\nExamine a multiverse of tests \n\nLabor developing relationships with relevant \nexperts; labor implementing multiverse analysis \n\nResearchers \n\nShare code and data \n\nLabor to put code and data into a shareable \nform; money developing and maintaining \nsharing platforms \n\nResearchers;  \nplatform maintainers \n\nImplement code checking \n\nLabor and money implementing code checking  Researches; journals \n\nLabor making code readable and hygienic \n\nResearchers \n\nUse capsules, version control, and \nmarkdown \n\nLabor learning and implementing new \nworkflows; money maintaining capsule and \nversion control platforms \n\nResearchers;  \nplatform maintainers \n\nComputational \nreproducibility \n\nImprove documentation of \ncodebases \n\nTable 1. Five challenges in psychology and some proposed remedies for these challenges. The challenges all ask \nresearchers to bear extra costs to improve some aspect of study rigor.  \n \n\n \n\n \n\n\x0c4 \n\nWe suggest that the views that prioritize one aspect of rigor over the others all share an \nimportant, mistaken assumption: the pool of resources available for investment in a given study \nis fixed. If the resources devoted to a single study are not fixed, these resources may be raised to \na level that is high enough that no tradeoffs are necessary among the various types of research \nrigor. Instead, a researcher need merely select as many of the rigor-enhancing strategies from \nTable 1 as they like and invest the money, human resources, and skills required to deploy those \nstrategies. \n\nThus, although they are often discussed separately, all five of psychology\xe2\x80\x99s major \n\nproblems may share a common cause: under-investment of resources, whether those resources \nare money, person-hours, or specialized expertise, in psychology studies (see also Cuccolo et al., \n2021; Uhlmann et al., 2019). Instead of asking which aspects of study rigor ought to be \nprioritized, reframing psychology\xe2\x80\x99s problems from this perspective allows us to ask a different \nquestion: how can we increase the resources invested in psychology studies and ascertain that the \nextra investment is used efficiently? \n\nIncreasing resource investment through big team science organizations \n\nIncreasing the per-study resource investment across an entire research ecosystem faces a \n\ncollective action problem: as long as scientists are rewarded for publishing more studies, any \nsingle scientist who decides to invest more resources in fewer studies will be outcompeted by \nscientists who invest less resources in more studies (Smaldino & McElreath, 2016). Any attempt \nto increase the per-study resource investment of the entire ecosystem must adopt one of two \nsolutions: (1) directly change the institutional incentives that prioritize quantity of publication \nand/or (2) devise new institutions, which we call big team science organizations, that allow blocs \nof scientists to increase the resource investments in concert. \n\nDirect change in the institutions of science is difficult because science itself has a \n\ndecentralized structure \xe2\x80\x93 changing its reward structures requires buy-in from many independent \nactors. This limits the effectiveness of initiatives to directly change the incentives that prioritize \nquantity of publication. A similar problem afflicts the establishment of new organizations that \ncoordinate the efforts of scientists to jointly invest in single projects. Fortunately, new \ninformation and communication technologies, such as the Google suite of collaboration tools, the \nOpen Science Framework, Slack, and Zoom, have made the establishment of such organizations \nmore feasible because they permit rapid, low-cost communication across far-flung countries and \ncircumstances (Teasley & Wolinsky, 2001). Such communication lays the groundwork for new \ninstitutions that can change how scientific actors spend resources on their science (Spellman, \n2015). \n\nThe function of an organization is to coordinate the activities of many actors \n\nsimultaneously. An organization solves the collective action problem faced by individual actors \nby allowing multiple actors to pre-commit to a coordinated, simultaneous course of action. In the \ncontext of science, a big team science organization allows many scientists to pre-commit to \ninvesting their limited scientific resources into one big project in exchange for an individual \nreward \xe2\x80\x93 usually, authorship on a publication, though other rewards are possible, such as money \nor networking opportunities. During the project itself, the organization coordinates the actions of \n\n \n\n\x0c5 \n\nthe scientists so that they do not work at cross-purposes. These functions allow the per-study \ninvestments from big team science projects to eclipse what is achievable in small team science. \n\nWe believe that the primary and most definitional function of a big team science \n\norganization is to allow larger investments of material and intellectual resources in a given \nproject. However, once established, big team science organizations have the potential to provide \nbenefits to both individual scientists and the broader scientific ecosystem that are external to this \nprimary function. As an example of one such positive externality, big team science organizations \ncentralize many aspects of project administration in one organization rather than forcing \nindividual labs to complete these functions on their own. This not only produces an efficiency for \nthe scientific ecosystem, but also allows researchers at organizations without strong research \ninfrastructure, such as instructors at teaching-focused universities (Wagge, Baciu, et al., 2019; \nWagge, Brandt, et al., 2019) to do more research than would otherwise be possible. Big teams \nalso allow individual researchers to specialize into roles that match their skills rather than \noccupying all roles simultaneously, producing an efficiency for the scientific ecosystem and \ngiving the individual researcher the opportunity to develop a specialized skill and thereby \ncommand a higher salary. Researchers who join a big team science organization also gain access \nto an expansive community. For the individual researcher, this reduces intellectual isolation and \nopens the door to professional opportunities; for the scientific ecosystem, this can generate new \ncollaborations that would not have been possible absent the expansive community. Finally, if big \nteam science organizations become robust enough, they can become political actors of their own \nright within the broader scientific ecosystem. Thus, they can nudge the incentives and values of \nthe entire scientific ecosystem by, for example, prioritizing the qualities of scientists to improve \nteam productivity rather than individual productivity (Tiokhin et al., 2021). \n\nThus, big team science organizations can invest more resources into individual projects \n\nthan is possible in small teams. Big team science organizations also have the potential to provide \na variety of benefits to individual researchers and the broader scientific ecosystem that are \nexternal to this primary function. Whether these benefits materialize depend on establishing the \norganizations in the first place and ensuring the organizations do not fall prey to some of the \nunique risks and challenges of this style of science. \n\nBig team science outside of psychology \n\nSome disciplines have already adopted a big team science model, so they can provide \n\nuseful examples of how big team science organizations can arise. These examples can also \nillustrate whether big team science can address the limitation of under-resourced studies. In the \n1990s, behavioral genetics generally featured small studies examining the relationship between \nvariation in a single candidate gene and a complex behavior or trait. In one prominent example, \nonly 52 patients provided genetic material for an analysis of the relationship between the 5-\nHTTLPR polymorphism and major depression (Heils et al., 1996), a finding that spurred \nenormous interest in the biological mechanisms through which these genes might cause \ndepression (Dannlowski et al., 2008; Gotlib et al., 2008). Unfortunately, and similar to the \ncurrent situation in psychology, these early results were contradicted by failed replication studies \n(Gillespie et al., 2005). As more and more replication studies contradicted the earlier optimistic \nones, researchers in behavioral genetics realized that changes were in order (Rieckmann, 2009).  \n\n \n\n\x0c6 \n\nThe two decades that followed were a turning point for behavioral genetics because they \ngave rise to big team science in the form of large research consortia. Without pooling resources, \nresearchers would have been unable to attain the scale of data necessary to advance our \nunderstanding of behavioral genetics. Before these consortia could attain such large-scale data, \nthey needed to invest considerable research, time, and funds devising methodological workflows \n(Corvin et al., 2010), designing infrastructure to manage and harmonize datasets (Sullivan, \n2010), and developing processes that increased the accuracy of the measurement of genotypes \nand phenotypes (23andMe Research Team et al., 2018; Corvin et al., 2010). With these solutions \nin hand, the research consortia served as a conduit through which multiple labs pooled resources \nto achieve large, heterogeneous samples that would otherwise have been out of reach for any \nsingle lab working independently. The UK Biobank is a prominent example, hosting a repository \nfor data from over 500,000 participants, a sample that dwarfs the sample of 52 used for the first \n5-HTTLPR study. The resulting findings have revolutionized our understanding of genes and \nbehavior: we now know that, instead of single genetic variants exerting large influences, a large \nnumber of variants have small influences (23andMe Research Team et al., 2018). These impacts \nwere subtle enough that they could not be observed and studied using the small, homogeneous \nsamples typically employed in the early 1990s. Innovations in methods, workflows, and \nmeasurement allowed for a sea change in our understanding of the relationship between genes \nand behavior. \n\nParticle physics provides another, still larger-scale, example. The 1950s saw a growing \n\nrecognition among particle physicists that further advances would require a scale of research and \nequipment that would strain the budgets of entire nations, let alone single laboratories (Krige, \n2004). For their part, Western governments saw cooperative large-scale investments in physics \nas a potential diplomatic tool; the projects served as a highly visible counterweight growing \nSoviet power, demonstrating the benefits that could accrue from an internationalist, democratic, \nand cooperative world order (Krige, 2004). These twin recognitions spurred the establishment of \n\xe2\x80\x9cmega-collaborations\xe2\x80\x9d: large, international collaborations spanning multiple countries. Such \nmega-collaborations could only realize their aims through both technical and social \nachievements to establish the infrastructure, documentation, and workflows necessary to \ncoordinate the efforts of huge numbers of scientists (Bakker, 1955; Brumfiel, 2011; for a modern \nworkflow, see Espinosa et al., 2020). One of the pre-eminent examples of these mega-\ncollaborations is the European Organization for Nuclear Research (CERN), which, together with \nBrookhaven National Laboratory, helped usher in a new era of particle physics: high-energy \nphysics, in which large teams of scientists harness vast resources to probe the most fundamental \nconstituents of matter (Bryant, 1994). \n\n \n\n \n\n \n\n \n\n \n\n \n\n\x0c7 \n\n \n\nName \n\nType \n\nDescription \n\nReproducibility Project: Psychology \n(Open Science Collaboration, 2015) \n\nAd-hoc \n\nA large team-science project in psychology, which involved 270 \ncontributing authors, who replicated 100 effects.  \n\nManyLabs (Ebersole et al., 2016; Klein et \nal., 2014, 2018, 2019) \n\nAd-hoc \n\nThese projects involve dozens of researchers, each of whom collects \ndata using their own local resources. The individual datasets are pooled \nto create a large common dataset.  \n\nMany Smiles Collaboration (Coles et \nal., 2019) \n\nAd-hoc \n\nA collaborative project designed to find the best way to test the \nhypothesis that facial expressions influence emotions. The test was \ndeveloped through consensus design. The resulting design uses multiple \noperationalizations and will be tested in a multi-site study. \n\nResearch contests to reduce implicit \nrace preferences (Lai et al., 2014, 2016) \n\nAd-hoc \n\nA series of contests to develop the most effective interventions to \nreduce implicit race preferences. Separate teams submitted \ninterventions; all interventions were run together on a large online \nplatform. \n\nRegistered Replication Reports (Simons \net al., 2014) \n\nStanding \n\nAn initiative to conduct multi-lab, preregistered close replications of \nprevious studies. The initiative supervised by a hosting journal. \nOriginally initiated at Perspectives at Psychological Science, other \njournals have adopted similar initiatives. Exact policies differ across \njournals. \n\nCollaborative Replication and \nEducation Project (Wagge, Baciu, et al., \n2019; Wagge, Brandt, et al., 2019) \n\nStanding \n\nA framework for engaging undergraduates in replication research. On \nconsultation with student advisors, the CREP team selects target effects \nfor replication. CREP develops a set of templates that guide the \nreplication process and uses a team of reviewers to ensure that the \nmethods for each individual lab are true to the template. \n\nPsychological Science Accelerator \n(Jones et al., 2021; Moshontz et al., \n2018) \n\nManyBabies (Byers-Heinlein et al., \n2020) \n\nStanding \n\nA standing, democratically structured network of labs focused on \nimproving the national diversity of psychology samples. \n\nStanding \n\nA standing network that conducts multi-site infancy research. \n\n \nTable 2. Eight big team science initiatives in psychology. \n \nBig team science in psychology \n\nPsychology has also started to witness an increase in big team science projects. Although \n\nmost of these projects are recent, they have already had an outsized impact on the field (see \nTable 2 for details). However, these efforts have revealed three categories of obstacles that must \nbe overcome if big team science is to maintain a sustained presence in the research landscape. \nThese obstacles are: incentivizing labor within the collaboration; developing and maintaining \ninfrastructure to coordinate team science activities; and dealing with institutions designed \naround research conducted by smaller teams. \n\n \n\n \n\n \n\n\x0cBarrier \n\nDescription \n\nSolutions \n\nIncentives \n\nAcademic prestige goes \ndisproportionately to the first-listed \nauthor on publications \n\nConsortium authorship \n\nContributorship systems, like CRediT \n\n8 \n\nInfrastructure \n\nCoordinating team science requires \ndedicated infrastructure \n\nGoogle Suite of tools for general online collaboration \n\nOpen Science Framework for sharing materials & data \n\nZoom and other videoconferencing for online meetings \n\nformr for online deployment of big team science projects \n\nexperimentum for building experiments and managing projects \n\nScienceVerse for documenting big team science projects \n\nInstitutions \n\nBig team science must navigate \nfrictions created by institutional \npolicies developed around solo PIs \n\nNone at present \n\nTable 3. Barriers to big team science and some potential solutions. \n\nIncentives. Due to the central importance of prestige in obtaining academic rewards, \n\nacademics are incentivized to obtain publications that can be used to enhance their prestige. This \nmeans that any collective research enterprise that wishes to direct the labor of academic scientists \nmust either rely on sporadic volunteerism, find some other reward, like money, that substitute for \nprestige, or find a way to dole out prestige. We describe the risks of relying on volunteerism and \nthe barriers to providing money as a reward in the sections on institutional barriers and risks of \nfinancial unsustainability. Here we deal more completely with prestige as a form of \ncompensation. \n\nA central problem with prestige as an incentive for participation in big team science is \n\nthat, under the current system for awarding credit for publications, the bulk of credit goes to the \nfirst-listed author. This reward structure does not effectively incentivize the labor of the \nnumerous other people who are necessary to produce big team science publications. Moreover, \nbig team science organizations must do a tremendous amount of administration and coordination \nthat, while critical to the success of the collaboration as a whole, is not easily creditable on \npublications.  \n\nWe see at least two innovations that may help resolve this problem. The first is \n\nconsortium authorship, in which a publication is credited to a collective entity rather than a \ngroup of individuals, or in which all members of the collaboration are listed alphabetically on all \npublications to render the individual subservient to the collective (Birnholtz, 2008). This was the \napproach to credit taken by the Open Science Collaboration when they published the \n\n \n\n \n\n\x0c9 \n\nReproducibility Project: Psychology (Open Science Collaboration, 2015). This is also the \napproach taken by some large physics collaborations, especially for papers that document details \nof infrastructure or instrumentation (e.g., ALICE collaboration, 2010; Harry & the LIGO \nScientific Collaboration, 2010; The ATLAS collaboration et al., 2018). This innovation \nincentivizes collaboration by attempting to flatten the credit reward structure. Consortium \nauthorship systems come at some risk, however: they can under-reward highly active but less \nvisible consortium members, or lead consortium members to seek informal routes of gaining \nrecognition, thereby effectively substituting the flat, formal system of credit with a more \narbitrary informal system (Birnholtz, 2008). \n\nThe second innovation is to disclose contributions in a more fine-grained way through \n\ncontributorship systems, such as CRediT (McNutt et al., 2018). These systems allow contributors \nto list within a manuscript the specific roles contributors play in projects. In principle, \ncontributorship systems should allow people to develop reputations for filling certain types of \nproject roles, such as data analysis or project management, and they should allow more fine-\ngrained accountability when errors are detected in the project. Finally, contributorship systems \nenable contributors to provide evidence of excellence in a particular project role when applying \nfor grants, jobs, or other professional rewards.  \n\nHowever, contributorship systems will only serve their purpose of giving recognition to \nexcellent team scientists if people who control professional rewards, such as members of hiring \ncommittees, promotion committees, and funding committees, actually attend to and reward \nevidence of excellent team science contributions. Although we see promise in contributorship \nsystems as a method to incentivize big team science labor (Holcombe, 2019), the existence of \ncontributorship information is by itself not sufficient to ensure that incentives are aligned to \nreward big team science labor. The people who control professional rewards must also attend to \nthese systems if they are to serve their intended functions. \n\n \nInfrastructure. A second obstacle is the need for infrastructure to help facilitate and \ncoordinate big team science projects. Some of this infrastructure already exists, such as the \nSlack, Google suite of collaboration tools, and Zoom. This existing infrastructure has been \ninstrumental in propelling big team science to where it is today. For example, projects run \nthrough the Psychological Science Accelerator (Moshontz et al., 2018) use a combination of \nSlack and email for project coordination, Google Docs, Sheets, and Forms for collaborative \nproject workflows (for a writing worklow, see Moshontz et al., 2019), a shared Google Drive for \ncollaborative files management, and Zoom for conference calls. However, this existing \ninfrastructure is general-purpose and therefore does not fully meet the specialized needs of \nbehavioral science. For example, most psychology data collection platforms are designed for use \nby one or two users. This is insufficient in a team of researchers numbering in the hundreds. \nAlthough not every researcher needs to be part of the development of, for example, a survey, \nmultiple users need to access survey instruments when helping with translation or other parts of \nthe survey development process. \n\nAnother example of where research infrastructure is insufficient is when recruiting \nsamples of participants that vary linguistically \xe2\x80\x93 a common occurrence when working with \nworldwide collaborators. Often, translated versions of the target measures are simply \nunavailable. Creating and validating a greater array of translated measure versions will go a long \n\n \n\n\x0c10 \n\nway toward facilitating big team science projects. A lack of applicability across languages and \ncountries apply to infrastructure across all stages of the research lifecycle. Data collection and \nmanagement platforms in particular must cope with both issues involving language and \ntranslation and the varying ethical and legal standards that govern private data. \n\nFortunately, some promising initiatives already exist that, if they are properly supported, \nmay help resolve these more specialized problems. For example, the formr experiment platform \n(Arslan et al., 2019) relies on some of the infrastructure that is already in place, namely Google \nSheets, to permit easy, flexible, and collaborative construction of online experiments. Another \npromising project in this vein is the experimentum framework (https://psa.psy.gla.ac.uk/), which \naims to develop an experiment builder and project management framework that is specialized for \nlarge, cross-linguistic, big team science settings. Finally, the ScienceVerse project (Lakens & \nDeBruine, 2021) aims to create a fully functional \xe2\x80\x9cgrammar of science\xe2\x80\x9d that can be used for \nnaming, describing, and identifying relationships between components of any scientific project. \nSuch a grammar would be especially helpful for highly collaborative, cross-linguistic, big team \nscience projects that must document a large number of components while navigating specialized \nethical and legal standards. \n\nThese initiatives will only succeed in meeting the specialized infrastructure needs of big \n\nteam science in the social sciences if they are supported financially and on a sustainable basis. \nFunders should recognize that these and other initiatives that support big team science also \nprovide benefits for small team science. For example, although formr is especially useful in team \nsettings, it provides a useful platform for teams of any size. Smart and ongoing investment in \nthese projects will go a long way toward facilitating both small team and big team science. \n\nInstitutions. Another set of obstacles relates to the institutions that have developed \nassuming that projects are led by a solo (or small number of) Principal Investigators. These \ninstitutions cause friction in almost every aspect of large, collaborative science. The frictions \nstart with funding mechanisms, most of which assume that projects are led by a sole PI and their \nstaff. The European Research Council\xe2\x80\x99s Consolidator Grant, for example, supports a single PI \nand their host institution, and the European Research Council enforces highly detailed \naccounting rules to ensure all money is used in support of a solo PI\xe2\x80\x99s project. As another \nexample, some grant funders, such as the US National Science Foundation, require applicants to \nlist all their collaborators from a specified time period. This requirement places burdensome \nrestrictions on prospective big team scientists whose collaborators can easily number in the \nhundreds. Even once money is awarded, administrative and legal barriers can make it difficult to \nsend the money to the institutions and countries where it is needed. \n\nSimilar frictions plague almost every part of the big team science research lifecycle. \n\nAcademic psychology departments do not typically train scientists to operate in large, distributed \nteam settings and do not have specialized training tracks for scientists who wish to specialize in \nthe many specific roles that big team science requires. Ethical Review Boards are often not \nprepared to evaluate a project that will be executed at a hundred sites worldwide. A big team \nscience project with 100 collaborating labs may need to submit 100 variations of the same \napplication to 100 Ethical Review Boards \xe2\x80\x93 a process that leads to immense waste and \nduplication of effort (Ervin et al., 2016; Schneider, 2015, pg. 44). Even the content of ethical \nregulations themselves differs across locations due to varying laws and norms. Some \n\n \n\n\x0c11 \n\norganizations are not subject to clear ethical requirements at all \xe2\x80\x93 or, as is the case in East Africa, \nfor example \xe2\x80\x93 they are only able to complete an ethical review in exchange for a costly fee that \ncompetes with other research costs (Kaplay, 2016; Osborne & Luoma, 2018).  \n\nAs another example, journal submission portals are built assuming that articles are \n\nwritten by a relatively small number of authors \xe2\x80\x93 not 200 \xe2\x80\x93 and the process of entering author \ninformation is cumbersome and wasteful. A similar barrier obstructs the entry of authorship \ninformation in preprint servers like PsyArXiv. Once research is complete, tenure and evaluation \ncommittees do not know how to evaluate publications with hundreds of authors. This decision is \nhigh stakes, because if researchers that prioritize big team science are penalized by these \ncommittees, these \xe2\x80\x9cbig team scientists\xe2\x80\x9d will be effectively selected out of the ecology of science \n(Smaldino & McElreath, 2016). \n\nHere we are less certain what the future holds. If the institutions of psychological science \n\nadapt to the emergence of big team science, they will make this sort of science less costly and \ntherefore more common. Alternatively, psychological science could develop an entirely new set \nof institutions, such as funders, ethical review boards, and journals, that are more \naccommodating of big team science. Whatever the future may hold, we hope that the institutions \nof science recognize the potential of big team science and act accordingly. \n\nRisks of big team science \n\nAlthough we believe that big team science approaches have great promise, they also \n\n \nbring a unique set of risks that stem from the very feature that provides their main benefits: the \nfact that they coordinate and centralize resources to be deployed in a single project (Stokols et \nal., 2008). These risks are unaccountable leadership, management failures, conservatism, \nsustainability failures, and mega-mistakes.  \n\nUnaccountable leadership. The primary virtue of big team science is its ability to deploy \n\nresources at a large scale. Often, this requires turning over resources to a single scientific \norganization, which carries the same risk as that entailed in turning over resources to a single \ngovernmental organization: the leadership of that organization could use the resources to \ncentralize power within themselves and make themselves unaccountable to influence and \ncriticism. \n\nThe negative consequences of unaccountable leadership power can impact many features \nof the scientific process. For example, in the idea generation phase, unaccountable leadership can \nselect their own pet topics for investigation without heeding the ideas of people with less power, \nleading to ideas that are less creative and impactful (Wu et al., 2019). Unaccountable leadership \ncan also stifle diversity in the strategies that are selected to develop and test the ideas that are \nselected (Devezer et al., 2019), slowing the pace of discovery. Finally, unaccountable leadership \ncan have negative consequences for individual scholars who do not conform to leadership\xe2\x80\x99s \nperhaps narrow expectations of who is a proper \xe2\x80\x9cbig team scientist\xe2\x80\x9d \xe2\x80\x93 especially if the leadership \nhas influence over important mechanisms of career advancement (Azoulay et al., 2019). \n\n \n\n \n\n \n\n\x0c12 \n\nRisk \n\nDescription \n\nMitigation strategies \n\nUnaccountable leadership \n\nLeadership of big team science \norganizations make themselves \nunaccountable to influence and \ncriticism \n\nArticulate organizational values \n\nCreate structured bylaws \n\nIntroduce democratic accountability \n\nEmpower a board of directors \n\nArticulate roles and responsibilities \n\nManagement failures \n\nBig team science organizations \ninvolve large numbers of \nrelationships, and navigating the \ninterpersonal issues that arise \nrequires effective management of \npeople \n\nCreate structured methods of making decisions \n\nCreate formal and informal ways to solicit team member feedback \n\nObtain formal management training \n\nProactively partner with organizations that serve underrepresented groups \n\nConservatism \n\nCreate mechanisms to facilitate mobility and advancement within the team \n\nBig team science organizations \nmay make decisions that cater to \nthe median desire of the people in \ntheir group \n\nSustainability failures \n\nBig team science organizations \nrequire planning to be financially \nsustainable over the long term \n\nMega-mistakes \n\nWhen big team science \norganizations make mistakes, they \ntend to be big ones \n\nTable 4. Risks of team science and some risk mitigation strategies. \n \n\nSeparate idea generation from project implementation \n\nPay people the people who are responsible for the organization's \nmaintenance \n\nCreate and follow a sustainability plan \n\nMitigate the other four types of risk \n\nInstitute pro-active quality control processes \n\nMitigating this risk. At a baseline, big team science organizations should have a set of \n\nbylaws that lay out who is empowered to do what and a set of values. The bylaws need not \nspecify a completely de-centralized power structure. Centralizing power within a small number \nof leaders can have important benefits, such as enabling those leaders to develop a coherent \norganizational strategy and allowing those leaders to make fast, agile decisions to adjust that \nstrategy (Baum & Wally, 2003). However, to mitigate the risk of a lack of accountability, \ncentralization of leadership must be accompanied with mechanisms to hold leadership \naccountable for its decisions if those leaders pursue directions with which important stakeholders \ndisagree. \n\nAn important tool for enabling such accountability is a clearly articulated statement of the \nvalues that guide the big team science organization\xe2\x80\x99s mission. Once articulated, these values can \n\n \n\n\x0c13 \n\nallow the organization\xe2\x80\x99s stakeholders to identify leadership mistakes that might require \naccountability by allowing those stakeholders to evaluate whether leadership actions do indeed \nfit with the organization\xe2\x80\x99s values. Insofar as leaders do issue regular statements about their \norganizational strategy, a set of well-articulated values also allows stakeholders to evaluate \nwhether that strategy is indeed pursuing those values.  \n\nHowever, values are empty unless a group of organization stakeholders are empowered to \n\ntake action if and when leaders violate those values. One example of an empowered stakeholder \nis a board of directors that is empowered by the organization\xe2\x80\x99s bylaws to remove leaders with \nwhom they are unsatisfied. This board of directors should receive regular updates on the status of \nthe big team science organization so that they can ask leaders to make corrections before a crisis \noccurs. Another possibility is to introduce explicit democratic mechanisms into the organization, \nsuch as regular elections for leadership positions. Whatever the form, accountability must be \nbuilt into the organization\xe2\x80\x99s structure if it is to effectively check the risks of unaccountable \nleadership.  \n\nManagement failures. The number of relationships within a team increases \n\ncombinatorially with the number of people within the team. Larger teams also create more \nopportunities for people with widely different backgrounds to collide \xe2\x80\x93 or for single trouble-\nmakers to create a toxic environment. Finally, larger teams are more likely to be characterized by \nspecialized roles and communication channels. These features vastly increase the complexity of \nrelationships in very large teams. In failure cases, this complexity can lead to management \nfailures. \n\nThe first of these failures is role ambiguity. Role ambiguity occurs when the information \n\na person has about the expectations that go along with the role, the methods for fulfilling those \nexpectations, and the consequences for violating those expectations are unclear (Van Sell et al., \n1981). Role ambiguity can occur in small groups, but the number of roles in a large scientific \nteam, combined with the specialization of roles and complex decision-making apparatus, \nincrease the risk of role ambiguity as teams increase in size. Role ambiguity increases the risk of \nerrors because team members are uncertain which responsibilities apply to them. This \nuncertainly can lead to duplicated work on the one hand and unfulfilled tasks on the other. Role \nambiguity also breeds feelings of dissatisfaction, leading people to grow disillusioned with the \nbig team science organization and, in extreme failure cases, to leave it (Tubre & Collins, 2000).  \n\nThe second failure is ineffective management of interpersonal issues. Some interpersonal \n\nissues, such as conflict between team members, are inevitable due simply to the number of \nrelationships involved in big team science organizations. Some level of conflict can even be \nconstructive when it is managed properly and kept task-focused (Forsyth, 2014; Loughry & \nAmason, 2014; Rahim, 2003). However, conflict can also create organizational dysfunction, \npotentially polarizing the team and derailing entire projects (Rahim, 2003). In the presence of \nsharp power differences between team members and unclear mechanisms for accountability, \nconflict can also take on a darker guise in the form of harassment and abuse (Berdahl & Raver, \n2011). The risks of conflict can be especially high when team members have sharp cultural or \nepistemic differences, as might be expected in big team science settings that bring together \npeople of varying personal, cultural, and disciplinary backgrounds (Bender et al., 2015). \n\n \n\n\x0c14 \n\nMitigating this risk. The most fundamental step a big team science organization can take \n\nto mitigate the risk of organizational failures is to establish a well-defined leadership structure \nthat clearly communicates and enforces the organization\xe2\x80\x99s roles and decisions (Stokols, 2006). \nThe leadership structure should also create both formal and informal ways of soliciting feedback \nfrom members of the big team science organization. These feedback mechanisms should include \na means to report problematic behavior by individual team members, along with a means to hold \nproblematic team members accountable, such as reporting the behavior to the team member\xe2\x80\x99s \nworkplace. The feedback mechanisms should establish a feedback loop between decision-makers \nand team members so that the organization responds effectively to issues as they arise. The goal \nof the feedback loop will not eliminate interpersonal issues, as some number of issues are simply \npart of the human condition, but rather to instill a sense that procedures are fair when issues do \noccur (Konovsky, 2000). Effective communication also fosters an environment of psychological \nsafety in which team members feel empowered to express issues, concerns, and points of \ndisagreement without creating outright fights (Frazier et al., 2017). \n\nClarifying roles and instilling psychological safety both take time. However, their \n\nimportance to effective collaboration means that team science organizations should prioritize \ncreating structures that allow transparent communication, decision-making, and enforcement, \nideally through a formal collaboration agreement that is drafted before the collaboration begins. \nStructured methods of communication, decision-making, and enforcement lay the groundwork to \nallow team members to build the mutual trust that is necessary for collaboration (Astuti & Bloch, \n2012). \n\nManagement of teams is a vital skill that is seldom taught in the academy. Universities \n\nshould recognize the central role that management plays in the success of projects, especially the \nlarger ones that typify big team science. If big team science is to take root in the social sciences, \nmanagement training needs to become a central part of the behavioral science curriculum. \n\nConservatism. Large organizations require large amounts of people to keep themselves \nrunning. If these organizations attempt to satisfy the desires of these large numbers of people, \nthey will tend to cater to the median desire. This means that organizations will usually be \nconservative \xe2\x80\x93 or at least conservative with respect to the people within the group. This \nconservatism can manifest in two ways: in the selection of personnel and the selection of \nprojects. \n\nPeople tend to form social relationships with others who have similar characteristics and \n\nbackgrounds (Kossinets & Watts, 2009; McPherson et al., 2001). Given that contemporary \npsychology is dominated by North America and Europe (Rad et al., 2018; Thalmayer et al., \n2020), this raises a risk that big team science organizations will be similarly dominated by people \nfrom those continents. For example, not a single first author of the papers in Table 2 comes from \na nation outside North America and Europe. Once big team science organizations dominated by \nNorth America and Europe are established, they may inadvertently crowd out organizations from \nelsewhere with different goals and personnel. Systematically excluding large subsets of humanity \nfrom psychology perpetuates unfair systems of inequality and can lead to a science that focuses \nunduly on the preoccupations of a small subset of humanity (Medin et al., 2017). \n\n \n\n\x0c15 \n\nConservatism can also manifest in project selection. The primary purpose of big team \nscience is to scale up the resources that can be invested in a given project. However, this very \nvirtue may induce big team science collaborations to select projects that they perceive as \xe2\x80\x9csafe\xe2\x80\x9d \xe2\x80\x93 \nboth in terms of whether they can be feasibly executed and in terms of the degree to which they \ndeviate from the scientific mainstream. Given that some degree of theoretical risk is necessary to \nspur scientific progress (Devezer et al., 2019; Meehl, 1978), this theoretical conservatism could \nfeasibly slow scientific progress. \n\nMitigating this risk. To mitigate the risk of conservatism in personnel selection, we can \n\ndraw on a general slate of strategies to improve the inclusion of underrepresented groups in \nscience (Enriquez, 1979; Henrich et al., 2010; Syed & Kathawalla, 2020). These solutions must \nbegin at the start of big team science organizations and must be continually re-evaluated \nthroughout the organization\xe2\x80\x99s lifecycle. At the organization\xe2\x80\x99s founding, the founders must \nproactively and systematically partner with researchers in a broad array of non-Western and non-\nelite institutions \xe2\x80\x93 and especially those who are not part of the \xe2\x80\x9cvirtual academic commons\xe2\x80\x9d \nformed by Twitter, Facebook, and science blogs. Once the organization has been founded, the \norganization\xe2\x80\x99s leadership must create mechanisms that allow for mobility within the team so that \nmembers of underrepresented groups can rise to positions of leadership. However, even these \nmeasures will not be sustainable without the direct investment of material resources into research \ninfrastructure in under-resourced world regions.  \n\nDiversity in big team science personnel will not ensure diversity in big team science \n\nprojects. Mitigating the risk of this type of conservatism requires maintaining a separation \nbetween the idea generation and the project implementation phases of big team science projects. \nDuring the idea generation phase, much smaller groups of scientists can identify problems and \napproaches unconstrained from a broader team consensus. The smaller teams then develop \nproposals based on their ideas and submit them for consideration by the larger consortium. The \nlarger team can even explicitly build in mechanisms to solicit proposals from teams whose \nperspectives may differ from the scientific mainstream \xe2\x80\x93 such as those from outside North \nAmerica and Europe. \n\nSustainability failures. The history of science is littered with promising initiatives and \n\norganizations that, once established, could not be sustained (Borgman et al., 2016). This problem \nrecurs so frequently because scientific organizations are typically public-minded: they wish to \nprovide a public good at little or no cost. Yet, this very public-mindedness invites free-ridership \n\xe2\x80\x93 people who will use the organization\xe2\x80\x99s service but who are either unwilling or unable to \nsupport the organization financially (Neylon, 2017). \n\nThis dynamic also threatens big team science organizations. All the initiatives listed in \nTable 2 were formed because the founders thought that large-scale collaboration could lead to \nbetter science rather than out of a desire to make money. If the initiative is intended to last for a \nsingle project, the project could survive using an ad hoc organizational structure run on the back \nof volunteer labor. However, continually creating one-shot organizations is wasteful and \ninefficient because such a model prevents the accumulation of organizational knowledge. If an \ninitiative is to last beyond a single project, its leadership must at some point figure out how to \ngenerate the money required to retain long-term staff while not compromising the vision that \ninspired the project in the first place.  \n\n \n\n\x0c16 \n\nMitigating this risk. The first step to mitigating the risk of a sustainability failure is to \n\nrecognize that organizations cannot support themselves for free. If we want to have organizations \nthat are capable of coordinating very large groups of scientists, those organizations must receive \nfinancial support so that they can generate and maintain the knowledge, staff, and other scientific \nresources necessary to fulfill their function. A corollary of this dictum is that, over the long haul, \nat least some skilled positions in a big team science organization must receive monetary \ncompensation. Running entirely on volunteerism risks exploiting the idealistic people willing to \ndonate their time (Kim et al., 2020), leading to burnout and interpersonal conflict. \n\nThe second step to mitigating this risk involves creating and following a sustainability \nplan (for an example, see Forscher & IJzerman, 2021) that maps out how the organization will \ngenerate the funds necessary to maintain itself. The funding models could entail what is most \nfamiliar to academic scientists: designating some team members as grantwriters who spend part \nof their time identifying and applying for large grants to support the big team science \norganization. However, this grantwriting model puts big team science organizations at the whims \nof large funders, who are often only willing to spend money on projects that generate \ndiscoveries, not the infrastructure that makes the discoveries possible (Zakaria et al., 2021). Most \nscience grantmakers also rely on competitive peer review to select the most \xe2\x80\x9cworthy\xe2\x80\x9d projects, a \nfunding model that risks inefficiency because it encourages grantwriters to spend more time on \ngrants than the activities the grants would fund (Gross & Bergstrom, 2019). For these reasons, \nwe believe that a grantwriting-based funding model carries inherent risks of both research waste \nand instability in personnel due to feast-or-famine funding cycles. \n\nWe believe that a more promising approach is to use one of the funding models that have \nled to sustainable funding for other large public-minded scientific organizations (Neylon, 2017). \nThese funding models will likely involve either creating a system that \xe2\x80\x9ctaxes\xe2\x80\x9d all beneficiaries of \nthe big team science infrastructure by, say, imposing membership dues rather as a scientific \nsociety does, or, alternatively providing the infrastructure as a byproduct of selling another \nservice. The other service could involve many things, but it should leverage what big team \nscience organizations already do well \xe2\x80\x93 running multi-site studies. Thus, the organization might \nrun some multi-site studies for a fee, or it might sell a service that it has needed to perfect in \norder to run multi-site studies, such as translation or project management. We believe that all \nthese options are viable, though each has their own tradeoffs; membership dues could make the \nbig team science organization inaccessible to lower-resourced members, whereas selling services \ncould introduce conflicts of interest that threaten the organization\xe2\x80\x99s mission. Choosing and \nfollowing a sustainability plan is necessary to maintain any public-minded organization, but the \ntradeoffs of the available options must also be evaluated with care. \n\nMega-mistakes. The final risk of big team science is that of what we term mega-mistakes. \n\nBig team science\xe2\x80\x99s primary virtue is its ability to \xe2\x80\x9cscale up\xe2\x80\x9d small projects into big ones by \npooling resources across labs. However, this very virtue makes errors all the more costly: these \nerrors risk wasting much more resources than would be wasted on a smaller project. These errors \ncan even occur at the time of project selection if the topic of the project is not one that deserves \nthe high investment of resources that big team science brings. \n\nErrors in big team projects often have no one simple cause. Consider the example of the \n\nHuman Brain Project. This project aimed to unlock massive advances for neuroscience by \n\n \n\n\x0c17 \n\ndeveloping and building large-scale computer simulations of brain regions, and eventually of \nentire brains (Markram, 2012). The leader of the project, Henry Markram, set an ambitious \nagenda; in 2009, he claimed that, after only ten more years of development the technology that \npowered the project would allow the simulation of an entire rodent brain, an achievement that he \nbelieved would revolutionize neuroscience (Abbott, 2020). The project\xe2\x80\x99s vision and ambition \nattracted major investment from both universities and funders; in 2015, the project had 112 \nuniversity partners and a project budget of over \xe2\x82\xac1 billion (Abbott, 2015). \n\nYet the project was plagued with problems. These problems spanned multiple domains, \n\nranging from concerns that the project was monopolizing resources that could be allocated to \nother worthy projects, to disagreement over the project\xe2\x80\x99s core aims, to dissatisfaction with the \nproject\xe2\x80\x99s autocratic leadership structure (Abbott, 2014). Regardless of the specific causes, by \n2015 most scientists agreed that the project was not living up to Henry Markram\xe2\x80\x99s original vision \n(Abbott, 2015). Markram was removed from his leadership position the following year (Abbott, \n2020). The project is still ongoing, but whether it will produce the revolution in neuroscience \nthat Markram promised is anything but clear. \n\nMitigating this risk. As the example of the Human Brain Project illustrates, mega-\n\nmistakes can emerge due to a failure to manage other risks of big team science, such as the risk \nof mis-management. Thus, mitigating other risks of big team science may be one useful way of \nmitigating the risks of mega-mistakes. \n\nHowever, big team science organizations can also proactively implement quality control \n\nprocesses that minimize the risks of mega-mistakes more directly. These quality control \nprocesses entail everything from formally reviewing project proposals, to instituting code \nreview, pilot tests, and project \xe2\x80\x9csoft launches\xe2\x80\x9d, to formal methods of optionally stopping data \ncollection to avoid overinvestment in bad ideas (Lakens, 2014; Sch\xc3\xb6nbrodt et al., 2017). Quality \ncontrol can also involve partnering with outside scientific organizations that implement their own \nmethods of quality control, such as journals that administer Registered Reports.  \n\nAn additional promising strategy involves instituting audits of scientific processes by \npeople who are formally independent of the big team science collaboration. These audits can \ntake the form of research design review by outside experts, systems of back-translation to check \nthe quality of forward-translation, formal code review, and even \xe2\x80\x9cred teams\xe2\x80\x9d who receive \nbounties for spotting bugs and other project flaws (Lakens, 2020).  \n\nAll these quality control methods have high value in solo PI projects, but take on renewed \n\nimportance in projects that command resources on the scale of big team science. \n\nBig team science will never be a wholesale replacement to solo science \xe2\x80\x93 nor should it \n\n \nbe. Absent the coordination constraints of a large team, solo scientists have the freedom to \nflexibly explore ideas that might be infeasible in a larger group setting without creating a risk of \nsquandering a scale of resources that rises to the level of a \xe2\x80\x9cmega-mistake\xe2\x80\x9d. \n\nWhat big team science can do is vastly \xe2\x80\x9cscale up\xe2\x80\x9d the amount of resources \xe2\x80\x93 in other \nwords, the money, person-hours, and specialized expertise \xe2\x80\x93 that can be deployed in a single, \n\nConclusion \n\n \n\n\x0c18 \n\nwell-chosen and well-vetted project. This increased resource investment can result in projects \nthat are larger, more rigorous, and more representative of humanity. We believe this virtue is not \nto be underestimated, as low resource investment is likely a key ultimate cause of why \npsychology\xe2\x80\x99s progress has been slow. Moreover, other disciplines may find themselves facing \nsimilar problems. Ecologists identified similar problems in their own discipline, and team \nscience is emerging as a potential solution (Fraser et al., 2013). Pre-clinical cancer biology faces \nproblems even conducting the studies needed to assess whether a replicability problem exists \n(Friedl, 2019). A greater focus on big team science may help pre-clinical cancer biology \xe2\x80\x9cscale \nup\xe2\x80\x9d the resources devoted to the typical study in the discipline, which may help lay the \ngroundwork to allow these replication studies to be conducted. \n\nWe believe that big team science has the potential to simultaneously tackle the many \n\nchallenges that psychology faces. However, to fulfill this potential, psychological scientists must \nrecognize and manage the many barriers and risks that this approach entails. If properly managed \nto leverage its virtues while mitigating its risks, we believe that big team science can be \ninstrumental in the movement to build more reliable, informative, and rigorous science. \n\n \n\n \n \n\n \n\n\x0c19 \n\n \n23andMe Research Team, COGENT (Cognitive Genomics Consortium), Social Science Genetic \n\nReferences \n\nAssociation Consortium, Lee, J. J., Wedow, R., Okbay, A., Kong, E., Maghzian, O., \n\nZacher, M., Nguyen-Viet, T. A., Bowers, P., Sidorenko, J., Karlsson Linn\xc3\xa9r, R., Fontana, \n\nM. A., Kundu, T., Lee, C., Li, H., Li, R., Royer, R., \xe2\x80\xa6 Cesarini, D. (2018). Gene \n\ndiscovery and polygenic prediction from a genome-wide association study of educational \n\nattainment in 1.1 million individuals. Nature Genetics, 50(8), 1112\xe2\x80\x931121. \n\nhttps://doi.org/10.1038/s41588-018-0147-3 \n\nAbbott, A. (2014). Row hits flagship brain plan. Nature, 511(7508), 133\xe2\x80\x93134. \n\nhttps://doi.org/10.1038/511133a \n\nAbbott, A. (2015). Human Brain Project votes for leadership change. Nature, nature.2015.17060. \n\nhttps://doi.org/10.1038/nature.2015.17060 \n\nAbbott, A. (2020). Documentary follows implosion of billion-euro brain project. Nature, \n\n588(7837), 215\xe2\x80\x93216. https://doi.org/10.1038/d41586-020-03462-3 \n\nALICE collaboration. (2010). Alignment of the ALICE Inner Tracking System with cosmic-ray \n\ntracks. Journal of Instrumentation, 5(03), P03003\xe2\x80\x93P03003. https://doi.org/10.1088/1748-\n\n0221/5/03/P03003 \n\nArslan, R. C., Walther, M. P., & Tata, C. S. (2019). formr: A study framework allowing for \n\nautomated feedback generation and complex longitudinal experience-sampling studies \n\nusing R. Behavior Research Methods. https://doi.org/10.3758/s13428-019-01236-y \n\nAstuti, R., & Bloch, M. (2012). Anthropologists as Cognitive Scientists. Topics in Cognitive \n\nScience, 4(3), 453\xe2\x80\x93461. https://doi.org/10.1111/j.1756-8765.2012.01191.x \n\n \n\n\x0c20 \n\nAzoulay, P., Fons-Rosen, C., & Zivin, J. S. G. (2019). Does Science Advance One Funeral at a \n\nTime? American Economic Review, 109(8), 2889\xe2\x80\x932920. \n\nhttps://doi.org/10.1257/aer.20161574 \n\nBakker, C. J. (1955). CERN: European Organization for Nuclear Research. Physics Today, 8, 8\xe2\x80\x93\n\n13. \n\nBaum, J. R., & Wally, S. (2003). Strategic decision speed and firm performance. Strategic \n\nManagement Journal, 24(11), 1107\xe2\x80\x931129. https://doi.org/10.1002/smj.343 \n\nBender, A., Beller, S., & Nersessian, N. J. (2015). Diversity as Asset. Topics in Cognitive \n\nScience, 7(4), 677\xe2\x80\x93688. https://doi.org/10.1111/tops.12161 \n\nBerdahl, J. L., & Raver, J. L. (2011). Sexual harassment. In S. Zedeck (Ed.), APA handbook of \n\nindustrial and organizational psychology, Vol 3: Maintaining, expanding, and \n\ncontracting the organization. (pp. 641\xe2\x80\x93669). American Psychological Association. \n\nhttps://doi.org/10.1037/12171-018 \n\nBirnholtz, J. (2008). When Authorship Isn\xe2\x80\x99t Enough: Lessons from CERN on the Implications of \n\nFormal and Informal Credit Attribution Mechanisms in Collaborative Research. The \n\nJournal of Electronic Publishing, 11(1). https://doi.org/10.3998/3336451.0011.105 \n\nBorgman, C. L., Darch, P. T., Sands, A. E., & Golshan, M. S. (2016). The Durability and \n\nFragility of Knowledge Infrastructures: Lessons Learned from Astronomy. \n\nArXiv:1611.00055 [Astro-Ph]. http://arxiv.org/abs/1611.00055 \n\nBotvinik-Nezer, R., Holzmeister, F., Camerer, C. F., Dreber, A., Huber, J., Johannesson, M., \n\nKirchler, M., Iwanir, R., Mumford, J. A., Adcock, A., Avesani, P., Baczkowski, B., \n\nBajracharya, A., Bakst, L., Ball, S., Barilari, M., Bault, N., Beaton, D., Beitner, J., \xe2\x80\xa6 \n\n \n\n\x0c21 \n\nSchonberg, T. (2019). Variability in the analysis of a single neuroimaging dataset by \n\nmany teams [Preprint]. Neuroscience. https://doi.org/10.1101/843193 \n\nBrumfiel, G. (2011). High-energy physics: Down the petabyte highway. Nature, 469(7330), \n\n282\xe2\x80\x93283. https://doi.org/10.1038/469282a \n\nBryant, P. J. (1994). A Brief History and Review of Accelerators (No. 261062). CERN. \n\nButton, K. S., Ioannidis, J. P. A., Mokrysz, C., Nosek, B. A., Flint, J., Robinson, E. S. J., & \n\nMunaf\xc3\xb2, M. R. (2013). Power failure: Why small sample size undermines the reliability \n\nof neuroscience. Nature Reviews Neuroscience, 14(5), 365\xe2\x80\x93376. \n\nhttps://doi.org/10.1038/nrn3475 \n\nByers-Heinlein, K., Bergmann, C., Davies, C., Frank, M. C., Hamlin, J. K., Kline, M., \n\nKominsky, J. F., Kosie, J. E., Lew-Williams, C., Liu, L., Mastroberardino, M., Singh, L., \n\nWaddell, C. P. G., Zettersten, M., & Soderstrom, M. (2020). Building a collaborative \n\npsychological science: Lessons learned from ManyBabies 1. Canadian \n\nPsychology/Psychologie Canadienne, 61(4), 349\xe2\x80\x93363. \n\nhttps://doi.org/10.1037/cap0000216 \n\nColes, N. A., March, D. S., Marmolejo-Ramos, F., Arinze, N. C., Ndukaihe, I. L. G., \xc3\x96zdo\xc4\x9fru, \n\nA. A., Aczel, B., Hajdu, N., Nagy, T., Basnight-Brown, D., Zambrano, D., Foroni, F., \n\nWillis, M., Pfuhl, G., Kaminski, G., IJzerman, H., Vezirian, K., Banaruee, H., Suarez, I., \n\n\xe2\x80\xa6 Liuzza, M. T. (2019). A Multi-Lab Test of the Facial Feedback Hypothesis by The \n\nMany Smiles Collaboration [Preprint]. PsyArXiv. https://doi.org/10.31234/osf.io/cvpuw \n\nCorvin, A., Craddock, N., & Sullivan, P. F. (2010). Genome-wide association studies: A primer. \n\nPsychological Medicine, 40(7), 1063\xe2\x80\x931077. https://doi.org/10.1017/S0033291709991723 \n\n \n\n\x0c22 \n\nCuccolo, K., Irgens, M. S., Zlokovich, M. S., Grahe, J., & Edlund, J. E. (2021). What \n\nCrowdsourcing Can Offer to Cross-Cultural Psychological Science. Cross-Cultural \n\nResearch, 55(1), 3\xe2\x80\x9328. https://doi.org/10.1177/1069397120950628 \n\nDannlowski, U., Ohrmann, P., Bauer, J., Deckert, J., Hohoff, C., Kugel, H., Arolt, V., Heindel, \n\nW., Kersting, A., Baune, B. T., & Suslow, T. (2008). 5-HTTLPR Biases Amygdala \n\nActivity in Response to Masked Facial Expressions in Major Depression. \n\nNeuropsychopharmacology, 33(2), 418\xe2\x80\x93424. https://doi.org/10.1038/sj.npp.1301411 \n\nDevezer, B., Nardin, L. G., Baumgaertner, B., & Buzbas, E. O. (2019). Scientific discovery in a \n\nmodel-centric framework: Reproducibility, innovation, and epistemic diversity. PLOS \n\nONE, 14(5), e0216125. https://doi.org/10.1371/journal.pone.0216125 \n\nDonoho, D., Arian, M., Imam, R., Morteza, S., & Stodden, V. (2008). 15 Years of Reproducible \n\nResearch in Computational Harmonic Analysis (pp. 1\xe2\x80\x9326) [Technical Report]. \n\nDepartment of Statistics, Stanford University. \n\nhttp://statweb.stanford.edu/~wavelab/DonohoEtAlCISESubmission.pdf \n\nEbersole, C. R., Atherton, O. E., Belanger, A. L., Skulborstad, H. M., Allen, J. M., Banks, J. B., \n\nBaranski, E., Bernstein, M. J., Bonfiglio, D. B. V., Boucher, L., Brown, E. R., Budiman, \n\nN. I., Cairo, A. H., Capaldi, C. A., Chartier, C. R., Chung, J. M., Cicero, D. C., Coleman, \n\nJ. A., Conway, J. G., \xe2\x80\xa6 Nosek, B. A. (2016). Many Labs 3: Evaluating participant pool \n\nquality across the academic semester via replication. Journal of Experimental Social \n\nPsychology, 67, 68\xe2\x80\x9382. https://doi.org/10.1016/j.jesp.2015.10.012 \n\nEnriquez, V. G. (1979). Towards cross-cultural knowledge through cross-indigenous methods \n\nand perspective. Philippine Journal of Psychology, 12(1), 9\xe2\x80\x9315. \n\n \n\n\x0c23 \n\nErvin, A.-M., Taylor, H. A., & Ehrhardt, S. (2016). NIH Policy on Single-IRB Review\xe2\x80\x94A New \n\nEra in Multicenter Studies. New England Journal of Medicine, 375(24), 2315\xe2\x80\x932317. \n\nhttps://doi.org/10.1056/NEJMp1608766 \n\nEspinosa, J. P. A., Levcovitz, G. B., Bianchi, R.-M., Brock, I., Carli, T., Castro, N. F., Ciocio, \n\nA., Colautti, M., Menezes, A. C. D. S., da Fonseca, G. D. O., Alves, L. D. M., Hoecker, \n\nA., Ramos, B. L., Pinh\xc3\xa3o, G. L. L., Maidantchik, C., Malek, F., McPherson, R., Picco, G., \n\n& Santos, M. T. D. (2020). The ATLAS Publication Process Supported by Continuous \n\nIntegration and Web Framework. ArXiv:2005.06989 [Hep-Ex]. \n\nhttp://arxiv.org/abs/2005.06989 \n\nFanelli, D. (2010). \xe2\x80\x9cPositive\xe2\x80\x9d results increase down the hierarchy of the sciences. PLoS ONE, \n\n5(4), e10068. https://doi.org/10.1371/journal.pone.0010068 \n\nFinkel, E. J., Eastwick, P. W., & Reis, H. T. (2017). Replicability and other features of a high-\n\nquality science: Toward a balanced and empirical approach. Journal of Personality and \n\nSocial Psychology, 113(2), 244\xe2\x80\x93253. https://doi.org/10.1037/pspi0000075 \n\nForscher, P. S., & IJzerman, H. (2021, January 11). How should we fund the PSA? \n\nPsychological Science Accelerator Blog. https://psysciacc.org/2021/01/11/how-should-\n\nwe-fund-the-psa/ \n\nForsyth, D. R. (2014). Group dynamics (6th ed). Wadsworth Cengage Learning. \n\nFraser, L. H., Henry, H. A., Carlyle, C. N., White, S. R., Beierkuhnlein, C., Cahill, J. F., Casper, \n\nB. B., Cleland, E., Collins, S. L., Dukes, J. S., Knapp, A. K., Lind, E., Long, R., Luo, Y., \n\nReich, P. B., Smith, M. D., Sternberg, M., & Turkington, R. (2013). Coordinated \n\ndistributed experiments: An emerging tool for testing global hypotheses in ecology and \n\n \n\n\x0c24 \n\nenvironmental science. Frontiers in Ecology and the Environment, 11(3), 147\xe2\x80\x93155. \n\nhttps://doi.org/10.1890/110279 \n\nFrazier, M. L., Fainshmidt, S., Klinger, R. L., Pezeshkan, A., & Vracheva, V. (2017). \n\nPsychological Safety: A Meta-Analytic Review and Extension. Personnel Psychology, \n\n70(1), 113\xe2\x80\x93165. https://doi.org/10.1111/peps.12183 \n\nFriedl, P. (2019). Rethinking research into metastasis. ELife, 8, e53511. \n\nhttps://doi.org/10.7554/eLife.53511 \n\nGillespie, N. A., Whitfield, J. B., Williams, B., Heath, A. C., & Martin, N. G. (2005). The \n\nrelationship between stressful life events, the serotonin transporter (5-HTTLPR) genotype \n\nand major depression. Psychological Medicine, 35(1), 101\xe2\x80\x93111. \n\nhttps://doi.org/10.1017/S0033291704002727 \n\nGotlib, I. H., Joormann, J., Minor, K. L., & Hallmayer, J. (2008). HPA Axis Reactivity: A \n\nMechanism Underlying the Associations Among 5-HTTLPR, Stress, and Depression. \n\nBiological Psychiatry, 63(9), 847\xe2\x80\x93851. https://doi.org/10.1016/j.biopsych.2007.10.008 \n\nGross, K., & Bergstrom, C. T. (2019). Contest models highlight inherent inefficiencies of \n\nscientific funding competitions. PLOS Biology, 17(1), e3000065. \n\nhttps://doi.org/10.1371/journal.pbio.3000065 \n\nHardwicke, T. E., Mathur, M. B., MacDonald, K., Nilsonne, G., Banks, G. C., Kidwell, M. C., \n\nHofelich Mohr, A., Clayton, E., Yoon, E. J., Henry Tessler, M., Lenne, R. L., Altman, S., \n\nLong, B., & Frank, M. C. (2018). Data availability, reusability, and analytic \n\nreproducibility: Evaluating the impact of a mandatory open data policy at the journal \n\nCognition. Royal Society Open Science, 5(8), 180448. \n\nhttps://doi.org/10.1098/rsos.180448 \n\n \n\n\x0c25 \n\nHarry, G. M. & the LIGO Scientific Collaboration. (2010). Advanced LIGO: The next \n\ngeneration of gravitational wave detectors. Classical and Quantum Gravity, 27(8), \n\n084006. https://doi.org/10.1088/0264-9381/27/8/084006 \n\nHeils, A., Teufel, A., Petri, S., St\xc3\xb6ber, G., Riederer, P., Bengel, D., & Lesch, K. P. (1996). \n\nAllelic Variation of Human Serotonin Transporter Gene Expression. Journal of \n\nNeurochemistry, 66(6), 2621\xe2\x80\x932624. https://doi.org/10.1046/j.1471-\n\n4159.1996.66062621.x \n\nHenrich, J., Heine, S. J., & Norenzayan, A. (2010). The weirdest people in the world? \n\nBehavioral and Brain Sciences, 33(2\xe2\x80\x933), 61\xe2\x80\x9383. \n\nhttps://doi.org/10.1017/S0140525X0999152X \n\nHolcombe, A. O. (2019). Contributorship, Not Authorship: Use CRediT to Indicate Who Did \n\nWhat. Publications, 7(3), 48. https://doi.org/10.3390/publications7030048 \n\nJones, B. C., DeBruine, L. M., Flake, J. K., Liuzza, M. T., Antfolk, J., Arinze, N. C., Ndukaihe, \n\nI. L. G., Bloxsom, N. G., Lewis, S. C., Foroni, F., Willis, M. L., Cubillas, C. P., Vadillo, \n\nM. A., Turiegano, E., Gilead, M., Simchon, A., Saribay, S. A., Owsley, N. C., Jang, C., \n\n\xe2\x80\xa6 Coles, N. A. (2021). To which world regions does the valence-dominance model of \n\nsocial perception apply? Nature Human Behaviour. https://doi.org/10.1038/s41562-020-\n\nKaplay, S. (2016, July 6). In clinical trials, for-profit review boards are taking over for hospitals. \n\nShould they? STAT. https://www.statnews.com/2016/07/06/institutional-review-boards-\n\nKim, J. Y., Campbell, T. H., Shepherd, S., & Kay, A. C. (2020). Understanding contemporary \n\nforms of exploitation: Attributions of passion serve to legitimize the poor treatment of \n\n01007-2 \n\ncommercial-irbs/ \n\n \n\n\x0c26 \n\nworkers. Journal of Personality and Social Psychology, 118(1), 121\xe2\x80\x93148. \n\nhttps://doi.org/10.1037/pspi0000190 \n\nKlein, R. A., Cook, C. L., Ebersole, C. R., Vitiello, C. A., Nosek, B. A., Chartier, C. R., \n\nChristopherson, C. D., Clay, S., Collisson, B., Crawford, J., Cromar, R., Dudley, D., \n\nGardiner, G., Gosnell, C., Grahe, J. E., Hall, C., Joy-Gaba, J. A., Legg, A. M., Levitan, \n\nC., \xe2\x80\xa6 Ratliff, K. A. (2019). Many Labs 4: Failure to Replicate Mortality Salience Effect \n\nWith and Without Original Author Involvement [Preprint]. PsyArXiv. \n\nhttps://doi.org/10.31234/osf.io/vef2c \n\nKlein, R. A., Ratliff, K. A., Vianello, M., Adams, R. B., Bahn\xc3\xadk, \xc5\xa0., Bernstein, M. J., Bocian, K., \n\nBrandt, M. J., Brooks, B., Brumbaugh, C. C., Cemalcilar, Z., Chandler, J., Cheong, W., \n\nDavis, W. E., Devos, T., Eisner, M., Frankowska, N., Furrow, D., Galliani, E. M., \xe2\x80\xa6 \n\nNosek, B. A. (2014). Investigating Variation in Replicability: A \xe2\x80\x9cMany Labs\xe2\x80\x9d \n\nReplication Project. Social Psychology, 45(3), 142\xe2\x80\x93152. https://doi.org/10.1027/1864-\n\n9335/a000178 \n\nKlein, R. A., Vianello, M., Hasselman, F., Adams, B. G., Adams, R. B., Alper, S., Aveyard, M., \n\nAxt, J. R., Babalola, M. T., Bahn\xc3\xadk, \xc5\xa0., Batra, R., Berkics, M., Bernstein, M. J., Berry, D. \n\nR., Bialobrzeska, O., Binan, E. D., Bocian, K., Brandt, M. J., Busching, R., \xe2\x80\xa6 Nosek, B. \n\nA. (2018). Many Labs 2: Investigating Variation in Replicability Across Samples and \n\nSettings. Advances in Methods and Practices in Psychological Science, 1(4), 443\xe2\x80\x93490. \n\nhttps://doi.org/10.1177/2515245918810225 \n\nKonovsky, M. A. (2000). Understanding Procedural Justice and Its Impact on Business \n\nOrganizations. Journal of Management, 26(3), 489\xe2\x80\x93511. \n\nhttps://doi.org/10.1177/014920630002600306 \n\n \n\n\x0c27 \n\nKossinets, G., & Watts, D. J. (2009). Origins of Homophily in an Evolving Social Network. \n\nAmerican Journal of Sociology, 115(2), 405\xe2\x80\x93450. https://doi.org/10.1086/599247 \n\nKrige, J. (2004). I. I. Rabi and the Birth of CERN. Physics Today, 57(9), 44\xe2\x80\x9348. \n\nhttps://doi.org/10.1063/1.1809091 \n\nLai, C. K., Marini, M., Lehr, S. A., Cerruti, C., Shin, J.-E. L., Joy-Gaba, J. A., Ho, A. K., \n\nTeachman, B. A., Wojcik, S. P., Koleva, S. P., Frazier, R. S., Heiphetz, L., Chen, E. E., \n\nTurner, R. N., Haidt, J., Kesebir, S., Hawkins, C. B., Schaefer, H. S., Rubichi, S., \xe2\x80\xa6 \n\nNosek, B. A. (2014). Reducing implicit racial preferences: I. A comparative investigation \n\nof 17 interventions. Journal of Experimental Psychology: General, 143(4), 1765\xe2\x80\x931785. \n\nhttps://doi.org/10.1037/a0036260 \n\nLai, C. K., Skinner, A. L., Cooley, E., Murrar, S., Brauer, M., Devos, T., Calanchini, J., Xiao, Y. \n\nJ., Pedram, C., Marshburn, C. K., Simon, S., Blanchar, J. C., Joy-Gaba, J. A., Conway, J., \n\nRedford, L., Klein, R. A., Roussos, G., Schellhaas, F. M. H., Burns, M., \xe2\x80\xa6 Nosek, B. A. \n\n(2016). Reducing implicit racial preferences: II. Intervention effectiveness across time. \n\nJournal of Experimental Psychology: General, 145(8), 1001\xe2\x80\x931016. \n\nhttps://doi.org/10.1037/xge0000179 \n\nLakens, D. (2014). Performing high-powered studies efficiently with sequential analyses: \n\nSequential analyses. European Journal of Social Psychology, 44(7), 701\xe2\x80\x93710. \n\nhttps://doi.org/10.1002/ejsp.2023 \n\nLakens, D. (2020). Pandemic researchers\xe2\x80\x94Recruit your own best critics. Nature, 581(7807), \n\n121\xe2\x80\x93121. https://doi.org/10.1038/d41586-020-01392-8 \n\nLakens, D., & DeBruine, L. M. (2021). Improving Transparency, Falsifiability, and Rigor by \n\nMaking Hypothesis Tests Machine-Readable. Advances in Methods and Practices in \n\n \n\n\x0c28 \n\nPsychological Science, 4(2), 251524592097094. \n\nhttps://doi.org/10.1177/2515245920970949 \n\nLandy, J. F., Jia, M. (Liam), Ding, I. L., Viganola, D., Tierney, W., Dreber, A., Johannesson, M., \n\nPfeiffer, T., Ebersole, C. R., Gronau, Q. F., Ly, A., van den Bergh, D., Marsman, M., \n\nDerks, K., Wagenmakers, E.-J., Proctor, A., Bartels, D. M., Bauman, C. W., Brady, W. J., \n\n\xe2\x80\xa6 Uhlmann, E. L. (2020). Crowdsourcing hypothesis tests: Making transparent how \n\ndesign choices shape research results. Psychological Bulletin, 146(5), 451\xe2\x80\x93479. \n\nhttps://doi.org/10.1037/bul0000220 \n\nLeBel, E. P., Berger, D., Campbell, L., & Loving, T. J. (2017). Falsifiability is not optional. \n\nJournal of Personality and Social Psychology, 113(2), 254\xe2\x80\x93261. \n\nhttps://doi.org/10.1037/pspi0000106 \n\nLoughry, M., & Amason, A. (2014). Why won\xe2\x80\x99t task conflict cooperate? Deciphering stubborn \n\nresults. International Journal of Conflict Management, 25(4), 333\xe2\x80\x93358. \n\nhttps://doi.org/10.1108/IJCMA-01-2014-0005 \n\nMarkram, H. (2012). The Human Brain Project. Scientific American, 306(6), 50\xe2\x80\x9355. JSTOR. \n\nMcNutt, M. K., Bradford, M., Drazen, J. M., Hanson, B., Howard, B., Jamieson, K. H., Kiermer, \n\nV., Marcus, E., Pope, B. K., Schekman, R., Swaminathan, S., Stang, P. J., & Verma, I. M. \n\n(2018). Transparency in authors\xe2\x80\x99 contributions and responsibilities to promote integrity in \n\nscientific publication. Proceedings of the National Academy of Sciences, 115(11), 2557\xe2\x80\x93\n\n2560. https://doi.org/10.1073/pnas.1715374115 \n\nMcPherson, M., Smith-Lovin, L., & Cook, J. M. (2001). Birds of a Feather: Homophily in Social \n\nNetworks. Annual Review of Sociology, 27(1), 415\xe2\x80\x93444. \n\nhttps://doi.org/10.1146/annurev.soc.27.1.415 \n\n \n\n\x0c29 \n\nMedin, D., Ojalehto, B., Marin, A., & Bang, M. (2017). Systems of (non-)diversity. Nature \n\nHuman Behaviour, 1(5), 0088. https://doi.org/10.1038/s41562-017-0088 \n\nMeehl, P. E. (1978). Theoretical risks and tabular asterisks: Sir Karl, Sir Ronald, and the slow \n\nprogress of soft psychology. Journal of Consulting and Clinical Psychology, 46, 806\xe2\x80\x93\n\n834. https://doi.org/10.1037/0022-006X.46.4.806 \n\nMoshontz, H., Campbell, L., Ebersole, C. R., IJzerman, H., Urry, H. L., Forscher, P. S., Grahe, J. \n\nE., McCarthy, R. J., Musser, E. D., Antfolk, J., Castille, C. M., Evans, T. R., Fiedler, S., \n\nFlake, J. K., Forero, D. A., Janssen, S. M. J., Keene, J. R., Protzko, J., Aczel, B., \xe2\x80\xa6 \n\nChartier, C. R. (2018). The Psychological Science Accelerator: Advancing Psychology \n\nThrough a Distributed Collaborative Network. Advances in Methods and Practices in \n\nPsychological Science, 1(4), 501\xe2\x80\x93515. https://doi.org/10.1177/2515245918797607 \n\nMoshontz, H., Ebersole, C. R., Weston, S. J., & Klein, R. A. (2019). A Guide for Many Authors: \n\nWriting Manuscripts in Large Collaborations [Preprint]. PsyArXiv. \n\nhttps://doi.org/10.31234/osf.io/92xhd \n\nNewell, A. (1973). You can\xe2\x80\x99t play 20 questions with nature and win: Projective comments on the \n\npapers of this symposium. Visual Information Processing, Carnegie-Mellon University. \n\nNeylon, C. (2017). Sustaining Scholarly Infrastructures through Collective Action: The Lessons \n\nthat Olson can Teach us. KULA: Knowledge Creation, Dissemination, and Preservation \n\nStudies, 1(1), 3. https://doi.org/10.5334/kula.7 \n\nObels, P., Lakens, D., Coles, N. A., Gottfried, J., & Green, S. A. (2019). Analysis of Open Data \n\nand Computational Reproducibility in Registered Reports in Psychology [Preprint]. \n\nPsyArXiv. https://doi.org/10.31234/osf.io/fk8vh \n\n \n\n\x0c30 \n\nOpen Science Collaboration. (2015). Estimating the reproducibility of psychological science. \n\nScience, 349(6251), aac4716\xe2\x80\x93aac4716. https://doi.org/10.1126/science.aac4716 \n\nOsborne, T. L., & Luoma, J. B. (2018). Overcoming a primary barrier to practice-based research: \n\nAccess to an institutional review board (IRB) for independent ethics review. \n\nPsychotherapy, 55(3), 255\xe2\x80\x93262. https://doi.org/10.1037/pst0000166 \n\nRad, M. S., Martingano, A. J., & Ginges, J. (2018). Toward a psychology of Homo sapiens: \n\nMaking psychological science more representative of the human population. Proceedings \n\nof the National Academy of Sciences, 115(45), 11401\xe2\x80\x9311405. \n\nhttps://doi.org/10.1073/pnas.1721165115 \n\nRahim, M. A. (2003). Toward a Theory of Managing Organizational Conflict. SSRN Electronic \n\nRieckmann, N. (2009). Gene-Environment Interactions and Depression. JAMA, 302(17), 1859. \n\nJournal. https://doi.org/10.2139/ssrn.437684 \n\nhttps://doi.org/10.1001/jama.2009.1578 \n\nSchneider, C. (2015). The censor\xe2\x80\x99s hand the misregulation of human-subject research. The MIT \n\nPress. \n\nSch\xc3\xb6nbrodt, F. D., Wagenmakers, E.-J., Zehetleitner, M., & Perugini, M. (2017). Sequential \n\nhypothesis testing with Bayes factors: Efficiently testing mean differences. Psychological \n\nMethods, 22(2), 322\xe2\x80\x93339. https://doi.org/10.1037/met0000061 \n\nSears, D. O. (1986). College sophomores in the laboratory: Influences of a narrow data base on \n\nsocial psychology\xe2\x80\x99s view of human nature. Journal of Personality and Social \n\nPsychology, 51(3), 515\xe2\x80\x93530. https://doi.org/10.1037/0022-3514.51.3.515 \n\nSilberzahn, R., Uhlmann, E. L., Martin, D. P., Anselmi, P., Aust, F., Awtrey, E., Bahn\xc3\xadk, \xc5\xa0., Bai, \n\nF., Bannard, C., Bonnier, E., Carlsson, R., Cheung, F., Christensen, G., Clay, R., Craig, \n\n \n\n\x0c31 \n\nM. A., Dalla Rosa, A., Dam, L., Evans, M. H., Flores Cervantes, I., \xe2\x80\xa6 Nosek, B. A. \n\n(2018). Many Analysts, One Data Set: Making Transparent How Variations in Analytic \n\nChoices Affect Results. Advances in Methods and Practices in Psychological Science, \n\n1(3), 337\xe2\x80\x93356. https://doi.org/10.1177/2515245917747646 \n\nSimons, D. J., Holcombe, A. O., & Spellman, B. A. (2014). An Introduction to Registered \n\nReplication Reports at Perspectives on Psychological Science. Perspectives on \n\nPsychological Science, 9(5), 552\xe2\x80\x93555. https://doi.org/10.1177/1745691614543974 \n\nSmaldino, P. E., & McElreath, R. (2016). The natural selection of bad science. Royal Society \n\nOpen Science, 3(9), 160384. https://doi.org/10.1098/rsos.160384 \n\nSpellman, B. A. (2015). A Short (Personal) Future History of Revolution 2.0. Perspectives on \n\nPsychological Science, 10(6), 886\xe2\x80\x93899. https://doi.org/10.1177/1745691615609918 \n\nStokols, D. (2006). Toward a Science of Transdisciplinary Action Research. American Journal \n\nof Community Psychology, 38(1\xe2\x80\x932), 79\xe2\x80\x9393. https://doi.org/10.1007/s10464-006-9060-5 \n\nStokols, D., Misra, S., Moser, R. P., Hall, K. L., & Taylor, B. K. (2008). The Ecology of Team \n\nScience. American Journal of Preventive Medicine, 35(2), S96\xe2\x80\x93S115. \n\nhttps://doi.org/10.1016/j.amepre.2008.05.003 \n\nSullivan, P. F. (2010). The Psychiatric GWAS Consortium: Big Science Comes to Psychiatry. \n\nNeuron, 68(2), 182\xe2\x80\x93186. https://doi.org/10.1016/j.neuron.2010.10.003 \n\nSyed, M., & Kathawalla, U.-K. (2020). Cultural Psychology, Diversity, and Representation in \n\nOpen Science [Preprint]. PsyArXiv. https://doi.org/10.31234/osf.io/t7hp2 \n\nTeasley, S., & Wolinsky, S. (2001). Scientific Collaborations at a Distance. Science, 292(5525), \n\n2254\xe2\x80\x932255. https://doi.org/10.1126/science.1061619 \n\n \n\n\x0c32 \n\nThalmayer, A. G., Toscanelli, C., & Arnett, J. J. (2020). The neglected 95% revisited: Is \n\nAmerican psychology becoming less American? American Psychologist. \n\nhttps://doi.org/10.1037/amp0000622 \n\nThe ATLAS collaboration, Aaboud, M., Aad, G., Abbott, B., Abdinov, O., Abeloos, B., Abidi, S. \n\nH., AbouZeid, O. S., Abraham, N. L., Abramowicz, H., Abreu, H., Abreu, R., Abulaiti, \n\nY., Acharya, B. S., Adachi, S., Adamczyk, L., Adelman, J., Adersberger, M., Adye, T., \n\n\xe2\x80\xa6 Zwalinski, L. (2018). Searches for heavy ZZ and ZW resonances in the \xe2\x84\x93\xe2\x84\x93qq and \xce\xbd\xce\xbdqq \n\nfinal states in pp collisions at s = 13 $$ \\sqrt{s}=13 $$ TeV with the ATLAS detector. \n\nJournal of High Energy Physics, 2018(3), 9. https://doi.org/10.1007/JHEP03(2018)009 \n\nTiokhin, L., Hackman, J., Munira, S., Jesmin, K., & Hruschka, D. (2019). Generalizability is not \n\noptional: Insights from a cross-cultural study of social discounting. Royal Society Open \n\nScience, 6(2), 181386. https://doi.org/10.1098/rsos.181386 \n\nTiokhin, L., Panchanathan, K., Smaldino, P. E., & Lakens, D. (2021). Shifting the level of \n\nselection in science [Preprint]. MetaArXiv. https://doi.org/10.31222/osf.io/juwck \n\nTubre, T. C., & Collins, J. M. (2000). Jackson and Schuler (1985) Revisited: A Meta-Analysis of \n\nthe Relationships Between Role Ambiguity, Role Conflict, and Job Performance. Journal \n\nof Management, 26(1), 155\xe2\x80\x93169. https://doi.org/10.1177/014920630002600104 \n\nUhlmann, E. L., Ebersole, C. R., Chartier, C. R., Errington, T. M., Kidwell, M. C., Lai, C. K., \n\nMcCarthy, R. J., Riegelman, A., Silberzahn, R., & Nosek, B. A. (2019). Scientific Utopia \n\nIII: Crowdsourcing Science. Perspectives on Psychological Science, 14(5), 711\xe2\x80\x93733. \n\nhttps://doi.org/10.1177/1745691619850561 \n\n \n\n\x0c33 \n\nVan Sell, M., Brief, A. P., & Schuler, R. S. (1981). Role Conflict and Role Ambiguity: \n\nIntegration of the Literature and Directions for Future Research. Human Relations, 34(1), \n\n43\xe2\x80\x9371. https://doi.org/10.1177/001872678103400104 \n\nVuorre, M., & Curley, J. P. (2018). Curating Research Assets: A Tutorial on the Git Version \n\nControl System. Advances in Methods and Practices in Psychological Science, 1(2), 219\xe2\x80\x93\n\n236. https://doi.org/10.1177/2515245918754826 \n\nWagge, J. R., Baciu, C., Banas, K., Nadler, J. T., Schwarz, S., Weisberg, Y., IJzerman, H., \n\nLegate, N., & Grahe, J. (2019). A Demonstration of the Collaborative Replication and \n\nEducation Project: Replication Attempts of the Red-Romance Effect. Collabra: \n\nPsychology, 5(1), 5. https://doi.org/10.1525/collabra.177 \n\nWagge, J. R., Brandt, M. J., Lazarevic, L. B., Legate, N., Christopherson, C., Wiggins, B., & \n\nGrahe, J. E. (2019). Publishing Research With Undergraduate Students via Replication \n\nWork: The Collaborative Replications and Education Project. Frontiers in Psychology, \n\n10, 247. https://doi.org/10.3389/fpsyg.2019.00247 \n\nWu, L., Wang, D., & Evans, J. A. (2019). Large teams develop and small teams disrupt science \n\nand technology. Nature, 566(7744), 378\xe2\x80\x93382. https://doi.org/10.1038/s41586-019-0941-9 \n\nYarkoni, T. (2019). The Generalizability Crisis [Preprint]. PsyArXiv. \n\nhttps://doi.org/10.31234/osf.io/jqw35 \n\nZakaria, S., Grant, J., & Luff, J. (2021). Fundamental challenges in assessing the impact of \n\nresearch infrastructure. Health Research Policy and Systems, 19(1), 119. \n\nhttps://doi.org/10.1186/s12961-021-00769-z \n\n \n\n \n\n\x0c""","Forscher et al. - The Benefits, Barriers, and Risks of Big Team Scie.pdf"
"b'Y\nT\nT\nE\nG\n\nTamarins are one of more than 40 primate species that researchers can study through the ManyPrimates collaboration.\n\nBuild up big-team science\n\nNicholas A. Coles, J. Kiley Hamlin, Lauren L. Sullivan, Timothy H. Parker & Drew Altschul\n\nResearchers are creating \ngrass-roots collaborative \nnetworks to tackle difficult \nquestions in primate studies \nand more, but they need \nfunding and other support.\n\nAre some of science\xe2\x80\x99s biggest questions \n\nsimply  unanswerable  without \nredefining how research is done? \nThis is the question that motivated \nthe  researchers  who  would  later \nestablish the ManyBabies Consortium: a \ngrass-roots network of some 450 collabora-\ntors from more than 200 institutions who \npool resources to complete massive studies \non infant development (see, for example, \nref. \xc2\xa01). Human infants are perhaps the most \npowerful learning machines on the planet \xe2\x80\x94 \nand understanding how that learning occurs \ncould inform artificial intelligence, public \npolicy, education and more. Yet a full under-\nstanding of infant learning seemed difficult \n(if not impossible) under the current research \nmodel. \n\nSpringer\n2022\n\xc2\xa9\n \n \n \n\nLimited.\nNature\n \n \n\nrights\nAll\nreserved.\n \n \n\nConsider the question of what captures \ninfants\xe2\x80\x99 attention. Surely the probability that \nan infant will pay attention to, say, a rabbit, \ndepends on presentation (for example, by \na mother or a stranger), the child\xe2\x80\x99s previ-\nous experiences with mammals, what else \nis present alongside the rabbit, and much \nmore.  Unpacking  this  effectively  would \nrequire dozens of experimental conditions \nand hundreds of infant participants. But most \nresearch projects are run by individual prin-\ncipal investigators and a shifting population \nof PhD students, meaning that data-collection \nefforts typically recruit fewer than 25 infants \nfor each condition being tested2.\n\nBut  what  if  researchers  worked  inter-\ndependently and distributed work across \nmany laboratories? Such consortia might be \n\nNature  |  Vol 601  |  27 January 2022  |  505\n\n\x0cable to answer questions that no individual \nlab could tackle alone. In a proof-of-concept \nstudy, the ManyBabies Consortium used word \nof mouth, social media and e-mail lists to amass \na team of 69 labs to test whether infants across \nseveral world regions prefer \xe2\x80\x98baby talk\xe2\x80\x99: the \nhigh-pitched, sing-song speech that adults \nin many cultures use with babies. Data from \n2,329\xc2\xa0 infants  in  16  countries  provided  a \nresounding yes, demonstrating that infants \neven prefer baby talk that is not in their native \nlanguage3. This study, the largest of its kind, \nwas cited more than 100 times within a year of \nits publication, according to Google Scholar. \nThe ManyBabies Consortium is not a one-\noff. It is part of a broader movement towards \ngrass-roots big-team science: endeavours in \nwhich an unusually large number of research-\ners \xe2\x80\x94 often dispersed across institutions \nand world regions \xe2\x80\x94 self-organize to pool \nintellectual and material resources in pur-\nsuit of a common goal4. In addition to the \nManyBabies Consortium, the authors have \ncollectively been involved in creating the \nPsychological Science Accelerator (involv-\ning some 1,200\xc2\xa0researchers)5, the Disturbance \nand Resources Across Global Grasslands net-\nwork (DRAGNet; around 100 researchers; \nhttps://dragnetglobal.weebly.com) and the \nManyPrimates project (comprising about \n150\xc2\xa0researchers6; see \xe2\x80\x98Examples of big-team \nscience\xe2\x80\x99). These self-organized consortia \npool resources to conduct massive studies in \npsychology, ecology and primatology, respec-\ntively. They perform collaborative endeav-\nours similar to those of the Human Genome \nProject and groups within CERN, Europe\xe2\x80\x99s \nparticle-physics lab near Geneva, Switzer-\nland, but have been founded without formal \nfunding  mechanisms  or  well-developed \ninfrastructure.\n\nWe have found that grass-roots big-team \nscience is capable of generating knowledge \nthat is difficult to obtain \xe2\x80\x94 but faces several \nbarriers to sustainability.\n\nBarrier 1: rewarding team players\nMichele Grigsby Coffey, a historian at the Uni-\nversity of Memphis in Tennessee, has described \nacademia as \xe2\x80\x9ca selfish sport\xe2\x80\x9d in which research-\ners \xe2\x80\x9care rewarded for self-absorbed fixations\xe2\x80\x9d, \nand in which \xe2\x80\x9cprioritizing yourself at the \nexpense of others is encouraged\xe2\x80\x9d7. Big-team \nscience, however, is a team sport that often \nrequires researchers to prioritize discovery \nover their own self-interests. For example, the \nfirst ManyPrimates study (of which D.A. is a \nco-author) examined the working-memory \ncapacity of more than 40 species of primate \nby testing whether the animals could remem-\nber the locations of hidden food after short \ntime delays8. D.A. estimates that he commit-\nted some 200 hours to the project. Yet on the \nresulting paper, the consortium is listed as the \nfirst author, the corresponding author e-mail \n\n506  |  Nature  |  Vol 601  |  27 January 2022\n\nis a shared mailbox and D.A. occupies one of \n79\xc2\xa0slots in the alphabetically sorted author \nlist. Such authorship arrangements highlight \nthe accomplishments of the team over any \nindividual.\n\nPursuing relatively selfless ideals of big-\nteam science can mean being penalized by the \nreferees of the selfish sport of academia. For \nexample, when one of us (N.A.C.) was nom-\ninated to direct the Psychological Science \nAccelerator during a postdoctoral fellowship, \na well-meaning adviser told him that it was an \nimportant role and that he was a great fit, but \nthat pursuing it would \xe2\x80\x9ckill chances of get-\nting a tenure-track position\xe2\x80\x9d. The more senior \nco-authors of this manuscript ( J.K.H., L.L.S. \nand T.H.P.) have offered junior colleagues \nsimilar warnings. For instance, they have \nseen members of hiring committees baulk \nwhen a job candidate\xe2\x80\x99s CV contains several \npapers in which their name is in the middle of \na long list of authors. A selfish sport rewards \nstars \xe2\x80\x94 not those who have crucial supporting \nroles. Indeed, when one of us in a big-team \neffort expressed excitement about a recent \nmilestone to a department head, the response \nwas: \xe2\x80\x9cGreat. Just make sure you have work \ncoming out of your own research group.\xe2\x80\x9d\n\n\xe2\x80\x9cLeading the big-team-\nscience movement can feel \nlike climbing mountains \nwithout so much as a rope.\xe2\x80\x9d\n\nAcademia  could  change  the  game  by \nrewarding researchers who make large con-\ntributions to team efforts. Otherwise, teams \nwill be forced to find other ways to increase \nbenefits or decrease the costs of participa-\ntion. For example, project leaders could fund \ncollaborators, as is being done for a collabo-\nration designing tools to predict the replica-\nbility of research findings in the social and \nbehavioural sciences, supported by the US \nDefense Advanced Research Projects Agency \n(www.cos.io/score). Such solutions, however, \nprohibit researchers with fewer resources \nfrom leading big-team science efforts. As \nan alternative, some collaborations offer \nnon-financial  perks.  For  example,  both \nDRAGNet and the Nutrient Network offer \nparticipating researchers exclusive access \nto the full project database. However, these \npolicies conflict with goals to make science \nmore open and inclusive. A reduction in \ncosts could be accomplished by recruiting \neven more researchers to split the bill, but \nthis makes coordination much more difficult.\n\nBarrier 2: diversity \nOne large potential benefit of this way of \ndoing science is the opportunity to increase \nthe diversity of participants, researchers and \n\nSpringer\n2022\n\xc2\xa9\n \n \n \n\nLimited.\nNature\n \n \n\nrights\nAll\nreserved.\n \n \n\nresearch questions. But we have noticed a \nworrisome trend: pre-existing inequality in \nscience infrastructure seems to be perpetu-\nated in big-team science.\n\nA 2021 analysis noted that researchers in \npreviously colonized countries often lack the \naccess to lab space and funding that are nec-\nessary to participate in big-team science9. Not \nsurprisingly, perhaps, these inequalities also \nseem to affect who leads these endeavours. \nNot a single behavioural-science big-team \nproject included in this analysis was led by a \nresearcher in a developing nation. Further-\nmore, the combined governing and steering \nboards of ManyBabies, the Psychological \nScience Accelerator, the Nutrient Network \nand DRAGNet include only 4 (of 32) mem-\nbers from outside North America or Western \nEurope (17\xc2\xa0are from the United States, 5\xc2\xa0from \nCanada, 6 from Western Europe, 1 from Kenya, \n1 from Argentina, 1 from Australia and 1 from \nIndia). \n\nBig-team  science  should  find  ways  to \nenable change. For example, the Psycholog-\nical Science Accelerator uses donations to \naward participation grants to researchers in \nunder-represented regions. The ManyBabies \nConsortium launched an extension of its first \nstudy that provides funding, training and \nsupport for data collection in Africa \xe2\x80\x94 an \noperation that would have been impossible \nwithout support from the Jacobs Foundation \nin Zurich, Switzerland. DRAGNet minimizes \ncosts at institutions that have few resources \nby getting them to ship seed samples for \nprocessing at better-resourced institutions. \nMany Primates fosters connections in the \nglobal south by participating in local meet-\nings and reaching the community through \npublications in languages such as Spanish \nand French. \n\nResearchers can also help to close the infra-\nstructure gap by training and supporting \nresearchers in under-represented areas. For \nexample, a big-team project testing how peo-\nple in various African regions evaluate moral \ntransgressions is led by a PhD student from \nNigeria, and is supported by several members \nof the Psychological Science Accelerator10.\n\nBarrier 3: funding and sustainability\nDespite  well-recognized  outputs,  we  all \nscramble constantly to keep our big-team \ninitiatives going. These grass-roots projects \ncan be established with little funding, but they \nare difficult to maintain without financial sup-\nport. Big-team science needs funds to retain \nresearchers who know how to coordinate the \nnext wave of science, to support tools for man-\naging increasingly complex workflows, and to \nsupport participation from researchers who \nare not well resourced. \n\nFor example, the first Psychological Science \nAccelerator  study  examined  how  people \naround the world judge others on the basis \n\n\x0cEXAMPLES OF BIG-TEAM SCIENCE\nLarge teams of researchers have come together in various ways to tackle difficult questions in science, from soil samples to cancer biology. \n\nConsortium or project name How and when organized\n\nExample of project finding or question\n\nData collection\n\nGrass-roots consortium launched by \na 2017 blogpost. Now involves some \n1,200\xc2\xa0researchers.\n\n\xe2\x80\x98Cognitive reappraisal\xe2\x80\x99 improves \nemotional reactions to the COVID-19 \npandemic.\n\nData from more than 20,000 people \nin 87 countries collected by more than \n450\xc2\xa0researchers12. \n\nPsychological Science \nAccelerator\n\nManyBabies Consortium\n\nManyPrimates project\n\nNutrient Network (NutNet)\n\nGrass-roots consortium launched by \na 2015 blogpost. Now involves around \n450\xc2\xa0researchers.\nGrass-roots consortium launched through \n2018 symposium, word of mouth, e-mail \nand social media; now involves about \n150\xc2\xa0researchers.\nLaunched in 2006 through e-mail and \nTwitter requests to join the network. \nData\xc2\xa0collection began in 2007.\n\nInfants prefer \xe2\x80\x98baby talk\xe2\x80\x99 even when it\xe2\x80\x99s \nnot in their native language.\n\nAmong 41 closely related primate \nspecies, phylogeny matters more for \nshort-term memory than do ecology or \nsocial factors. \nDoes herbivory and light availability \nresolve the loss of plant species caused \nby nutrient addition? \n\nData from 2,329 infants collected by \n150\xc2\xa0researchers in 16 countries across \nthe\xc2\xa0world3.\n81 researchers studied 421 primates8.\n\nUsing data from the broader Nutrient \nNetwork experiment (>130 collaborating \nsites), researchers documented effects \nof controlled combinations of nutrient \naddition and herbivore exclusion on plant \ndiversity at 40 sites across the globe13. \nNearly 50 researchers collected data from \n3,878 participants across 19 countries14.\n\nMany Smiles Collaboration\n\nGrass-roots effort launched in 2018. \nCollaborators formed adversarial teams \nrecruited through social media and e-mail.\n\nDoes changing facial expression affect \nemotions?\n\nReproducibility Project: \nCancer Biology\n\nLaunched in 2013 through funding provided \nby Arnold Ventures to the Center of Open \nScience and Science Exchange.\n\nCan the results of experiments from \nhigh-impact cancer-biology papers be \nreproduced? \n\n200 collaborators attempted to \nreplicate 158 effects from 50 preclinical \nexperiments15.\n\nDisturbance and Resources \nAcross Global Grasslands \nnetwork (DRAGNet)\n\nGrass-roots consortium conceived in 2018. \nData collection began in 2019; network built \nthrough e-mail and Twitter.\n\nWhen grasslands are disturbed by tilling \nand nutrient additions, how do they \nrespond?\n\nSome 90 researchers monitor 70 sites \nin 18\xc2\xa0countries (https://dragnetglobal.\nweebly.com). \n\nof facial appearance11. The project involved \n241\xc2\xa0collaborators and 11,570\xc2\xa0participants \nspanning 41\xc2\xa0countries. In principle, this study \nshould have cost hundreds of thousands of \ndollars. If participants and research assistants \nwere each paid just US$5 for every 30-minute \ndata-collection session, the cost would be \nmore than $115,000. The price tag gets much \nbigger when factoring in labour for project \nmanagement, which included acquiring more \nthan 150 ethics-approval documents, trans-\nlating study materials into 23 languages and \ndeveloping research tools to track progress \nand validate data from labs all over the world \n(see go.nature.com/3jcsutx). Yet the pro-\nject officially operated on less than $2,000; \nhundreds of collaborators donated their time \nand resources to make up the difference (see \ngo.nature.com/3qstumf). \n\nOperations that run on shoestring dona-\ntions are neither sustainable nor scalable. \nThis hard truth became apparent at the begin-\nning of 2020, when the Psychological Science \nAccelerator received 66 urgent proposals for \nglobal research projects on the psychology \nof the COVID-19 pandemic. Financial consid-\nerations meant that the network had to reject \nall but three. One rejected proposal aimed to \ntest whether reminding people to consider \naccuracy before sharing news could help to \ncurb COVID-19 misinformation in different \nworld regions and demographics. Every time \nwe see a post promoting false claims that \nthe antiparasitic drug ivermectin prevents \nCOVID-19, that pregnant women should not get \nvaccinated or that COVID-19 vaccines contain \n\nmicrochips, we are painfully reminded of the \nwork we did not have the funds to support.\n\nWhy is it so hard to get funding for grass-\nroots big-team science initiatives? Govern-\nment and philanthropic funders have provided \nvarious reasons. For instance, they worry that \nbig-team science will ultimately prove to be \nunsustainable because of academia\xe2\x80\x99s selfish \nrulebook. They say that big-team science is \nstill not diverse enough in terms of researchers \nand research questions. They say that their \nsystems are not set up to process proposals \nwith hundreds of collaborators, or to handle \nfunding requests that go out to dozens of \nresearch sites. Most frustratingly, they say that \nbig-team science has managed so far without \ntheir support.\n\nLeading the big-team-science movement \ncan sometimes feel like climbing the world\xe2\x80\x99s \ntallest mountains without so much as a rope. \nWe have caught glimpses of the peaks and can \nimagine the views they might offer, but we lack \nthe resources to climb higher. Every step for-\nward will become increasingly treacherous \nuntil  academic  institutions  and  funders \nprovide long-overdue support. \n\nThe authors\n\nNicholas A. Coles is a research scientist at \nStanford University, California, USA, and \nthe director of the Psychological Science \nAccelerator. J.\xc2\xa0Kiley Hamlin is a professor \nof psychology at the University of British \nColumbia, Vancouver, Canada, and a member \n\nof the ManyBabies Consortium Steering \nCommittee. Lauren\xc2\xa0L.\xc2\xa0Sullivan is an assistant \nprofessor of biological sciences at the \nUniversity of Missouri, Columbia, USA, and \nthe principal investigator of the Disturbance \nand Resources Across Global Grasslands \n(DRAGNet) project. Timothy H.\xc2\xa0Parker is a \nprofessor at Whitman College, Walla Walla, \nWashington, USA, and a member of the \nDisturbance and Resources Across Global \nGrasslands (DRAGNet) project, among others. \nDrew Altschul is a postdoctoral fellow at the \nUniversity of Edinburgh, UK, and a member of \nthe ManyPrimates project. \ne-mail: ncoles@stanford.edu\n\n1.  Byers-Heinlein, K. et al. Can. Psychol. Can. 61, 349\xe2\x80\x93363 \n\n(2020).\n\n2.  Oakes, L. M. Infancy 22, 436\xe2\x80\x93469 (2017).\n3.  ManyBabies Consortium. Adv. Methods Pract. Psychol. \n\nSci. 3, 24\xe2\x80\x9352 (2020).\n\n4.  Forscher, P. S. et al. Preprint at PsyArXiv https://doi.\n\norg/10.31234/osf.io/2mdxh (2020).\n\n5.  Moshontz, H. et al. Adv. Methods Pract. Psychol. Sci. 1, \n\n501\xe2\x80\x93515 (2018).\n\n6.  Many Primates et al. PLoS ONE 14, e0223675 (2019).\n7.  Grigsby Coffey, M. \xe2\x80\x98The Unselfish Academic\xe2\x80\x99 Auntie \n\nBellum Magazine (3 December 2016).\n\n8.  Many Primates et al. Preprint at PsyArXiv https://doi.\n\norg/10.31234/osf.io/5etnf (2021).\n\n9.  Silan, M. et al. Assoc. Psychol. Sci. Obs. 34 (6), 64\xe2\x80\x9369 \n\n(2021).\n\n10.  Adetula, A. et al. Preprint at AfricArXiv https://doi.\n\norg/10.31730/osf.io/hxjbu (2021).\n\n11.  Jones, B. C. et al. Nature Hum. Behav. 5, 159\xe2\x80\x93169 (2021). \n12.  Wang, K. et al. Nature Hum. Behav. 5, 1089\xe2\x80\x931110 (2021). \n13.  Borer, E. T. et al. Nature 508, 517\xe2\x80\x93520 (2014). \n14. Coles, N. A. et al. Preprint at PsyArXiv https://doi.\n\norg/10.31234/osf.io/cvpuw (2021).\n\n15.  Errington, T. M. et al. eLife 10, e71601 (2021).\n\nThe authors declare competing interests; see go.nature.\ncom/3tnjtrr for details.\n\nNature  |  Vol 601  |  27 January 2022  |  507\n\nSpringer\n2022\n\xc2\xa9\n \n \n \n\nLimited.\nNature\n \n \n\nrights\nAll\nreserved.\n \n \n\n\x0c'",Coles et al. - 2022 - Build up big-team science.pdf
"b'850561 PPSXXX10.1177/1745691619850561Uhlmann et al.Crowdsourcing Science\n\nresearch-article2019\n\nASSOCIATION FOR\nPSYCHOLOGICAL SCIENCE\n\nPerspectives on Psychological Science\n2019, Vol. 14(5) 711 \xe2\x80\x93733\n\xc2\xa9 The Author(s) 2019\n\nArticle reuse guidelines: \nsagepub.com/journals-permissions\nDOI: 10.1177/1745691619850561\nhttps://doi.org/10.1177/1745691619850561\nwww.psychologicalscience.org/PPS\n\nScientific Utopia III:  \nCrowdsourcing Science\n\nEric Luis Uhlmann1, Charles R. Ebersole2,  \nChristopher R. Chartier3, Timothy M. Errington4\nMallory C. Kidwell5, Calvin K. Lai6, Randy J. McCarthy7,  \nAmy Riegelman8, Raphael Silberzahn9, and  \nBrian A. Nosek2,4\n1Organizational Behaviour Area, INSEAD, Singapore; 2Department of Psychology, University of Virginia; \n3Department of Psychology, Ashland University; 4Center for Open Science, Charlottesville, Virginia; 5Department \nof Psychology, University of Utah; 6Department of Psychological and Brain Sciences, Washington University in \nSt. Louis; 7Center for the Study of Family Violence and Sexual Assault, Northern Illinois University; 8University \nLibraries, University of Minnesota; and 9Department of Business and Management, University of Sussex\n\n,  \n\nAbstract\nMost scientific research is conducted by small teams of investigators who together formulate hypotheses, collect data, \nconduct analyses, and report novel findings. These teams operate independently as vertically integrated silos. Here \nwe argue that scientific research that is horizontally distributed can provide substantial complementary value, aiming \nto  maximize  available  resources,  promote  inclusiveness  and  transparency,  and  increase  rigor  and  reliability.  This \nalternative approach enables researchers to tackle ambitious projects that would not be possible under the standard \nmodel. Crowdsourced scientific initiatives vary in the degree of communication between project members from largely \nindependent work curated by a coordination team to crowd collaboration on shared activities. The potential benefits \nand challenges of large-scale collaboration span the entire research process: ideation, study design, data collection, \ndata analysis, reporting, and peer review. Complementing traditional small science with crowdsourced approaches can \naccelerate the progress of science and improve the quality of scientific research.\n\nKeywords\ncrowdsourcing, collaboration, teams, methodology, metascience\n\nThere  is  no  perfect  study.  Scientists,  in  their  effort  to \nunderstand  nature,  are  constrained  by  limited  time, \nresources, and expertise. This constraint may produce a \ndilemma between choosing a lower quality, expedient \napproach or conducting a better powered, more inten-\nsive investigation allowing for stronger inferences. Ideals \nof  the  scientific  process  can  be  outweighed  by  the \npragmatic reality of scientists\xe2\x80\x99 available resources and \npursuit of career advancement. Scientists are rewarded \nfor  being  the  originators  of  new  ideas  and  evidence \nthrough the authorship of articles. These cultural incen-\ntives  foster  a  focus  on  novelty  and  authorship  that  \ncan come at the expense of rigor and foster question-\nable  practices  (Bakker,  van  Dijk,  &  Wicherts,  2012;  \nGreenland & Fontanarosa, 2012; Nosek, Spies, & Motyl, \n2012; Open Science Collaboration, 2015). One alterna-\ntive is for researchers to take more time for individual \n\nstudies, expend more resources on each project, and \npublish fewer findings. Scientists could also work more \ncollectively, combining resources across more contribu-\ntors. But such choices have implications for productiv-\nity, individual credit, and career advancement.\n\nHere  we  consider  the  standard  model  of  scientific \ninvestigation and describe a complementary model\xe2\x80\x94\ncrowdsourcing science. Crowdsourced approaches seek \nto  maximize  the  use  of  available  resources,  diversify \n\nCorresponding Authors:\nEric Luis Uhlmann, INSEAD, Organisational Behaviour Area, 1 Ayer \nRajah Ave., 138676 Singapore \nE-mail: eric.luis.uhlmann@gmail.com\n\nBrian A. Nosek, University of Virginia, Department of Psychology, Box \n400400, Charlottesville, VA 22904-4400 \nE-mail: nosek@virginia.edu\n\n\x0c712 \n\nUhlmann et al.\n\ncontributions, enable big science, and increase trans-\nparency and reliability. The adaptation of cultural norms \nand incentives to promote crowdsourcing as a comple-\nment to the standard model promises to make science \nmore rigorous and inclusive and accelerate discovery.\n\nTwo Models of Doing Science\nStandard model: vertical integration\nSome  academic  research  resembles  a  vertically  inte-\ngrated business. An individual or small research team \nconceives a research question, designs studies to inves-\ntigate  the  question,  implements  the  studies,  analyzes \nthe data, and writes a report of what was found. The \nclosed team conducts the entire process from conceiv-\ning  the  idea  to  reporting  the  outcomes.  The  team  \nmembers responsible for these steps are active collabo-\nrators  and  coauthors  on  a  manuscript  reporting  the \nresearch.  The  sought-after  reward  is  acceptance  and \npublication  in  the  most  widely  read  and  prominent \njournal possible.\n\nThis model has several notable characteristics. It is \nlocalized,  with  funding  distributed  to  particular  labs \nand institutions, and resource intensive, with the project \nwork divided among a few individuals. Access to pro-\nductive research pipelines is constrained, and experi-\nence  and  status  lead  to  opportunities  to  engage  in \nresearch collaborations (Merton, 1968). It produces a \nlarge  quantity  of  small  science  with  teams  of  limited \nsize conducting projects that are correspondingly lim-\nited in scope\xe2\x80\x94a small team can collect only so much \ndata,  carry  out  only  so  many  analyses,  and  consider \nonly so many alternatives to their methodology. Finally, \ncontribution  is  recognized  and  rewarded  through \nauthorship on the final publication.\n\nThe standard model is akin to the philosopher model \nof scholarly contribution. An independent thinker con-\nceives and generates a stand-alone piece of scholarship. \nAfter  peer  review  by  a  small  number  of  select  col-\nleagues,  that  scholarship  is  entered  into  the  market-\nplace of ideas for others to examine, discuss, critique, \nand extend. Independence in developing and enacting \nthe idea allows the scholar to dig deeply into a question \nor  idea  without  interference,  and  credit  allocation  is \nstraightforward. Scholars are evaluated on the basis of \nthe  reception  of  their  work  in  the  idea  marketplace. \nOutstanding ideas and evidence may become perma-\nnently linked to the scholar\xe2\x80\x99s identity, securing a lasting \nreputation and impact.\n\nSo what is wrong with the standard approach to sci-\nence? For many research questions and contributions, \nnothing. Independently generated contributions are an \nefficient  means  of  getting  initial  evidence  for  many \n\nideas into the marketplace. Indeed, the decentralized \nnature of science is presumed to feed the productive \ngeneration  and  culling  of  ideas  by  the  independent \nactions of scholars with different priors, assumptions, \nexpertise, and interests. Small teams often work together \nrepeatedly  and  develop  cospecializations  that  enable \ndeep dives into a methodology or phenomenon. A com-\nmunity  of  scientists  then  shares  its  work,  exchanges \nfeedback, and serially builds on each other\xe2\x80\x99s findings.\nAt the same time, for some research questions and \ncontributions, the standard model may limit progress. \nIndividual researchers and small teams must consider \ncertain trade-offs when directing their research efforts. \nThey  could  vary  design  elements  and  stimuli  instead \nof  holding  them  constant,  collect  larger  samples  for \nfewer studies instead of smaller samples for more stud-\nies, and they could replicate their findings across mul-\ntiple conditions or contexts rather than demonstrate a \nphenomenon and then move on. Researchers inevitably \nweigh  these  trade-offs  against  the  potential  rewards. \nAnd because the present culture prizes innovation and \ndiscovery  (Bakker  et\xc2\xa0 al.,  2012),  some  behaviors  that \nwould foster research credibility and cumulative prog-\nress are performed ineffectively or infrequently. Under-\nperformed  behaviors  include  collecting  large, \ncross-cultural samples to evaluate generalizability and \nestimate  effect  sizes  precisely  (Henrich,  Heine,  & \nNorenzayan, 2010), replicating findings systematically \nin independent laboratories (Klein et\xc2\xa0al., 2014; Makel, \nPlucker,  &  Hegarty,  2012;  Mueller-Langer,  Fecher,  \nHarhoff, & Wagner, 2019; Simons, 2014), obtaining sev-\neral different perspectives on how to analyze the same \ndata (Silberzahn et\xc2\xa0al., 2018), and using a wide variety \nof study designs and stimuli ( Judd, Westfall, & Kenny, \n2012; Wells & Windschitl, 1999).\n\nAlternative model: horizontal distribution\nThe alternate model\xe2\x80\x94crowdsourcing\xe2\x80\x94eschews vertical \nintegration and embraces the horizontal distribution of \nownership, resources, and expertise (Howe, 2006). In \na distributed collaboration, numerous researchers each \ncarry out specific components of a larger project, usu-\nally  under  the  direction  of  a  core  coordination  team \n(such that crowd projects are rarely perfectly horizon-\ntally distributed). Modern science is already stretching \nthe  standard  model  in  more  collaborative  directions \n(see Supplement 1 in the Supplemental Material avail-\nable online). Solo authorship is now the exception in \nmost fields. This is partly due to the diversification of \nexpertise  required  to  conduct  research  with  modern \ntools (B\xc3\xb6rner et\xc2\xa0al., 2010). Across disciplines, team size \nalmost  doubled  from  1.9  in  the  1960s  to  3.5  in  2005 \n\n\x0cCrowdsourcing Science \n\n713\n\n(Valderas et\xc2\xa0al., 2007; Wuchty, Jones, & Uzzi, 2007), and \nworking in teams is associated with greater individual \ncareer success (Kniffin & Hanks, 2018). Team-authored \narticles are more cited than solo-authored articles, and \nthis  gap  in  scholarly  impact  has  increased  over  time \n(Valderas et\xc2\xa0al., 2007; Wuchty et\xc2\xa0al., 2007).\n\nRather  than  two  qualitatively  distinct  categories  of \nresearch, the vertically integrated and horizontally dis-\ntributed approaches are better conceived as a contin-\nuum, with variation in the depth of contribution by any \ngiven individual and the number of individuals contrib-\nuting to the project. New opportunities and challenges \nemerge  when  moving  further  across  the  continuum \nfrom  singular,  independent  scholars  to  a  distributed, \ninterdependent  community.  Crowdsourcing  carefully \nselected research questions, in parallel to the necessar-\nily  far  greater  number  of  small  team  projects,  holds \nseveral potential benefits for science, among which are \nenabling the conduct of large-scale research projects, \ndemocratizing who contributes to science, and assess-\ning the robustness of findings.\n\nEnabling big science.  An inclusive, diversified contri-\nbution model enables ambitious projects that would be \nunattainable  by  individuals  or  small  teams  working  in \nisolation.  Combining  resources  enables  crowdsourced \nteams to enact research designs that vastly exceed what \ncould be accomplished locally. Instead of holding sam-\npling,  stimulus,  or  procedural  variables  constant  and \nhoping  they  do  not  matter,  crowdsourced  teams  can \nallow them to vary and test whether they do. Instead of \ncarrying  out  a  low-powered,  imprecise  test,  crowd-\nsourced teams can conduct high-powered, precise stud-\nies  and  draw  confident  conclusions.  Crowdsourcing \ncomplex activities seeks to mobilize the crowd\xe2\x80\x99s compe-\ntencies, knowledge, and skills and may leverage under-\nused resources such as a better way to analyze the data, \naccess  to  hard-to-recruit  populations,  knowledge  of \nunpublished research or articles published in other lan-\nguages,  and  translation  of  research  materials  into  local \nlanguages  and  dialects.  Crowdsourcing  flips  research \nplanning  from  \xe2\x80\x9cwhat  is  the  best  we  can  do  with  the \nresources we have to investigate our question?\xe2\x80\x9d to \xe2\x80\x9cwhat \nis the best way to investigate our question, so that we can \ndecide what resources to recruit?\xe2\x80\x9d\n\nDemocratizing  science.  Although  personal  factors \n(Clemente, 1973; Hirsch, 2007; Williamson & Cable, 2003) \nand  merit  play  a  role  in  success  in  science,  scientific \ncareers  also  exhibit  a  Matthew  effect  (Merton,  1968). \nEarly advantages in doctoral institution rank, professional \nconnections, and grant funding accumulate benefits over \ntime (Bol, De Vaan, & van de Rijt, 2018; Clauset, Arbesman,  \n&  Larremore,  2015).  Grant  funding  is  overallocated  to \n\nelite universities, and evidence suggests that returns on \ninvestment would be greater if the funds were distributed \nmore evenly (Wahls, 2018). Early-career researchers from \nless  well-known  institutions,  underrepresented  demo-\ngraphic groups, and countries that lack economic resources \nmay never have a fair chance to compete (Petersen, Jung, \nYang, & Stanley, 2011; Wahls, 2018). Academic fields are \ngenerally rich in talent, such that globally distributed proj-\nects  can  recruit  individuals  with  advanced  training  and \nmuch to offer yet too few resources to enact the vertical \nmodel competitively on their own. Few people enjoy the \nresource benefits of research-intensive institutions, includ-\ning laboratory space, professional staff to support grant \nwriting and management, graduate students, light teach-\ning loads, and a community of colleagues for developing \nideas and sharing infrastructure. Crowdsourcing aims to \nprovide  a  new  avenue  through  which  those  outside  of \nmajor research institutions can contribute to high-profile \nprojects, increasing inclusiveness, merit, and returns on \ninvestment (Chargaff, 1978; Feyerabend, 1982).\n\nAssessing the robustness of findings.  A crowdsour ced \napproach is uniquely advantaged in determining the\xc2\xa0reli-\nability and generalizability of findings. The ecosystem of \nstandard science leads to the publication of massive num-\nbers of small-sample studies (Pan, Petersen, Pammolli, & \nFortunato, 2016), each with observations typically drawn \nfrom a single population (e.g., undergraduates from the \nresearchers\xe2\x80\x99  home  institution  in  the  case  of  behavioral \nexperiments; Sears, 1986). Combined with the filter of an \nacademic  review  process  that  primarily  permits  statisti-\ncally significant results to appear in the published record \n(Fanelli, 2010), the end result is a research literature filled \nwith inaccurately estimated effect sizes as a result of pub-\nlication bias (Ioannidis, 2005, 2008). The standard approach \nto  science  is  also  susceptible  to  issues  such  as  study \ndesigns  generated  from  a  single  theoretical  perspective \n(Monin, Pizarro, & Beer, 2007), unconsidered cultural dif-\nferences (Henrich et\xc2\xa0al., 2010), and researcher degrees of \nfreedom in data analysis (Gelman & Loken, 2014; Simmons, \nNelson,  &  Simonsohn,  2011).  Large-scale  collaboration \nhelped transform epi demiology into a more reliable field \n(Ioannidis, Tarone, & McLaughlin, 2011; Panagiotou, Willer, \nHirschhorn,  &  Ioannidis,  2013),  and  this  process  is  cur-\nrently under way in psychology and other scientific disci-\nplines. Multilab collaborations facilitate directly replicating \nfindings (same materials and methods, new observations; \nEbersole et\xc2\xa0al., 2016; Klein et\xc2\xa0al., 2014) and conceptually \nreplicating them (new approach to testing the same idea; \nLandy et\xc2\xa0al., 2018). Crowdsourcing research is a part of a \nchanging  landscape  of  science  that  seeks  to  improve \nresearch  reliability  and  advance  the  credibility  of  academic \nresearch (LeBel, McCarthy, Earp, Elson, & Vanpaemel, 2018; \nNosek et\xc2\xa0al., 2012).\n\n\x0c714 \n\nUhlmann et al.\n\nAt  the  same  time,  there  are  opportunity  costs  and \ndiminishing returns involved in organizing many labo-\nratories  to  carry  out  a  single  scientific  investigation. \nOrganizing a collective for a globally distributed project \ncan create bureaucracy and transaction costs. For the \nsame effort, a larger number of ideas with initial sup-\nporting evidence could have been introduced into the \nliterature by smaller teams working separately. Crowd-\nsourcing  allows  for  systematically  examining  cross-\npopulation variability, but it is important to begin by \nmaking sure the effect emerges reliably in at least one \nlocation. It will often be beneficial to rely on research \nfrom small teams for these reasons, especially when it \ncomes  to  new  areas  of  inquiry.  Crowd  projects  with \ndozens or even hundreds of authors also create credit \nambiguity and lack extrinsic incentives for participation, \ntopics we address in depth later when we discuss struc-\ntural reforms to encourage greater crowdsourcing. We \nbelieve the two models should coexist, with individual \ninvestigators  and  small  teams  generating  initial  evi-\ndence  for  new  ideas  and  crowdsourced  initiatives \nimplemented to select particularly critical questions for \nintense examination. A diverse array of scientific proj-\nects,  everywhere  along  the  continuum  from  lone \nresearchers to huge collectives, may produce the great-\nest  return  of  useful  knowledge  from  the  resources \ninvested. The remainder of this article discusses circum-\nstances in which crowdsourcing offers particular oppor-\ntunities and challenges as a complement to the standard \nmodel.\n\nForms of Scientific Crowdsourcing\nRather than supplanting the standard approach, orga-\nnizing  many  individuals  and  laboratories  into  shared \nprojects  seeks  to  offset  some  of  the  weaknesses  of \nvertically integrated science. Crowd initiatives vary on \nmultiple  dimensions  that  can  create  advantages  and \ndisadvantages depending on the research application \n(Lakhani, Jeppesen, Lohse, & Panetta, 2007; Muffatto, \n2006;  Salganik,  2017;  Srinarayan,  Sugumaran,  &  \nRajagopalan,  2002;  Surowiecki,  2005).  For  example, \ncrowdsourced projects vary in terms of the degree of \ncommunication between project members, from largely \nindependent work curated by a coordination team to \ncrowd collaboration on shared activities. Crowd-science \ninitiatives also vary in their inclusivity, from open calls \nfor  collaborators  to  carefully  chosen  groups  of  topic \nexperts.\n\nFigure 1 crosses the horizontal dimension of com-\nmunication (anchored at the left end by curated con-\ntributions and at the right by crowd collaboration) with \nthe vertical dimension of selectivity to create a 2 \xc3\x97 2 \nmatrix. Examples of relevant crowdsourced projects are \n\nplaced in this matrix as illustrations. These projects are \ndescribed in greater detail in the next section and in \nTables 1 and 2 (see also Supplements 1 and 2 in the \nSupplemental Material). Citizen-science initiatives that \ninclude anyone willing to collect data involve a high \ndegree of independence between actors and thus fall \ninto  the  bottom-left  quadrant  (Gura,  2013).  Posing  a \nresearch question to specialists (e.g., moral-judgment \nresearchers) and asking them to independently design \nstudies to test the same idea falls into the top-left quad-\nrant  (Landy  et\xc2\xa0 al.,  2018).  Iterative  contests  in  which \ntopic experts work together to improve experimental \ninterventions (Lai et\xc2\xa0al., 2014) and the collective devel-\nopment of open-source software (Muffatto, 2006) are \nin the top-right quadrant, and more inclusive forms of \ncrowd writing (Christensen & van Bever, 2014) are in \nthe bottom-right quadrant. Open peer review, in which \nanyone  can  publicly  comment  on  a  scientific  manu-\nscript  or  article,  falls  into  the  bottom-right  quadrant, \nand  crowd  review  by  experts  carefully  chosen  by  a \njournal editor falls into the top-right quadrant. Tradi-\ntional small-team research, with unrestricted commu-\nnication  and  select  membership,  falls  outside  the \nextreme top-right corner of the matrix at the far end of \nboth axes.\n\nMultistage projects may operate in different locations \nin this space during the research life cycle. For example, \nto explore consensus building about disparate findings \nfrom the same data set, Silberzahn et\xc2\xa0al. (2018) segued \nfrom isolated individual work to round-robin feedback \nand  then  open-group  debate.  Indeed,  much  crowd-\nsourced science moves gradually from left to right on \nthe communication dimension over the life course of \nthe project, culminating in collective e-mail exchanges \nand  editing  of  the  manuscript  draft.  Likewise,  crowd \nprojects tend to rely more on selective expertise over \ntime (i.e., move up the vertical axis), as project coor-\ndinators and specialized subteams of statistical experts \ncheck the collective work for errors and play leading \nroles in producing the final report.\n\nOn the vertical dimension, greater inclusivity facili-\ntates  scaling  up  for  massive  initiatives.  In  contrast, \nselectivity  in  project  membership  prioritizes  specific \nareas of expertise for contribution. It is not yet clear \nunder what conditions involving large crowds of con-\ntributors (i.e., moving downward on the vertical axis) \ncompromises overall project quality relative to applying \nmild  or  strong  selectivity  standards  for  contribution \n(Budescu & Chen, 2015; Mannes, Soll, & Larrick, 2014). \nResearch  done  by  lone  scientists  and  small  teams  is \nalready known to be prone to error (Bakker & Wicherts, \n2011; Berle & Starcevic, 2007; Garcia-Berthou & Alcaraz, \n2004; Salter et\xc2\xa0al., 2014; Westra et\xc2\xa0al., 2011), and the \nquality-quantity trade-off that can accompany scaling \n\n\x0cCrowdsourcing Science \n\n715\n\nSelective Projects, Low Communication\n\nSelective Projects, High Communication\n\nSelect Experts Only\n\n\xe2\x80\xa2  Crowdsourcing Designs of Experiments\n\xe2\x80\xa2  Prepublication Independent Replication\n\xe2\x80\xa2  Crowd Replication Initiatives (e.g., RP:P) \n\xe2\x80\xa2  Crowdsourcing Data Analysis\n\xe2\x80\xa2  Solution Contests\n\n\xe2\x80\xa2  Coordinated Analyses\n\xe2\x80\xa2  Peer Review by Select Crowd of Experts\n\xe2\x80\xa2  Intervention Contests\n\xe2\x80\xa2  Assembling Resources Using Online Platforms \n    (e.g., StudySwap)\n\xe2\x80\xa2  Polymath Projects\n\xe2\x80\xa2  Open-Source Software Development\n\nInclusive Projects, Low Communication\n\nInclusive Projects, High Communication\n\n\xe2\x80\xa2  Prediction and Decision Markets for Scienti\xef\xac\x81c \n    Results\n\xe2\x80\xa2  Leveraging Class Projects to Conduct \n    Replications (e.g., CREP)\n\xe2\x80\xa2  Citizen Science\n\n\xe2\x80\xa2  Crowdsourced Generation and Selection of\n    Ideas\n\xe2\x80\xa2  Crowd Writing\n\xe2\x80\xa2  Open Peer Review\n\ny\nt\ni\nv\ni\nt\nc\ne\ne\nS\n\nl\n\n \n.\ns\nv\n \ns\ns\ne\nn\ne\nv\ni\ns\nu\nc\nn\n\nl\n\nI\n\nCurated \n\nContributions\n\nCrowd \n\nCollaborations\n\nDegree of Communication Between Project Members\n\nOpen to Anyone\n\nFig. 1.  Forms and examples of crowdsourcing. Curated contributions refers to projects in which project coordinators collect the indi-\nvidual work of a crowd of contributors whose communication with one another is limited to nonexistent. Crowd collaborations refers \nto projects in which a large group of contributors engage in regular communication regarding their shared work. CREP = Collaborative \nReplication and Education Project; RP:P = Reproducibility Project: Psychology.\n\nup is potentially offset by the numerous eyes available \nto  catch  mistakes  (e.g.,  Silberzahn  et\xc2\xa0 al.,  2018).  The \navailable evidence suggests that data collected by citi-\nzen scientists are comparable in error rates and general \nquality to those assembled by professionals (Kosmala, \nWiggins, Swanson, & Simmons, 2016; Thelen & Thiet, \n\n2008). Online coders and political scientists reach near-\nperfect agreement on policy positions in political mani-\nfestos (Benoit, Conway, Lauderdale, Laver, & Mikhaylov, \n2016), Wikipedia entries are as accurate as the Ency-\nclopedia  Britannica  (Giles,  2005),  highly  published \nand  less  prolific  researchers  are  similarly  likely  to \n\nTable 1.  Crowdsourcing Different Stages of the Research Process\n\nStudy design\n\nThe same research hypothesis is given to different scientists, who independently design \n\nStage of research\n\nIdeation\nAssembling resources\n\nData collection\nData analysis\nReplicating findings before \n\npublication\n\nWriting research reports\nPeer review\nReplicating published findings\n\nHow crowds are leveraged\n\nCrowds are used to generate novel research ideas and solutions to problems\nOnline exchanges are used to match investigators with needs with partner laboratories who \n\nhave that resource\n\nstudies to test it\n\nits publication\n\nNumerous collaborators aid in obtaining research participants, observations, or samples\nA network of researchers carries out statistical analyses to address the same research question\nThe same methodology is repeated in independent laboratories to confirm the finding before \n\nA large group of contributors collectively writes a research article\nA large group of commentators writes public feedback on a scientific article\nThe same methods and materials from published articles are repeated in independent \n\nlaboratories to assess the robustness of the findings\n\nDeciding future directions\n\nCrowd predictions about future research outcomes are factored into decisions about how to \n\nallocate research resources for maximum impact\n\n\x0cUhlmann et al.\n\n716 \n\nSource\n\net\xc2\xa0al. (2018)\n\nInnoCentive.com\n\nTable 2.  Examples of Crowdsourced Scientific Initiatives\n\nSobel (2007)\n\nStarting in 1714, the British Parliament \n\nDevelopment of the marine chronometer\n\nMethod\n\nIdeation\n\nKey result(s)\n\nPolymath (2012, 2014)\n\nMathematical challenges are posted online for \n\nA new combinatorial proof to the density \n\nSchweinsberg, Feldman, \n\nCrowd of researchers asked to nominate \n\nThe crowd was able to generate interesting \n\nlaunched an open competition to solve how \nto calculate the longitude of a ship at sea\n\nopen crowd collaboration\n\nhypotheses for testing with a complex data \nset\n\nScientific problems are posted online, and \n\nprizes are offered for the best solution\n\nAssembling resources\n\nidentify and outsource specific research \nneeds\n\nresources available for use by others or \nneeded resources another researcher may \nhave\n\nStudy design\n\nexperiments to test the same hypothesis; \nresearch participants are then randomly \nassigned to different study versions\n\nversion of the Hales-Jewett theorem, among \nother solved mathematical problems\n\nhypotheses for later testing\n\n30% of 166 scientific problems solved via crowd \n\ncompetitions for prizes\n\npartnership with the Center for Open Science \nto conduct the Reproducibility Project: \nCancer Biology\n\nand small team projects\n\nDifferent study designs associated with widely \ndispersed effect-size estimates for the same \nresearch question; for four out of five \nhypotheses examined, the materials from \ndifferent teams returned significant effects in \nopposite directions\n\nScience Exchange\n\nOnline marketplace that enables scientists to \n\nProgram to independently validate antibodies; \n\nStudySwap\n\nPlatform for posting brief descriptions of \n\nUsed to gather resources for both crowdsourced \n\nLandy et\xc2\xa0al. (2018)\n\nIndependent research teams separately design \n\nData collection\n\nOlmstead (1834)\n\nIn 1833, Denison Olmsted used letter \n\nDetailed documentation of the great meteor \n\ncorrespondence to recruit citizen scientists \nto help document a meteor shower\n\nstorm of 1833; birth of citizen-science \nmovement\n\nKanefsky, Barlow, and \n\nClickworkers website from the National \n\nMapping of craters on Mars based on images \n\nGulick (2001)\n\nAeronautics and Space Administration asks \nvolunteers to help classify images\n\nfrom the Viking Orbiter\n\nChurch (2005)\n\nThe Personal Genome Project recruits \n\neveryday people willing to publicly share \ntheir personal genome, health, and trait data \nas a public-research resource\n\nCollection of data from 10,000 volunteers; full \nanalyses of the genomes of 56 participants \nwith identification of potential health impacts \nin 25% of cases; ongoing project to link \ngenetics, memory, and attention\n\nCooper et\xc2\xa0al. (2010)\n\nOnline game FoldIt in which more than 50,000 \n\nThe best human players outperform a computer \n\nPrice, Turner, Stencel, \nKloppenborg, and \nHenden (2012)\n\nKim et\xc2\xa0al. (2014)\n\nplayers compete to fold proteins\nCitizen sky project recruits amateur \n\nastronomers to help professionals gather \nobservations of the planets, moons, \nmeteors, comets, stars, and galaxies\nVideo game EyeWire in which players \n\nreconstruct part of an eye cell using three-\ndimensional images of microscopic bits of \nretinal tissue\n\nin terms of determining protein structures\n\nGathering observations of Epsilon Aurigae, an \nunusual multiple star system, among other \ntargets\n\nData from more than 2,000 elite gamers used \n\nto collectively map neural connections in the \nretina, contributing to a better understanding \nof how the eye detects motion\n\nMetaSUB International \n\nConsortium (2016)\n\nCommuters are enlisted to obtain samples \n\nIdentification of new species and novel \n\nfrom surfaces in subways and other public \nareas\n\nbiosynthetic gene clusters; global maps of \nantimicrobial resistance markers\n\n(continued)\n\n\x0cCrowdsourcing Science \n\nTable 2.  (Continued)\n\nSource\n\nS\xc3\xb8rensen et\xc2\xa0al. (2016)\n\nVideo game Quantum Moves in which the \n\nMoshontz et\xc2\xa0al. (2018)\n\nPsychological Science Accelerator (PSA), a \n\nplayer moves digital renditions of quantum \natoms\n\nnetwork of more than 300 laboratories to \nconduct replications and collect other data \nfor crowdsourced projects\n\nOnline platform where citizen volunteers assist \n\nprofessional researchers with projects\n\nZooniverse\n\nMethod\n\nKey result(s)\n\nGalaxy Zoo\n\nAsks volunteers to help classify galaxies on \n\nCollection of more than 100 million \n\nthe basis of images\n\n717\n\nThe data produced by the more than 200,000 \nusers has been leveraged to develop better \nquantum algorithms\n\nThe first large-scale PSA project will seek to \nreplicate earlier findings that people rate \nfaces on the basis of valence and dominance\n\nEnables citizen-science initiatives such as \n\xe2\x80\x9cMapping Prejudice,\xe2\x80\x9d in which project \nvolunteers identify racially restrictive property \ndeeds\n\nclassifications of galaxies based on \nshape, structure, and intensity; identifying \nsupernovas and potential interactions \nbetween galaxies\n\nLarge data set on bird migrations leveraged for \n\nscientific publications\n\nImproved prediction of survival of breast-cancer \npatients, drug sensitivity in breast-cancer cell \nlines, and biomarkers for early-Alzheimer\xe2\x80\x99s \ndisease cognitive decay\n\nChanges in physical activity over time affect \n\ncognitive function; education may not be a \nprotective factor against cognitive decline\n\nAudubon Christmas Bird \n\nCount\n\nStolovitzky, Monroe, and \n\nCalifano (2007)\n\nHofer and Piccinin (2009)\n\nBeginning with the Audubon Christmas Bird \nCount of 1900, amateur birdwatchers have \nbeen used to collect data on bird migrations\n\nData analysis\nIn the Dialogue for Reverse Engineering \nAssessments and Methods Challenges, \norganizers provide a test data set and a \nparticular question to be addressed to many \nindependent analysts and then apply the \nanalytic strategies to a hold-out data set to \nevaluate their robustness\n\nCoordinated analysis: network of researchers \nuse the same target constructs, model, and \ncovariates on different longitudinal data sets \nto address the same research question\n\nSchweinsberg, Feldman, \n\n42 analysts were asked to test hypotheses \n\nRadical effect-size dispersion, with analysts in \n\net\xc2\xa0al. (2018)\n\nrelated to gender, status, and science using \na complex data set on academic debates\n\nsome cases reporting significant effects in \nopposite directions for the same hypothesis \ntested with the same data\n\nSilberzahn et\xc2\xa0al. (2018)\n\nThe same data set was distributed to 29 analysis \nteams, who separately analyzed it to address \nthe same research question (\xe2\x80\x9cDo soccer \nreferees give more red cards to dark skin \ntoned players than light skin toned players?\xe2\x80\x9d)\n\nEffect-size estimates ranging from slightly \n\nnegative to large positive effects; 69% of \nanalysts reported statistically significant \nsupport for the hypothesis, and 31% reported \nnonsignificant results\n\nSchweinsberg et\xc2\xa0al. (2016)\n\n25 independent laboratories attempted to \n\n6 of 10 findings were robust and generalizable \n\nreplicate 10 unpublished findings from one \nresearch group\n\nacross cultures according to the preregistered \nreplication criteria\n\nReplicating findings before publication\n\nChristensen and van Bever \n\nOnline collaboration platform used to \n\nThe article \xe2\x80\x9cThe Capitalist\xe2\x80\x99s Dilemma,\xe2\x80\x9d which \n\n(2014)\n\nWriting research reports\n\ncollect ideas and comments regarding \nwhy companies often do not invest in \ninnovations that create new markets\n\nargues this occurs because companies \nincentivize their managers to find efficiency \ninnovations that eliminate jobs and pay off \nfast, rather than market innovations that pay \noff years later\n\n(continued)\n\n\x0cUhlmann et al.\n\nTable 2.  (Continued)\n\n718 \n\nSource\n\nList (2017)\n\nMethod\n\nKey result(s)\n\nPeer review\nSynlett implemented a crowdsourced \n\nreviewing process to allow more than \n100 referees to respond to articles after \nthey were posted to an online forum for \nreviewers\n\nReplicating published findings\n\nThe crowd review was faster and provided \nmore comprehensive feedback than the \ntraditional peer-review process\n\nSteward, Popovich, Dietrich, \n\nand Kleitman (2012)\n\nAlogna et\xc2\xa0al. (2014)\n\nInitiative to replicate spinal-cord-injury \nresearch in independent laboratories\n\nstudies\n\n2 successful replications out of 12 targeted \n\nRegistered Replication Report: attempt by \n\nVerbal overshadowing successfully replicated, \n\nmany laboratories to replicate the verbal \novershadowing effect\n\nbut with a smaller effect size than in the \noriginal article\n\nKlein et\xc2\xa0al. (2014)\n\nMany Labs 1: 36 laboratories attempted to \n\n10 of 13 findings replicated\n\nOpen Science Collaboration \n\nReproducibility project that attempted to \n\n36% of findings successfully replicated\n\n(2015)\n\nCamerer et\xc2\xa0al. (2016)\n\n61% of findings successfully replicated\n\nEbersole et\xc2\xa0al. (2016)\n\nMany Labs 3: 20 laboratories attempted to \n\n3 of 10 findings replicated; most unaffected by \n\nreplicate 13 psychology findings\n\nreplicate 97 original effects from top \npsychology journals in independent \nlaboratories\n\nExperimental Economics Replication Project: \ninitiative to replicate prominent findings in \nexperimental economics in independent \nlaboratories\n\nreplicate 10 psychology findings at different \ntimes of the semester\n\nRegistered Replication Report: attempt by \n\nmany laboratories to replicate the effects of \npriming hostility on impression formation\nReproducibility Project: Cancer Biology: an \n\ninitiative to replicate prominent findings in \ncancer biology\n\nto replicate 21 social-science findings in \nScience and Nature\n\nMany Labs 2: 28 psychology findings \n\nreplicated across 125 sites\n\nInitiative to replicate prominent findings in \nexperimental philosophy in independent \nlaboratories\n\nRegistered Replication Report: attempt by \nmany laboratories to replicate the effect \nof priming professors on intellectual \nperformance\n\nCollaborative Replications and Education \n\nProject initiative to replicate social-\npsychology findings in student methods \nclasses\n\nDeciding future directions\n\nMcCarthy et\xc2\xa0al. (2018)\n\nNosek and Errington (2017)\n\nKlein et\xc2\xa0al. (2018)\n\nCova et\xc2\xa0al. (2018)\n\nO\xe2\x80\x99Donnell et\xc2\xa0al. (2018)\n\nWagge et\xc2\xa0al. (2019)\n\ntime of semester\n\nFailure to replicate the hostility priming effect, \nwith low heterogeneity in effect sizes across \nlaboratories\n\nOf 12 replications thus far, 4 reproduced \nimportant parts of the original article, 4 \nreplicated some parts of the original article \nbut not others, 2 were not interpretable, and \n2 did not replicate the original findings\n\n14 of 28 findings replicated; heterogeneity in \neffect-size estimates was highest for large \neffect sizes and low for nonreplicable effects\n\n78% of findings successfully replicated\n\nFailure to replicate the professor priming effect, \nwith low heterogeneity in effect sizes across \nlaboratories\n\nFailure to replicate earlier findings that women \n\nare more attracted to men in photographs \nwith red borders\n\nCamerer et\xc2\xa0al. (2018)\n\nSocial Sciences Replication Project: an initiative \n\n13 (62%) of findings successfully replicated\n\nDreber et\xc2\xa0al. (2015)\n\nPrediction market to see whether independent \n\nAggregated predictions accurately anticipated \n\nscientists could forecast the results of the \nReproducibility Project: Psychology\n\nreplication results\n\n(continued)\n\n\x0c719\n\nCrowdsourcing Science \n\nTable 2.  (Continued)\n\nSource\n\nCamerer et\xc2\xa0al. (2016)\n\n(2018a)\n\nEitan et\xc2\xa0al. (2018)\n\nCamerer et\xc2\xa0al. (2018)\n\n(2018b)\n\nMethod\n\nKey result(s)\n\nPrediction market to see whether independent \nscientists could forecast replication results in \nexperimental economics\n\nAggregated predictions accurately anticipated \n\nreplication results\n\nDellaVigna and Pope \n\nPrediction survey to see whether forecasters \n\nAggregated predictions anticipated research \n\nLandy et\xc2\xa0al. (2018)\n\nPrediction survey to see whether independent \n\nAggregated predictions accurately anticipated \n\ncould anticipate the effects of treatment \nconditions on worker productivity\n\nPrediction survey to see whether scientists \ncould forecast the size of political biases \nin scientific abstracts and to gauge their \nreactions to the research results\n\nscientists could predict the results of \nconceptual replications\n\nPrediction market to see whether independent \nscientists could forecast results replications \nof social-science articles in Science and \nNature\n\ncould anticipate the effects of treatment \nconditions on worker productivity as \nwell as moderation by their demographic \ncharacteristics\n\noutcomes; expert behavioral scientists, \ndoctoral students, and Mechanical Turk \nworkers similarly accurate\n\nForecasters accurately predicted that \n\nconservatives would be explained more, \nand explained in more negative terms, in \nscientific abstracts in social psychology; they \nalso significantly overestimated the size of \nboth effects but updated their beliefs in light \nof the new evidence\n\noverall outcomes, including variability in \nresults across different study designs testing \nthe same hypothesis\n\nAggregated predictions accurately anticipated \n\nreplication results\n\neffects but overestimated the importance of \ndemographic moderators; academic seniority \ndid not moderate forecasting accuracy\n\nDellaVigna and Pope \n\nPrediction survey to see whether forecasters \n\nAggregated predictions anticipated treatment \n\nForsell et\xc2\xa0al. (2018)\n\nPrediction market to see whether independent \n\nAggregated predictions accurately anticipated \n\nscientists could predict the results of the \nMany Labs 2 replication initiative\n\nreplication results\n\nLai et\xc2\xa0al. (2014)\n\nContest to identify the most effective \n\n8 of 17 interventions effective in the short term \n\nintervention to reduce implicit preferences \nfor Whites over Blacks\n\nbut none effective a day or more after the \nintervention; teams were able to iteratively \nimprove their interventions between rounds.\n\nsuccessfully replicate a given behavioral effect (Bench, \nRivera, Schlegel, Hicks, & Lench, 2017; see also Klein, \nVianello,  Hasselman,  &  Nosek,  2018),  and  crowds  of \ninvestigators do not exhibit measurably different \xe2\x80\x9cflair\xe2\x80\x9d \nat  designing  studies  that  obtain  significant  findings \n(Landy et\xc2\xa0al., 2018).\n\nThese  null  findings  are  surprising\xe2\x80\x94there  must  be \nsome point at which a crowd project becomes overly \ninclusive  and  insufficiently  expert  members  compro-\nmise overall quality. One possibility is that coordinators \nof the crowd projects thus far have chosen the degree \nof inclusiveness and communication best suited to their \nresearch question (i.e., the correct location in Fig. 1), \nleading to judicious scaling without losses in quality. \nLogically,  only  individuals  with  specialized  training \n(e.g., with physiological equipment) would be recruited \nto collect data for certain projects (e.g., pooling data \n\nfrom fMRI across laboratories; top-left quadrant of Fig. \n1). Even with an open call, potential contributors may \nvolunteer  for  projects  in  which  they  feel  they  can \nadd\xc2\xa0 value  (e.g.,  an  avid  bird  watcher  volunteers  to \nhelp\xc2\xa0track migrations), leading to self-screening based \non  relevant  skill  sets.  Testing  the  conditions  under \nwhich crowdsourcing increases and decreases project \nquality will inform future investments in crowdsourced \nresearch.\n\nIn contrast, there is little direct evidence regarding \nthe  consequences  of  information  exchange  between \nproject members in crowdsourced scientific initiatives. \nNevertheless,  potential  costs  and  benefits  of  crowd \ncommunication are suggested by the literature on group \ninfluence and decision making. One of the virtues of \ncrowds of independent agents, especially demographi-\ncally and intellectually diverse ones, is their tendency \n\n\x0c720 \n\nUhlmann et al.\n\nto  balance  out  individual  biases  and  errors  in  the  \naggregate (Galton, 1907; Larrick, Mannes, & Soll, 2012;  \nSurowiecki, 2005). Crowdsourcing scientific investiga-\ntions with little to no communication between project \nmembers (i.e., the far-left regions of Fig. 1) may help \nto  avoid  the  potentially  biasing  effect  of  individuals\xe2\x80\x99 \novercommitment  to  intellectual  claims  (Berman  & \nReich, 2010; Luborsky et\xc2\xa0al., 1999; Manzoli et\xc2\xa0al., 2014; \nMynatta, Dohertya, & Tweneya, 1977) and path depen-\ndencies in which knowledge of others\xe2\x80\x99 approaches has \nan  inordinate  influence  (Derex  &  Boyd,  2016).  The \neffectiveness of crowds is more difficult to evaluate in \nsituations  that  lack  normatively  correct  answers  or \nobjective  measures  of  accuracy.  Yet  even  then,  the \ndiversity in approaches and results on the part of inde-\npendent scientists, for example in analytic choices and \nstudy designs, is at least made transparent to the reader \n(Landy et\xc2\xa0al., 2018; Silberzahn et\xc2\xa0al., 2018).\n\nThat  the \xe2\x80\x9cwisdom  of  the  crowd\xe2\x80\x9d  effect  is  spoiled \nwhen peer influence between members of the crowd \nis  possible  (Lorenz,  Rauhut,  Schweitzer,  &  Helbing, \n2011) suggests that the more one moves toward crowd \ncollaborations  (i.e.,  right  on  the  horizontal  axis),  the \nmore  conformity  and  deference  to  authority  become \nrisks. The one crowdsourced project that has tracked \nindividual beliefs under conditions of gradually increas-\ning  communication  found  little  evidence  of  conver-\ngence over time, beyond what would be expected from \nsensitivity  to  new  evidence  (see  Fig.  4  in  Silberzahn \net\xc2\xa0al., 2018). The circumstances under which conformity \neffects occur in crowd science remains an open empiri-\ncal  question,  and  future  projects  should  consider \nmanipulating factors such as task interdependence and \nanonymity of communications.\n\nAllowing information exchange and creating inter-\ndependencies  between  project  members  also  comes \nwith potential important benefits. One of the hypoth-\nesized benefits of crowd collaboration is the ability of \nmembers of the community to learn from each other \n(Wenger,  1998).  For  example,  teams  in  the  Lai  et\xc2\xa0 al. \n(2014) intervention contest observed the effectiveness \nof others\xe2\x80\x99 interventions between rounds and used those \ninsights to improve their own interventions. Likewise, \nthe  round-robin  feedback  between  different  analytic \nteams  in  the  crowdsourcing  data-analysis  initiative  \n(Silberzahn et\xc2\xa0al., 2018) helped several analysts to iden-\ntify  clear  errors  and  adopt  improved  specifications. \nThese are only anecdotal examples, and further research \nis needed to examine when peer learning occurs sys-\ntematically in iterative, multistage crowd collaborations \nand how it might best be facilitated. As reviewed next, \nevidence  of  the  viability  of  crowdsourcing  across  all \nstages of the research process has accumulated rapidly \nin recent years.\n\nCrowdsourcing Science in Action\nScience can benefit from crowdsourcing activities that \nspan the entire research process (see Table 1). These \ninclude coming up with research ideas, assembling the \nresearch team, designing the study, collecting and ana-\nlyzing the data, replicating the results, writing the arti-\ncle,  obtaining  reviewer  feedback,  and  deciding  next \nsteps for the program of research. Table 2 and Supple-\nment 2 in the Supplemental Material summarize some \nrecent crowdsourced scientific initiatives, organized by \nthe respective stages on which they focused their crowd \nefforts.\n\nIdeation\nCrowds  of  scientists  can  be  organized  to  collaborate \nvirtually on complex problem-solving challenges, each \nproposing ideas for solving components of the problem \nand  commenting  on  each  other\xe2\x80\x99s  suggestions  (open \ncommunication;  the  far-right  regions  of  Fig.  1).  This \napproach has been used to great effect in the Polymath \nprojects,  resulting  in  several  important  mathematical \nproofs (Ball, 2014; Polymath, 2012, 2014; Tao, Croot, & \nHelfgott,  2012).  Like  how  they  are  used  in  product-\ndesign  contests  (Poetz  &  Schreier,  2012),  crowds  of \nresearchers  can  also  be  used  to  generate  original \nresearch hypotheses and select which ideas are most \nlikely  to  be  of  broad  interest  and  impact  ( Jia  et\xc2\xa0 al., \n2018;  Schweinsberg,  Feldman,  et\xc2\xa0 al.,  2018).  This \napproach may be particularly useful when it comes to \ndata  sets  that  for  legal  or  ethical  reasons  cannot  be \npublicly posted or further distributed\xe2\x80\x94for instance, the \npersonnel records of a private firm, who might agree \nto share them with one research team or institution but \nnot  for  general  distribution.  Even  in  such  cases,  the \ncore coordination team who serves as custodians of the \ndata can post an overview of the variables and sample \nonline and publicly solicit ideas for testing ( Jia et\xc2\xa0al., \n2018). The crowdsourced generation and selection of \nresearch  ideas  is  one  way  to  open  up  data  sets  and \ncollaboration  opportunities  that  would  otherwise \nremain closed to most scientists.\n\nAssembling resources\nGenome-wide association studies distribute the task of \ninvestigating the entire genome across many collabora-\ntors and institutions with specialized roles, leading to \nimportant discoveries related to genes and pathways of \ncommon diseases (Visscher, Brown, McCarthy, & Yang, \n2012). Consider the innumerable lost opportunities for \nsimilarly  combining  resources  across  laboratories  in \nother scientific fields. For instance, a researcher at one \n\n\x0cCrowdsourcing Science \n\n721\n\ninstitution may have a great idea but lacks access to the \nright equipment or sample of subjects to test it. Else-\nwhere,  another  team  finds  they  have  an  excess  of \nresearch resources (e.g., they compensate participants \nfor  a  30-min  session  for  completing  a  15-min  study). \nSome  researchers  have  resources  that  could  produc-\ntively  be  used  by  other  researchers  who  need  those \nresources  to  meet  their  research  goals.  One  way  to \nattempt to minimize the collective waste and maximize \nresearchers\xe2\x80\x99 collective ability to meet their research goals \nis to match \xe2\x80\x9chaves\xe2\x80\x9d with \xe2\x80\x9cneeds\xe2\x80\x9d using online platforms \nsuch  as  Science  Exchange  (https://www.science \nexchange.com) and StudySwap (http://osf.io/view/Study \nSwap). Such exchanges, which could be expanded into \nfull-scale online academic labor markets similar to oDesk \nor Elance (Horton, 2010), seek to push academic com-\nmunities into the top-right quadrant of Figure 1 by open-\ning  novel  lines  of  communication  and  creating \nopportunities to connect resources and expertise.\n\nStudy design\nAnother limitation to standard science is narrow sam-\npling of the constructs of interest (Baribault et\xc2\xa0al., 2018; \nJudd et\xc2\xa0al., 2012; Monin & Oppenheimer, 2014; Wells \n& Windschitl, 1999). A small team is at risk of generat-\ning a limited set of stimuli, operationalizations of vari-\nables,  and  study  designs.  Another  team  might  have \ncarried out a very different test of the same idea because \nof different prior training and theoretical assumptions. \nEven  seemingly  small  differences  in  methods  might \nproduce substantial differences in research results. An \nalternative  crowd  approach  is  to  assign  the  same \nresearch question to different experts, who then inde-\npendently  design  studies  aimed  at  answering  it  (low \ncommunication combined with high expertise; top-left \ncorner of Fig. 1). Landy et\xc2\xa0al. (2018) did precisely this, \nfinding that variability in effect sizes due to researcher \ndesign  choices  was  consistently  high.  Indeed,  study \ndesigns from different researchers produced significant \neffects in opposite directions for four of five research \nquestions related to negotiation, moral judgment, and \nimplicit cognition. Crowdsourcing conceptual replica-\ntions  more  effectively  reveals  the  true  consistency  in \nsupport for a scientific claim.\n\nData collection\nOnline platforms for crowdsourced labor such as Ama-\nzon\xe2\x80\x99s Mechanical Turk have become widely used as a \nsource of inexpensive research participants and coders \n(Stewart, Chandler, & Paolacci, 2017; see Supplement \n3  in  the  Supplemental  Material).  Rather  than  merely \nserving as research subjects, members of the general \n\npublic can also be recruited to collect data and obser-\nvations. This strategy moves the project into the bot-\ntommost  left  corner  of  Figure  1  of  inclusive  projects \nwith low communication, with anyone willing to help \nbeing included as a project member. The tradition of \ncitizen science dates back to Denison Olmsted\xe2\x80\x99s use of \nobservations from a crowd of both amateur and profes-\nsional astronomers to track the great meteor storm of \n1833 (Littmann & Suomela, 2014; Olmsted, 1934). Citi-\nzen science today is a movement to democratize sci-\nence  (Chargaff,  1978;  Feyerabend,  1982),  engage  the \npublic, create learning opportunities, and gather data \nand solve problems at minimal cost with the aid of a \nhost  of  volunteers  (Cavalier  &  Kennedy,  2016;  Gura, \n2013). Amateur scientists participate actively in scien-\ntific investigations in biology, astronomy, ecology, con-\nservation, and other fields, working under the direction \nof  professionals  at  research  institutions.  A  related \napproach is to gamify scientific problems and recruit \ncitizen scientists to aid in cracking them, as in the video \ngame Quantum Moves, in which players move digital \nrenditions of atoms (S\xc3\xb8rensen et\xc2\xa0al., 2016), the online \ngame EyeWire, in which players help reconstruct eye \ncells (Kim et\xc2\xa0al., 2014), and the protein-folding game \nFoldIt (Cooper et\xc2\xa0al., 2010). Note that for some types \nof citizen-science projects, contributors may have sub-\nstantial skills and knowledge\xe2\x80\x94or even formal training, \nsuch as an advanced degree\xe2\x80\x94and in such cases are far \nfrom novices. One of the strengths of crowdsourcing \nis  the  ability  to  tap  into  the  expertise  of  individuals \noutside of mainstream academia who are able and will-\ning to contribute to science.\n\nData analysis\nResearchers working with a complex data set are con-\nfronted with a multitude of choices regarding potential \nstatistical approaches, covariates, operationalizations of \nconceptual  variables,  and  the  like.  In  a  quantitative \nreview, Carp (2012a, 2012b) found that 241 published \narticles  on  fMRI  used  223  distinct  analytic  strategies. \nResearchers may consciously or unconsciously choose \nstatistical  specifications  that  yield  desired  results,  in \nparticular statistically significant results, in support of \na favored theory (Bakker et\xc2\xa0al., 2012; Ioannidis, 2005; \nIoannidis  &  Trikalinos,  2007;  Simmons  et\xc2\xa0 al.,  2011; \nSimonsohn,  Nelson,  &  Simmons,  2014).  One  way  to \nmaximize transparency is to turn the analysis of data \nover to a crowd of experts. The same data set is dis-\ntributed to numerous scientists who are asked to test \nthe same theoretical hypothesis, at first without know-\ning  the  specifications  used  by  their  colleagues  (high \nexpertise combined with low communication; top-left \nquadrant of Fig. 1). This offers an opportunity to assess \n\n\x0c722 \n\nUhlmann et al.\n\nhow even seemingly minor differences in choices may \naffect research outcomes and reduces the pressure to \nobserve any particular outcome\xe2\x80\x94at least for the pur-\nposes of publishability. Silberzahn et\xc2\xa0al. (2018) found \nthat  29  different  teams  of  analysts  used  29  distinct \nspecifications and returned effect-size estimates for the \nsame  research  question  (\xe2\x80\x9cDo  dark  skin  toned  soccer \nplayers  receive  more  red  cards?\xe2\x80\x9d)  that  ranged  from \nslightly negative to large positive effects. Crowdsourc-\ning the analysis of the data reveals the extent to which \nresearch conclusions are contingent on the defensible \nyet subjective decisions made by different analysts.\n\nThe growth of large-scale data has created opportu-\nnities  to  leverage  this  diversity  to  identify  the  most \nrobust means of analyzing such complex and massive \ndata sets. Crowdsourced challenges have been used by \nresearchers for benchmarking new computational meth-\nods,  as  with,  for  instance,  the  Dialogue  for  Reverse \nEngineering Assessments and Methods (DREAM) Chal-\nlenge  focused  on  predicting  the  survival  of  breast- \ncancer patients (Saez-Rodriguez et\xc2\xa0al., 2016; Stolovitzky, \nMonroe, & Califano, 2007). Organizers provide a test \ndata set and a particular question to be addressed to \nmany  independent  analysts  (a  top-left-quadrant \napproach) and then apply the analytic strategies to a \nhold-out data set to evaluate their robustness.\n\nAnother  innovative  method  is  to  hold  constructs, \nmodels, and covariates constant and leverage a network \nof researchers to carry out this same analysis on differ-\nent existing data sets (a coordinated analysis; Hofer & \nPiccinin,  2009). This  approach  was  pioneered  by  the \nIntegrative Analysis of Longitudinal Studies on Aging \nnetwork (Lindwall et\xc2\xa0al., 2012). Testing a research ques-\ntion of common interest (e.g., \xe2\x80\x9cDoes education protect \nagainst  cognitive  decline?\xe2\x80\x9d;  Piccinin  et\xc2\xa0 al.,  2013)  on \nexisting data sets that include the same constructs (e.g., \nmeasures of cognitive function such as memory, reason-\ning,  and  fluency)  and  yet  measure  them  in  disparate \nways  in  different  populations  (e.g.,  Sweden,  Austria, \nthe  Netherlands,  and  the  United  Kingdom)  far  more \nsystematically assesses the generalizability of the results \nthan relying on a single data source. Because members \nof this network of experts communicate extensively to \nagree on their shared analytic approach and measures \nto use from each longitudinal data set, a coordinated \nanalysis falls into the top-right quadrant of Figure 1.\n\nNote  that  all  of  these  approaches  are  qualitatively \ndifferent from fields in which many researchers inde-\npendently leverage a central data source (e.g., the Gen-\neral Social Survey, or GSS). In fields such as political \nscience, resources such as the GSS are used to investi-\ngate separate research questions, such that aggregation \nand  metascientific  comparisons  are  less  informative. \nCrowdsourcing  is  especially  useful,  we  suggest,  for \n\nfields that rely on local resources that can remain siloed. \nThat said, the data corpus generated by crowdsourced \nprojects  often  serves  as  a  public  resource  after  the \npublication of the article (e.g., Open Science Collabora-\ntion, 2015; Tierney et\xc2\xa0al., 2016).\n\nReplicating findings before publication\nIndividual  laboratories  are  typically  constrained  in \nthe amount and type of data they can collect. Repli-\ncating  unpublished  findings  in  independent  labora-\ntories  before  they  are  submitted  for  publication \n(Schooler, 2014; Tierney, Schweinsberg, & Uhlmann, \n2018) addresses power and generalizability directly. \nAuthors can specify a priori in which replication sam-\nples  and  laboratories  they  expect  their  findings  to \nemerge;  for  example,  they  might  select  only  topic \nexperts as their replicators and thus moving up the \nvertical axis of Figure 1. This approach, which thus \nfar returns a modest reproducibility rate even under \nthe seemingly best of conditions (Schweinsberg et\xc2\xa0al., \n2016),  has  recently  been  integrated  into  graduate \nand  undergraduate  methods  classes  (Schweinsberg, \nVignanola, et\xc2\xa0al., 2018), thus traveling downward along \nthe  vertical  axis  toward  greater  inclusiveness.  Such \ncrowdsourced  pedagogical  initiatives  are  one  means \nof  turning  replication  into  a  commonplace  aspect  of \nhow\xc2\xa0science is conducted and students are educated  \n(Everett & Earp, 2015; Frank & Saxe, 2012; Grahe et\xc2\xa0al., \n2012).\n\nWriting research reports\nThe conceptualization, drafting, and revision of research \narticles represents another opportunity to leverage dis-\ntributed  knowledge.  The  article  \xe2\x80\x9cThe  Capitalist\xe2\x80\x99s \nDilemma,\xe2\x80\x9d conceptualized and written by two profes-\nsors  and  150  of  their  MBA  students,  is  one  example \n(Christensen & van Bever, 2014). As with other forms \nof  collaborative  writing  online,  such  as  Wikipedia, \nchanneling the contributions of many collaborators into \na quality finished article requires a few group leaders \nwho complete a disproportionate amount of the work \nand  organize  and  edit  the  written  material  of  others \n(Kittur & Kraut, 2008; Kittur, Lee, & Kraut, 2009). Our \npersonal experience with articles with many authors is \nthat a large number of contributors commenting pub-\nlicly on the draft greatly facilitates working out a solid \nframework and set of arguments, identifying relevant \narticles and literatures to cite (especially unpublished \nwork), ferreting out quantitative and grammatical errors, \nand  tempering  claims  appropriately.  More  radically, \nefforts  such  as  CrowdForge  suggests  that  nonexperts \n(e.g., elite Mechanical Turk workers) are surprisingly \n\n\x0cCrowdsourcing Science \n\n723\n\ncapable at drafting quality summaries of scientific find-\nings for lay readers (Kittur, Smus, Khamkar, & Kraut, \n2011).  Such  quality  raw  material  could  be  carefully \nvetted and included in reviews of scientific research for \npractitioners and lay audiences. This suggests cautious \noptimism in moving down the vertical axis of Figure 1 \nto  allow  for  written  work  from  unconventional  con-\ntributors, with the degree of inclusiveness varying by \nthe technical expertise and topic knowledge required \nfor a given article.\n\nPeer review\nIn  the  current  system  of  academic  peer  review,  an \nunpublished manuscript is submitted to a journal and \nevaluated by the editor and usually two to five external \nreferees,  each  of  whom  provides  detailed  feedback, \noften  over  multiple  rounds  of  revisions  and  serially \nacross multiple journals. Even when successful, it can \nbe a slow and arduous process taking months or years. \nFor  example,  Nosek  and  Bar-Anan  (2012)  reported  a \ncase study of a researcher\xe2\x80\x99s corpus of publications and \nfound that the average time from manuscript submis-\nsion to ultimate publication was 677 days. There is little \ndoubt that detailed feedback from colleagues can be \nimmensely  helpful,  yet  it  remains  unknown  whether \nresearch  reports  are  consistently  improved  by  the \nreview process (\xe2\x80\x9cRevolutionizing Peer Review?\xe2\x80\x9d 2005). \nEmpirical studies indicate that the interrater reliability \nof independent assessors is low, with median reliability \ncoefficients of .30 for journal articles and .33 for grant \nreviews  (Bornmann,  Mutz,  &  Daniel,  2010;  Cicchetti, \n1991; Marsh, Jayasinghe, & Bond, 2008) and that there \nis  bias  in  favor  of  authors  with  strong  networks  \n(Wenneras & Wold, 1997). There are also the diminish-\ning returns on time investments to consider\xe2\x80\x94completing  \niterative rounds of review and revisions consumes time \nthat  might  have  been  better  allocated  to  pursuing  a \nnovel  scientific  discovery.  The  reviewers,  typically \nanonymous, receive minimal professional benefit from \ntheir work, and the broader community may never hear \nworthy criticisms left unaddressed in the published ver-\nsion of the article. Ultimately, publication in a presti-\ngious  outlet  is  a  poor  signal  of  an  article\xe2\x80\x99s  scholarly \nimpact,  with  journal  impact  factors  driven  by  outlier \narticles  and  only  a  weak  predictor  of  the  citations \naccrued  by  the  typical  article  in  the  journal  (Baum, \n2011;  Holden,  Rosenberg,  Barker,  &  Onghena,  2006; \nSeglen, 1997).\n\nAn alternative is to open scientific communication \nand  crowdsource  the  peer-review  process  (Nosek  & \nBar-Anan, 2012). This moves rightward on the horizon-\ntal axis by opening communication and downward on \nthe  vertical  axis  to  the  extent  the  review  process  is \n\ninclusive of many commentators. Both might be accom-\nplished simultaneously using a centralized platform for \nreview and discussion of research reports, with a con-\ntent feed similar to social-media sites (e.g., Facebook, \nTwitter)  and  users  able  to  comment  on  and evaluate \ncontent  as  with  the  websites  run  by  Reddit,  Yelp,  \nAmazon, and others (Buttliere, 2014). Posted files could \ninclude not only manuscripts but also data sets, code, \nand materials and reanalyses, replications, and critiques \nby other scientists. Peer review would be open, cred-\nited,  and  citable,  and  prominent  articles  that  attract \nattention  would  be  evaluated  by  a  potentially  more \nreliable crowd of scientists rather than a small group \nof  select  colleagues.  Further,  reviewers  would  have \naccess to the underlying data, facilitating the early iden-\ntification of errors (Sakaluk, Williams, & Biernat, 2014). \nMeasures of contribution would be diverse, with schol-\narly reputation enhanced not just via citations to authored \nmanuscripts but also intellectual impact via proposals of \nnovel ideas, the posting of data and code that others find \nuseful, insightful feedback on others\xe2\x80\x99 work, and the cura-\ntion of content related to specialized topic areas (e.g., \nreplicability of the effects of mood on helping behaviors; \nLeBel  et\xc2\xa0 al.,  2018).  Original  authors  would  have  the \nopportunity to update their article in light of new evi-\ndence or arguments, with older versions archived, as in \nthe Living Reviews group of journals in physics.\n\nIn contrast to such a radical bottom-right-quadrant \napproach (open communication, highly inclusive), top-\nright-quadrant versions of peer review would invite a \ncrowd of topic experts carefully selected by a journal \neditor.  However,  in  this  more  conservative  scenario \njournal  reviews  would  still  be  public,  citable,  and \ngreater in number than is currently the norm. Open and \ncitable  reviews  allow  readers  who  weight  traditional \ncredentials highly to do so, whereas individuals lower \nin formal expertise but whose comments are high in \nquality  have  the  opportunity  to  be  recognized.  The \nbarriers  to  wider  experimentation  are  not  so  much  \ntechnological\xe2\x80\x94there are already platforms that facilitate \nopen scientific communication (Wolfman-Arent, 2014)\xe2\x80\x94\nbut  rather  social,  with  current  professional  reward \nstructures  still  encouraging  publication  via  the  tradi-\ntional process and outlets. Only by experimenting with \ndiverse  approaches,  some  staying  close  in  important \nrespects  to  traditional  academic  review  and  others \ndeparting radically, can we identify the most effective \nways to communicate scientific ideas and knowledge.\n\nReplicating published findings\nAmong  the  best  known  uses  of  crowdsourcing  are \nlarge-scale  initiatives  to  directly  replicate  published \nresearch in psychology, biomedicine, economics, and \n\n\x0c724 \n\nUhlmann et al.\n\nother fields (e.g., Alogna et\xc2\xa0al., 2014; Errington et\xc2\xa0al., \n2014; McCarthy et\xc2\xa0al., 2018; O\xe2\x80\x99Donnell et\xc2\xa0al., 2018). In \nthese  crowdsourced  projects,  up  to  100  laboratories \nattempt to repeat the methodology of previous studies, \ncollecting  much  larger  samples  to  provide  improved \nstatistical  power  to  detect  the  hypothesized  effect. \nAggregating  across  six  major  replication  initiatives  in \nthe  social  sciences,  examining  190  effects  in  total, \ncrowdsourced  teams  successfully  replicated  90  (47%; \nCamerer et\xc2\xa0al., 2016, 2018; Ebersole et\xc2\xa0al., 2016; Klein \net\xc2\xa0al., 2014, 2018; Open Science Collaboration, 2015).\nA crowdsourced approach to replicability reveals that \nhigh  levels  of  heterogeneity  in  effect-size  estimates \nacross  laboratories  are  observed  primarily  for  large \neffects,  not  small  ones  (Klein  et\xc2\xa0 al.,  2018).  In  other \nwords, effects that fail to be replicated tend to consis-\ntently fail to be replicated across cultures and demo-\ngraphic populations, which casts doubt on the argument \nthat as-yet-unidentified moderators explain the disap-\npointing results. The lack of consistent laboratory dif-\nferences  in  effect-size  estimates  (i.e.,  some  research \nteams are not \xe2\x80\x9cbetter\xe2\x80\x9d than others at obtaining support \nfor  the  original  hypothesis;  Bench  et\xc2\xa0 al.,  2017;  Klein \net\xc2\xa0al., 2014, 2018) suggests that cautious scaling (e.g., \nmoving  downward  on  the  vertical  axis  of  Figure  1 \ntoward greater inclusiveness) ought to be considered. \nThe  Collaborative  Replications  and  Education  Project \n(CREP; Grahe et\xc2\xa0al., 2013; Wagge et\xc2\xa0al., 2019) seeks to \nachieve this by organizing undergraduate experimental \nmethods classes into research teams, an approach that \npromises to radically scale up data collection for rep-\nlications by integrating this activity into student educa-\ntion (Everett & Earp, 2015; Frank & Saxe, 2012). The \nPsychological Science Accelerator, an international net-\nwork of more than 300 psychological-science labora-\ntories,  have  committed  to  contributing  to  large-scale \ncollaborations on an ongoing basis, including regularly \ninvolving their students via the Accelerated CREP initia-\ntive (Moshontz et\xc2\xa0al., 2018).\n\nDeciding what findings to pursue further\nFaced  with  a  voluminous  and  constantly  growing \nresearch  literature\xe2\x80\x94more  than  30  million  academic \narticles  have  been  published  since  1965  (Pan  et\xc2\xa0 al., \n2016)\xe2\x80\x94and evidence that many published findings are \nless robust than initially thought (Begley & Ellis, 2012; \nErrington  et\xc2\xa0 al.,  2014;  Open  Science  Collaboration, \n2015; Prinz, Schlange, & Asadullah, 2011), researchers \nmust determine how best to distribute limited replica-\ntion  resources.  Viable  options  include  focusing  on \nhighly cited articles, findings covered in student text-\nbooks, results that receive widespread media coverage, \n\nor on research with practical relevance (e.g., for gov-\nernment  policies  or  interventions  to  reduce  demo-\ngraphic gaps in educational attainment). The replication \nvalue of a study might be calculated on the basis of the \nimpact  of  the  finding  relative  to  the  strength  of  the \navailable evidence (e.g., statistical power of the original \ndemonstrations; Nosek et\xc2\xa0al., 2012).\n\nAnother  complementary  rather  than  competing \napproach is to leverage the collective wisdom of the \nscientific  community.  The  aggregated  estimates  of \ncrowds perform surprisingly well at predicting future \noutcomes\xe2\x80\x94such as election results, news and sporting \nevents,  and  stock-market  fluctuations\xe2\x80\x94because  in \nmany  cases,  the  aggregation  cancels  out  individual \nerrors (Galton, 1907; Mellers et\xc2\xa0al., 2014; Surowiecki, \n2005). Likewise, the averaged independent predictions \nof scientists regarding research outcomes\xe2\x80\x94based solely \non  examinations  of  short  summaries  of  the  findings, \nresearch abstracts, or study materials\xe2\x80\x94are remarkably \nwell aligned with realized significance levels and effect \nsizes (Camerer et\xc2\xa0al., 2016; DellaVigna & Pope, 2018a, \n2018b; Dreber et\xc2\xa0al., 2015; Forsell et\xc2\xa0al., 2018; Landy \net\xc2\xa0al., 2018). Senior academics (e.g., full professors) and \njunior academics (e.g., graduate students and research \nassistants)  exhibit  similar  forecasting  accuracy  \n(DellaVigna & Pope, 2018a, 2018b; Landy et\xc2\xa0al., 2018), \nsuggesting  the  feasibility  of  an  inclusive  bottom-left-\nquadrant approach. It may be reasonable to avoid allo-\ncating  replication  resources  to  findings  considered \neither clearly spurious or well-established by a hetero-\ngeneous crowd of scientists and focus on findings about \nwhich beliefs are conflicting or uncertain.\n\nA  decision  market  might  be  used  to  select  among \nthe many available options for independent replication, \nthe  idea  being  to  allocate  resources  as  efficiently  as \npossible. Crowdsourced replications will be most useful \nwhen a clear, widely agreed-on question of broad inter-\nest is present. Large-scale efforts seem less appropriate \nfor findings the community considers highly unlikely \nto be true (e.g., extrasensory perception) or not par-\nticularly  theoretically  interesting  if  true.  Such  crowd-\nbased  selection  might  be  ongoing,  with  attention \ndynamically shifting away from effects that have expe-\nrienced repeated replication failures and for which the \ncommunity\xe2\x80\x99s expectations drop below a predetermined \nthreshold (Dreber et\xc2\xa0al., 2015). This would help prevent \ncases in which numerous laboratories conduct replica-\ntions of an effect, collecting many thousands of partici-\npants,  when  fewer  tests  would  have  already  led  to \nstrong inferences. Decision markets might also be used \nto select the most and least likely populations an effect \nshould  emerge  in  as  an  initial  test  of  universality \n(Norenzayan & Heine, 2005).\n\n\x0cCrowdsourcing Science \n\n725\n\nCrowd  science  can  also  be  used  to  make  gradual \nimprovements to existing research paradigms and inter-\nventions. Lai and colleagues (Lai et\xc2\xa0al., 2014, 2016) held \na series of crowdsourced contests to identify the best \ninterventions for reducing implicit racial biases. Begin-\nning in the top-left quadrant of Figure 1 (low commu-\nnication, high expertise), research teams submitted 17 \ninterventions to reduce implicit biases (e.g., exposure \nto positive exemplars, perspective taking, empathy). Of \nthose  interventions,  8  successfully  reduced  implicit \nintergroup bias in the short term. Moving horizontally \ninto the top right of the quadrant by adding the element \nof information exchange, teams were able to observe \nand learn from each other\xe2\x80\x99s approaches between rounds \nof data collection. Several teams used this opportunity \nto improve their own intervention, leading to progres-\nsively greater effectiveness in reducing intergroup bias \nacross  rounds.  We  believe  this  contest  model  holds \nwidespread applicability for identifying and improving \nupon  practical  interventions  to  address  societal  chal-\nlenges.  We  envision  a  future  scientific  landscape  in \nwhich forecasting surveys and decision markets are run \nin tandem with research contests and other large-scale \nempirical data collections on an ongoing basis.\n\n2018). The receipt and renewal of such funds could be \nfurther linked to evidence of ongoing contributions to \nopen  science.  These  might  include  publicly  posting \ndata and materials (Simonsohn, 2013), disclosing data \nexclusions and stopping rules (Simmons et\xc2\xa0al., 2011), \nrunning  highly  powered  studies  (Stanley,  Carter,  &  \nDoucouliagos, 2018), preregistering studies and analy-\nsis plans (Nosek, Ebersole, DeHaven, & Mellor, 2018; \nNosek  &  Lakens,  2014;  Wagenmakers,  Wetzels,  \nBorsboom, van der Maas, & Kievit, 2012), conducting \nreplications, helping to develop new methods, sharing \nresources on platforms such as StudySwap, and partici-\npating in crowdsourced initiatives, among other options. \nA more equitable distribution of financial support for \nresearch could reward merit and encourage excellence, \nnot only by providing additional opportunities for those \nwith useful skills and knowledge to contribute (Wahls, \n2018) but also by directly incentivizing emerging best \npractices.  To  avoid  the  diffusion  of  responsibility  on \nprojects with many collaborators, not only authorship \nbut also grant funding might be made contingent on \nspecific deliverables (e.g., minimum number of partici-\npants collected, provision of annotated analysis code \nothers can reproduce).\n\nReforms to Facilitate Large-Scale \nCollaboration\nWe believe most researchers have an intrinsic interest \nin contributing to the accumulation of knowledge and \nare  not  solely  driven  by  prestige.  At  the  same  time, \nprofessional reward systems can be updated in ways \nto encourage voluntary participation in large-scale col-\nlaboration  and  better  align  intrinsic  and  extrinsic \nmotives. The current culture and reward system impose \npressures  for  researchers  to  act  independently  as \nopposed to collectively and pursue initial evidence for \nnovel findings rather than engage in systematic verifica-\ntion, more than is ideal for scientific progress. Further, \nalthough  merit  matters  in  science,  there  are  also  \nMatthew effects (Bol et\xc2\xa0al., 2018; Clauset et\xc2\xa0al., 2015;  \nMerton, 1968; Petersen et\xc2\xa0al., 2011; Wahls, 2018). The \nresulting hierarchical and network-based arrangements \ninterfere  with  inclusivity  for  researchers  who  have \nmuch  to  offer  but  come  from  disadvantaged  back-\ngrounds and/or lack resources. Thus, we advocate for \nchanges  to  include  greater  rewards  for  collective \nengagement.\n\nAuthor contribution statements\nAlthough  some  especially  elaborate  crowd  projects \ninvolve specialized subteams who are able to publish \na separate report of their work (e.g., Dreber et\xc2\xa0al., 2015; \nForsell et\xc2\xa0al., 2018), these are atypical cases. Articles \nwith  many  authors  that  report  large-scale  projects \nrequire reforms in how intellectual credit is allocated. \nInput can be documented through careful and detailed \nauthor contribution statements, which academic jour-\nnals increasingly require. A good starting point for the \ncrafting of clear contribution statements is the CRediT \ntaxonomy (Brand, Allen, Altman, Hlava, & Scott, 2015), \nin which contributions throughout the full research life \ncycle are represented in categories such as conceptu-\nalization, data curation, writing, and visualization. Pro-\nviding information about which coauthors contributed \nto  which  CRediT  categories  allows  collaborators  to \ntransparently communicate how authorship was deter-\nmined and which author deserves credit for which com-\nponents  of  a  research  project.  This  sort  of  detailed \naccounting is a necessary precursor for the acceptance \nof increasingly long author lists that are already com-\nmonplace in fields such as high-energy physics.\n\nDistribution of grant funding\nEmpirical  evidence  suggests  that  distributing  grant \nfunding more evenly would increase the total return on \ninvestment  in  terms  of  scientific  knowledge  (Wahls, \n\nSelection and promotion criteria\nIn  addition  to  traditional  metrics  of  scholarly  merit, \nsearch  and  promotion  committees  should  take  into \n\n\x0c726 \n\nUhlmann et al.\n\naccount an applicant\xe2\x80\x99s contributions to conducting rig-\norous  research  and  making  science  better.  In  some \nfields, a demonstrated commitment to open science and \nscientific reform is already starting to be factored into \nselection  and  promotion  decisions  (Nosek,  2017;  \nSch\xc3\xb6nbrodt, 2018). One way in which applicants might \nchoose  to  fulfill  these  criteria  is  by  participating  in \ncrowdsourced initiatives to replicate findings, reanalyze \ndata, generate and select ideas, and so forth. Compre-\nhensive shifts in incentives will require that hiring and \ntenure and promotion committees rely more on specific \nindicators of contribution (Brand et\xc2\xa0al., 2015), such as \nthe  author  contribution  statements  described  above, \nrather than heuristics of counting articles and whether \nthe person was first, last, or somewhere in the middle \nof an authorship list. In this way, individuals who led \nan important subcomponent of a massive project (e.g., \nthe  subteam  that  conducted  the  forecasting  survey, \nqualitative analyses, or Bayesian meta-analysis) can be \nmore fairly recognized.\n\nAnother more radical option is making entire project \nworkflows open and linked to each contributor (some-\nthing possible through the Open Science Framework; \nhttp://osf.io/) and for hiring and promotion committees \nto examine these workflows before making their deci-\nsions. In a future in which open peer review becomes \ncommonplace,  online  links  to  feedback  provided  on \nthe articles of colleagues might be formally listed on \none\xe2\x80\x99s curriculum vitae (CV) as further evidence of intel-\nlectual contribution and service to the field. If the mul-\ntifold  aspects  of  an  academic\xe2\x80\x99s  workflow  are  made \ntransparent, decision makers can move beyond heuris-\ntics and use more complete information to better allo-\ncate rewards on the basis of merit.\n\nIntegrating crowd science into pedagogy\nAnother  way  to  encourage  crowd  science  is  to  build \nsuch  initiatives  into  activities  that  scientists  in  many \nfields already do routinely, such as collecting data in \nmethods  classes  for  student  projects  and  analyzing \ncomplex data sets as part of graduate education (Everett \n& Earp, 2015; Frank & Saxe, 2012; Grahe et\xc2\xa0al., 2012; \nMavor  et\xc2\xa0 al.,  2016).  The  CREP  (Grahe  et\xc2\xa0 al.,  2013; \nWagge et\xc2\xa0al., 2019) and Pipeline Projects (Schweinsberg \net\xc2\xa0al., 2016; Schweinsberg, Viganolla, et\xc2\xa0al., 2018) offer \nopportunities to leverage such activities for articles with \nmany authors that report crowdsourced replications. In \nthese cases, for both students and course instructors, \nbeing the middle author on a report of an interesting \ninitiative is better than no author credit at all. Crowd-\nsourcing avoids letting the students\xe2\x80\x99 hard work collect-\ning  data  go  to  waste  through  repeating  established \nparadigms (e.g., the Stroop effect) in unpublishable \n\nclass projects the results of which are low in informa-\ntion gain. As a further incentive, the second Pipeline \nProject offers course instructors a free curriculum they \ncan use in their lectures, reducing course preparation \ntime  (https://osf.io/hj9zr).  Whether  graduate  pro-\ngrams provide opportunities for experiential educa-\ntion  and  authored  work  on  crowd-science  projects \ncould potentially be factored into their rankings and \naccreditations.\n\nChanges in publication criteria\nTop-down changes in publication requirements at jour-\nnals (e.g., disclosure rules and open-science badges) \nare  already  changing  how  science  is  done  and  what \ngets  published  (Everett  &  Earp,  2015;  Nosek  et\xc2\xa0 al., \n2015). Such systematic shifts in policies help to avoid \ncollective-action problems such that only a subset of \nscientists engage in best practices that increase research \nquality but may also reduce productivity, which risks \nplacing them at a professional disadvantage (Kidwell \net\xc2\xa0 al.,  2016).  One  option,  aimed  at  encouraging  pre-\npublication  independent  replication  (Schweinsberg \net\xc2\xa0al., 2016), is to include independent verification of \nfindings in another laboratory as a publication criterion \nat  the  most  prestigious  empirical  journals  (Mogil  & \nMacleod, 2017). It is often useful to get initial evidence \nfor a finding out there to be examined and debated by \nthe scientific community, and individual careers should \ncontinue to advance primarily in this way. However, it \nis also reasonable for those publication outlets that pro-\nvide  the  most  professional  benefit  to  authors  and  are \nperhaps perceived as most authoritative (e.g., Science, \nNature, Proceedings of the National Academy of Sciences) \nto set the bar higher. Prominent journal outlets are also \nincreasingly recognizing the value of metascientific work \nthat relies on a crowd approach, a trend that promises \nto encourage future crowdsourced projects. A more gen-\neral shift in emphasis toward rigorous verification, rela-\ntive to novelty, as a publication criterion would incentivize \nhigh-powered crowd projects well positioned to assess \nthe replicability and generalizability of findings.\n\nDeveloping infrastructure\nAnother avenue is to create infrastructure and tools to \nmake crowdsourcing easier and more efficient. Online \nplatforms  such  as  the  Harvard  Dataverse  and  Open \nScience Framework are available to host data, research \nand teaching materials, and preregistrations and docu-\nment  workflows.  Journal  mechanisms  such  as  Regis-\ntered  Reports  that  review  methodology  and  accept \narticles  in  principle  before  data  collection  have  now \nbeen  adopted  at  scores  of  outlets  (https://cos.io/rr), \n\n\x0cCrowdsourcing Science \n\n727\n\nand journals are increasingly experimenting with inno-\nvative formats such as open review, crowd review, and \nupdatable  articles.  Recently  introduced  tools  such  as \nStudySwap  and  standing  laboratory  networks  such  as \nthe  Psychological  Science  Accelerator  likewise  hold \npromise to change the landscape of everyday science.\nThese  approaches  to  encourage  large-scale  colla-\nboration  are  important  complements  to  reforms  in \nhow\xc2\xa0 small-team  science  is  conducted  and  funded. \nLarger samples (Stanley et\xc2\xa0al., 2018), disclosure rules \n(Simmons  et\xc2\xa0 al.,  2011),  preregistration  (Nosek  et\xc2\xa0 al., \n2018; Wagenmakers et\xc2\xa0al., 2012), and Registered Report \nformats at journals (Chambers, 2013; Nosek & Lakens, \n2014)  promise  to  increase  the  true  positive  rate  for \nsmall studies, with scaling up for crowd projects then \nallowing for strong inferences about the generalizability \nversus context sensitivity of particularly important find-\nings.  At  the  same  time,  crowdsourced  metascientific \ninvestigations  can  help  to  assess  the  effectiveness  of \nnew practices intended to improve science but that may \nalso have unwanted side effects. For instance, prereg-\nistration might reduce false-positive results but could \nalso negatively affect the rate of novel discoveries by \ndampening  creativity  (Brainerd  &  Reyna,  2018).  A \ncrowdsourced project in progress (Ebersole et\xc2\xa0al., 2018) \nwill  randomly  assign  researchers  to  preregister  their \nanalyses of a complex data set to empirically assess the \ncosts and benefits of this proposed reform. Finally, the \nencouragement  of  large-scale  collaborations  to  help \ndemocratize participation in research is a complement \nto supporting research at teaching institutions through \ngrants, addressing gender gaps in representation, and \nother  efforts  to  reduce  systematic  inequalities  in \nscience.\n\nConclusion\nCrowdsourcing  holds  the  potential  to  greatly  expand \nthe scale and impact of scientific research. It seeks to \npromote  inclusion  in  science,  maximize  material  and \nhuman resources, and make it possible to tackle prob-\nlems  that  are  orders  of  magnitude  greater  than  what \ncould be solved by individual minds working indepen-\ndently.  Although  most  commonly  used  in  the  data- \ncollection phase of research and for conducting replica-\ntions, opportunities to take advantage of a distributed, \ninterdependent  collective  span  the  entire  scientific \nendeavor\xe2\x80\x94from generating ideas to designing studies, \nanalyzing the data, replicating results, writing research \nreports, providing peer feedback, and making decisions \nabout what findings are worth pursuing further. Crowd-\nsourcing is the next step in science\xe2\x80\x99s progression from \nindividual  scholars  to  increasingly  larger  teams  and \n\nnow  massive  globally  distributed  collaborations.  The \ncrowdsourcing movement is not the end of the tradi-\ntional  scholar  or  of  the  vertically  integrated  model. \nRather, it seeks to complement this standard approach \nto  provide  more  options  for  accelerating  scientific \ndiscovery.\n\nAction Editor\nTimothy McNamara served as action editor for this article.\n\nAuthor Contributions\nThe study was outlined and the literature review conducted \nthrough a crowdsourced process to which all authors con-\ntributed. The third through ninth authors are listed alphabeti-\ncally  in  the  byline.  E.  L.  Uhlmann,  C.  R.  Ebersole,  C.  R. \nChartier, T. M. Errington, C. K. Lai, R. J. McCarthy, A. Riegel-\nman, R. Silberzahn, and B. A. Nosek drafted the body of the \nmanuscript. C. R. Ebersole created the figure and tables. All \nof  the  authors  provided  critical  edits  and  revisions  and \napproved the final manuscript for publication.\n\nORCID iD\nTimothy M. Errington \n\n https://orcid.org/0000-0002-4959-5143\n\nDeclaration of Conflicting Interests\nThe author(s) declared that there were no conflicts of interest \nwith  respect  to  the  authorship  or  the  publication  of  this \narticle.\n\nFunding\nThis research was supported by the Center for Open Science, \nJohn Templeton Foundation, Templeton World Charity Foun-\ndation,  Templeton  Religion  Trust,  Arnold  Ventures,  James \nMcDonnell  Foundation,  and  a  research  and  development \ngrant from INSEAD.\n\nSupplemental Material\nAdditional  supporting  information  can  be  found  at  http://\njournals.sagepub.com/doi/suppl/10.1177/1745691619850561\n\nReferences\nAlogna, V. K., Attaya, M. K., Aucoin, P., Bahnik, S., Birch, S., \nBirt, A. R., . . . Zwaan, R. A. (2014). Registered replication \nreport: Schooler & Engstler-Schooler (1990). Perspectives \non Psychological Science, 9, 556\xe2\x80\x93578.\n\nBakker, M., van Dijk, A., & Wicherts, J. M. (2012). The rules \nof the game called psychological science. Perspectives on \nPsychological Science, 7, 543\xe2\x80\x93554.\n\nBakker,  M.,  &  Wicherts,  J.  M.  (2011).  The  (mis)reporting \nof  statistical  results  in  psychology  journals.  Behavior \nResearch Methods, 43, 666\xe2\x80\x93678.\n\nBall, P. (2014). Crowd-sourcing: Strength in numbers. Nature, \n\n506, 422\xe2\x80\x93423.\n\n\x0c728 \n\nUhlmann et al.\n\nBaribault, B., Donkin, C., Little, D. R., Trueblood, J., Oravecz, \nZ., van Ravenzwaaij, D., . . . Vandekerckhove, J. (2018). \nMetastudies  for  robust  tests  of  theory.  Proceedings  of \nthe National Academy of Sciences, USA,  15,  2607\xe2\x80\x932612. \ndoi:10.1073/pnas.1708285114.\n\nBaum, J. A. (2011). Free-riding on power laws: Questioning \nthe validity of the impact factor as a measure of research \nquality  in  organization  studies. Organization,  18,  449\xe2\x80\x93\n466.\n\nBegley, C. G., & Ellis, L. M. (2012). Drug development: Raise \nstandards  for  preclinical  cancer  research. Nature,  483, \n531\xe2\x80\x93533.\n\nBench,  S.  W.,  Rivera,  G.  N.,  Schlegel,  R.  J.,  Hicks,  J.  A., \n&  Lench,  H.  C.  (2017).  Does  expertise  matter  in  repli-\ncation?  An  examination  of  the  reproducibility  project: \nPsychology.  Journal  of  Experimental  Social  Psychology, \n68, 181\xe2\x80\x93184.\n\nBenoit,  K.,  Conway,  D.,  Lauderdale,  B.  E.,  Laver,  M.,  & \nMikhaylov,  S.  (2016).  Crowd-sourced  text  analysis: \nReproducible  and  agile  production  of  political  data. \nAmerican Political Science Review, 110, 278\xe2\x80\x93295.\n\nBerle,  D.,  &  Starcevic,  V.  (2007).  Inconsistencies  between \nreported  test  statistics  and  p-values  in  two  psychiatry \njournals. International Journal of Methods in Psychiatric \nResearch, 16, 202\xe2\x80\x93207. doi:10.1002/mpr.225\n\nBerman, J. S., & Reich, C. M. (2010). Investigator allegiance \nand the evaluation of psychotherapy outcome research. \nEuropean Journal of Psychotherapy and Counselling, 12, \n11\xe2\x80\x9321.\n\nBol, T., De Vaan, M., & van de Rijt, A. (2018). The Matthew \neffect  in  science  funding.  Proceedings  of  the  National \nAcademy of Sciences, USA,  15,  4887\xe2\x80\x934890.  doi:10.1073/\npnas.1719557115.\n\nB\xc3\xb6rner, K., Contractor, N., Falk-Krzesinski, H. J., Fiore, S.\xc2\xa0M., \nHall, K. L., Keyton, J., . . . Uzzi, B. (2010). A multi-level \nsystems  perspective  for  the  science  of  team  science. \nScience  Translational  Medicine,  2(49),  Article  49cm24. \ndoi:10.1126/scitranslmed.3001399\n\nBornmann, L., Mutz, R., & Daniel, H.-D. (2010). A reliability-\ngeneralization study of journal peer reviews: A multilevel \nmeta-analysis of inter-rater reliability and its determinants. \nPLOS  ONE,  5(12),  Article  e14331.  doi:10.1371/journal \n.pone.0014331\n\nBrainerd, C. J., & Reyna, V. F. (2018). Replication, registra-\ntion, and scientific creativity. Perspectives on Psychological \nScience, 13, 428\xe2\x80\x93432. doi:10.1177/1745691617739421\n\nBrand, A., Allen, L., Altman, M., Hlava, M., & Scott, J. (2015). \nBeyond authorship: Attribution, contribution, collabora-\ntion, and credit. Learned Publishing, 28, 151\xe2\x80\x93155.\n\nBudescu, D. V., & Chen, E. (2015). Identifying expertise to \nextract the wisdom of crowds. Management Science, 61, \n267\xe2\x80\x93280. doi:10.1287/mnsc.2014.1909\n\nButtliere,  B.  T.  (2014).  Using  science  and  psychology  to \nimprove  the  dissemination  and  evaluation  of  scientific \nwork. Frontiers in Computational Neuroscience, 8, Article \n82. doi:10.3389/fncom.2014.00082\n\nCamerer, C. F., Dreber, A., Forsell, E., Ho, T. H., Huber, J., \nJohannesson, M., . . . Wu, H. (2016). Evaluating replicability  \n\nof  laboratory  experiments  in  economics. Science,  351, \n1433\xe2\x80\x931436.\n\nCamerer, C. F., Dreber, A., Holzmeister, F., Ho, T. -H., Huber, \nJ., Johannesson, M., . . . Wu, H. (2018). Evaluating replica-\nbility of social science experiments in Nature and Science \nbetween  2010  and  2015. Nature  Human  Behaviour,  2, \n637\xe2\x80\x93644.\n\nCarp, J. (2012a). On the plurality of (methodological) worlds: \nEstimating  the  analytic  flexibility  of  fMRI  experiments. \nFrontiers  in  Neuroscience,  6,  Article  149.  doi:10.3389/\nfnins.2012.00149\n\nCarp,  J.  (2012b).  The  secret  lives  of  experiments:  Methods \nreporting in the fMRI literature. NeuroImage, 63, 289\xe2\x80\x93300. \ndoi:10.1016/j.neuroimage.2012.07.004\n\nCavalier, D., & Kennedy, E. (2016). The rightful place of sci-\nence: Citizen science. Tempe, AZ: Consortium for Science, \nPolicy & Outcomes.\n\nChambers, C. D. (2013). Registered reports: A new publishing \n\ninitiative at Cortex. Cortex, 49, 609\xe2\x80\x93610.\n\nChargaff, E. (1978). Heraclitean fire: Sketches from a life before \n\nnature. New York, NY: Rockefeller University Press.\n\nChristensen, C. M., & van Bever, D. (2014). The capitalist\xe2\x80\x99s \n\ndilemma. Harvard Business Review, 92, 60\xe2\x80\x9368.\n\nChurch, G. M. (2005). The personal genome project. Molecular \nSystems  Biology,  1,  Article  2005.0030.  doi:10.1038/\nmsb4100040\n\nCicchetti,  D.  V.  (1991).  The  reliability  of  peer  review  for \nmanuscript  and  grant  submissions:  A  cross-disciplinary \ninvestigation. Behavioral & Brain Sciences, 14, 119\xe2\x80\x93135.\nClauset,  A.,  Arbesman,  S.,  &  Larremore,  D.  B.  (2015). \nSystematic  inequality  and  hierarchy  in  faculty  hiring \nnetworks.  Science  Advances,  1(1),  Article  e1400005. \ndoi:10.1126/sciadv.1400005\n\nClemente,  F.  (1973).  Early  career  determinants  of  research \nproductivity. American Journal of Sociology, 79, 409\xe2\x80\x93419.\nCooper, S., Khatib, F., Treuille, A., Barbero, J., Lee, J., Beenen, \nM., . . . Popovic, Z. (2010). Predicting protein structures \nwith a multiplayer online game. Nature, 466, 756\xe2\x80\x93760.\n\nCova,  F.,  Strickland,  B.,  Abatista,  A.,  Allard,  A.,  Andow,  J., \nAttie, M., . . . Zhou, X. (2018). Estimating the reproduc-\nibility of experimental philosophy. Review of Philosophy \nand Psychology. Advance online publication. doi:10.1007/\ns13164-018-0400-9\n\nDavis,  R.,  Espinosa,  J.,  Glass,  C.,  Green,  M.  R.,  Massague, \nJ.,  Pan,  D.,  &  Dang,  C.  V.  (2018).  Reproducibility proj-\nect: Cancer biology. Retrieved from https://elifesciences \n.org/collections/9b1e83d1/reproducibility-project-cancer-\nbiology\n\nDellaVigna,  S.,  &  Pope,  D.  G.  (2018a).  Predicting  experi-\nmental  results:  Who  knows  what?  Journal  of  Political \nEconomy, 126, 2410\xe2\x80\x932456.\n\nDellaVigna, S., & Pope, D. G. (2018b). What motivates effort? \nEvidence and expert forecasts. The Review of Economic \nStudies, 85, 1029\xe2\x80\x931069.\n\nDerex, M., & Boyd, R. (2016). Partial connectivity increases \ncultural accumulation within groups. Proceedings of the \nNational  Academy  of  Sciences,  USA,  113,  2982\xe2\x80\x932987. \ndoi:10.1073/pnas.1518798113\n\n\x0cCrowdsourcing Science \n\n729\n\nDreber, A., Pfeiffer, T., Almenberg, J., Isaksson, S., Wilson, \nB.,  Chen,  Y.,  .  .  .  Johannesson,  M.  (2015).  Using  pre-\ndiction markets to estimate the reproducibility of scien-\ntific  research. Proceedings  of  the  National  Academy  of \nSciences, USA, 112, 15343\xe2\x80\x9315347.\n\nEbersole, C. R., Atherton, O. E., Belanger, A. L., Skulborstad, \nH.  M.,  Allen,  J.  M.,  Banks,  J.  B.,  .  .  .  Nosek,  B.  A. \n(2016). Many Labs 3: Evaluating participant pool quality \nacross the academic semester via replication. Journal of \nExperimental Social Psychology, 67, 68\xe2\x80\x9382.\n\nEbersole, C. R., et\xc2\xa0al. (2018). Experimentally examining the \nconsequences  of  preregistered  analyses.  Manuscript  in \npreparation.\n\nEisenman, I., Meier, W. N., & Norris, J. R. (2014). A spuri-\nous  jump  in  the  satellite  record:  Has  Antarctic  sea  ice \nexpansion been overestimated? Cryosphere, 8, 1289\xe2\x80\x931296. \ndoi:10.5194/tc-8-1289-2014\n\nEitan,  O.,  Viganola,  D.,  Inbar,  Y.,  Dreber,  A.,  Johanneson, \nM., Pfeiffer, T., . . . Uhlmann, E. L. (2018). Is scientific \nresearch  politically  biased?  Systematic  empirical  tests \nand a forecasting tournament to address the controversy. \nJournal of Experimental Social Psychology, 79, 188\xe2\x80\x93199.\n\nErrington,  T.  M.,  Iorns,  E.,  Gunn,  W.,  Tan,  F.  E.,  Lomax, \nJ., & Nosek, B. A. (2014). An open investigation of the \nreproducibility of cancer biology research. eLife, 3, Article \ne04333. doi:10.7554/eLife.04333\n\nEverett, J. A. C., & Earp, B. D. (2015). A tragedy of the (aca-\ndemic)  commons:  Interpreting  the  replication  crisis  in \npsychology as a social dilemma for early-career research-\ners. Frontiers in Psychology, 6, Article 1152. doi:10.3389/\nfpsyg.2015.01152\n\nFanelli,  D.  (2010).  \xe2\x80\x9cPositive\xe2\x80\x9d  results  increase  down  the \nHierarchy of the Sciences. PLOS ONE, 5(4), Article e10068. \ndoi:10.1371/journal.pone.0010068\n\nFeyerabend,  P.  (1982).  Science  in  a  free  society.  London, \n\nEngland: New Left Books.\n\nForsell, E., Viganola, D., Pfeiffer, T., Almenberg, J., Wilson, \nB., Chen, Y., . . . Dreber, A. (2018). Predicting replication \noutcomes in the Many Labs 2 study. Journal of Economic \nPsychology.  Advance  online  publication.  doi:10.1016/ \nj.joep.2018.10.009\n\nFrank, M., & Saxe, R. (2012). Teaching replication. Perspectives \n\non Psychological Science, 7, 600\xe2\x80\x93604.\n\nGalton, F. (1907). Vox populi. Nature, 75, 450\xe2\x80\x93451.\nGarcia-Berthou,  E.,  &  Alcaraz,  C.  (2004).  Incongruence \nbetween  test  statistics  and  p-values  in  medical  papers. \nBMC Medical Research Methodology, 4, Article 13. doi:10 \n.1186/1471-2288-4-13\n\nGelman, A., & Loken, E. (2014). The statistical crisis in sci-\n\nence. American Scientist, 102, 460\xe2\x80\x93465.\n\nGiles,  J.  (2005).  Internet  encyclopedias  go  head  to  head. \n\nNature, 438, 900\xe2\x80\x93901.\n\nGrahe, J. E., Brandt, M. J., IJzerman, H., Cohoon, J., Peng, \nC.,  Detweiler-Bedell,  B.,  .  .  .  Weisberg,  Y.  (2013). \nCollaborative Replications and Education Project (CREP). \nRetrieved  from  the  Open  Science  Framework  website: \nhttps://osf.io/wfc6u\n\nGrahe, J. E., Reifman, A., Herman, A., Walker, M., Oleson, \nK., Nario-Redmond, M., & Wiebe, R. (2012). Harnessing \n\nthe undiscovered resource of student research projects. \nPerspectives on Psychological Science, 7, 605\xe2\x80\x93607.\n\nGreenland,  P.,  &  Fontanarosa,  P.  B.  (2012).  Ending  honor-\nary authorship. Science,  337, 1019. doi:10.1126/science \n.1224988\n\nGura,  T.  (2013).  Citizen  science:  Amateur  experts. Nature, \n\nHand, E. (2010). Citizen science: People power. Nature, 466, \n\n496, 259\xe2\x80\x93261.\n\n685\xe2\x80\x93687.\n\nHenrich, J., Heine, S. J., & Norenzayan, A. (2010). The weird-\nest  people  in  the  world?  Behavioral  &  Brain  Sciences, \n33, 61\xe2\x80\x9383.\n\nHirsch, J. E. (2007). Does the h index have predictive power? \nProceedings  of  the  National  Academy  of  Sciences,  USA, \n104, 19193\xe2\x80\x9319198.\n\nHofer, S. M., & Piccinin, A. M. (2009). Integrative data analysis \nthrough coordination of measurement and analysis proto-\ncol across independent longitudinal studies. Psychological \nMethods, 14, 150\xe2\x80\x93164. doi:10.1037/a0015566\n\nHolden, G., Rosenberg, G., Barker, K., & Onghena, P. (2006). \nAn assessment of the predictive validity of impact factor \nscores: Implications for academic employment decisions \nin  social  work.  Research  on  Social  Work  Practice,  16, \n613\xe2\x80\x93624.\n\nHorton, J. (2010). Online labor markets. In A. Saberi (Ed.), \nWorkshop on internet and network economics (pp. 515\xe2\x80\x93\n522). Basel, Switzerland: Springer.\n\nHowe, J. (2006, June 1). The rise of crowdsourcing. Wired \nMagazine. Retrieved from http://www.wired.com/wired/\narchive/14.06/crowds.html\n\nIoannidis, J. P., Tarone, R., & McLaughlin, J. K. (2011). The \nfalse-positive  to  false-negative  ratio  in  epidemiologic \nstudies. Epidemiology, 22, 450\xe2\x80\x93456.\n\nIoannidis,  J.  P.  A.  (2005).  Why  most  published  research \nfindings  are  false.  PLOS  Medicine,  2(8),  Article  e124. \nRetrieved  from  http://www.plosmedicine.org/article/\ninfo%3Adoi%2F10.1371%2Fjournal.pmed.0020124\n\nIoannidis, J. P. A. (2008). Why most discovered true associa-\n\ntions are inflated. Epidemiology, 19, 640\xe2\x80\x93648.\n\nIoannidis, J. P. A., & Trikalinos, T. A. (2007). An exploratory \ntest for an excess of significant findings. Clinical Trials, \n4, 245\xe2\x80\x93253.\n\nJia,  M.,  Ding,  I.,  Falc\xc3\xa3o,  H.,  Schweinsberg,  M.,  Chen,  Y., \nPfeiffer, T., . . . Uhlmann, E. L. (2018). The crowdsourced \ngeneration, evaluation, and testing of research hypotheses. \nManuscript in preparation.\n\nJudd, C. M., Westfall, J., & Kenny, D. A. (2012). Treating stim-\nuli as a random factor in social psychology: A new and \ncomprehensive solution to a pervasive but largely ignored \nproblem.  Journal  of  Personality  and  Social  Psychology, \n103, 54\xe2\x80\x9369.\n\nKanefsky, B., Barlow, N. G., & Gulick, V. C. (2001, March). \nCan distributed volunteers accomplish massive data anal-\nysis  tasks?  Poster  presented  at  the  Proceedings  of  the \n32nd  Annual  Lunar  and  Planetary  Science  Conference, \nHouston, TX.\n\nKidwell,  M.  C.,  Lazarevic\xc2\xb4,  L.  B.,  Baranski,  E.,  Hardwicke, \nT.\xc2\xa0E., Piechowski, S., Falkenberg, L.-S., . . . Nosek, B. A. \n(2016). Badges to acknowledge open practices: A simple,  \n\n\x0c730 \n\nUhlmann et al.\n\nlow-cost,  effective  method  for  increasing  transparency. \nPLOS  Biology,  14(5),  Article  e1002456.  doi:10.1371/ \njournal.pbio.1002456doi:10.1371/journal.pbio.1002456\n\nKim, J. S., Greene, M. J., Zlateski, A., Lee, K., Richardson, M., \nTuraga, S. C., . . . Seung, H. S. (2014). Space-time wir-\ning specificity supports direction selectivity in the retina. \nNature, 509, 331\xe2\x80\x93336.\n\nKittur, A., & Kraut, R. E. (2008). Harnessing the wisdom of \ncrowds  in  Wikipedia:  Quality  through  coordination.  In \nProceedings  of  the  2008  ACM  Conference  on  Computer \nSupported Cooperative Work (pp. 37\xe2\x80\x9346). New York, NY: \nAssociation for Computing Machinery.\n\nKittur, A., Lee, B., & Kraut, R. E. (2009). Coordination in col-\nlective intelligence: The role of team structure and task \ninterdependence. In Proceedings of the SIGCHI Conference \non Human Factors in Computing Systems (pp. 1495\xe2\x80\x931504). \nNew York, NY: Association for Computing Machinery.\n\nKittur,  A.,  Smus,  B.,  Khamkar,  S.,  &  Kraut,  R.  E.  (2011). \nCrowdForge: Crowdsourcing complex work. In Proceed-\nings of the 24th Annual ACM Symposium on User Interface \nSoftware  and  Technology  (pp.  43\xe2\x80\x9352).  New  York,  NY: \nAssociation for Computing Machinery.\n\nKlein,  R.  A.,  Ratliff,  K.  A.,  Vianello,  M.,  Adams,  R.  B.,  Jr., \nBahn\xc3\xadk,  \xc5\xa0.,  Bernstein,  M.  J.,  .  .  .  Nosek,  B.  A.  (2014). \nInvestigating variation in replicability: A \xe2\x80\x9cmany labs\xe2\x80\x9d rep-\nlication project. Social Psychology, 45, 142\xe2\x80\x93152.\n\nKlein, R. A., Vianello, M., Hasselman, F., . . . Nosek, B. A. \n(2018). Many Labs 2: Investigating variation in replicabil-\nity across sample and setting. Advances in Methods and \nPractices in Psychological Science, 1, 443\xe2\x80\x93490.\n\nKniffin,  K.  M.,  &  Hanks,  A.  S.  (2018).  The  trade-offs  of \nteamwork  among  STEM  doctoral  graduates. American \nPsychologist, 73, 420\xe2\x80\x93432. doi:10.1037/amp0000288\n\nKosmala, M., Wiggins, A., Swanson, A., & Simmons, B. (2016). \nAssessing  data  quality  in  citizen  science.  Frontiers  in \nEcology and the Environment, 14, 551\xe2\x80\x93560. doi:10.1002/\nfee.1436\n\nLai, C. K., Marini, M., Lehr, S. A., Cerruti, C., Shin, J. L., Joy-\nGaba, J. A., . . . Nosek, B. A. (2014). Reducing implicit \nracial  preferences:  I.  A  comparative  investigation  of \n17  interventions.  Journal  of  Experimental  Psychology: \nGeneral, 143, 1765\xe2\x80\x931785.\n\nLai, C. K., Skinner, A. L., Cooley, E., Murrar, S., Brauer, M., \nDevos,  T.,  .  .  .  Nosek,  B.  A.  (2016).  Reducing  implicit \nracial  preferences:  II.  Intervention  effectiveness  across \ntime. Journal of Experimental Psychology: General, 145, \n1001\xe2\x80\x931016.\n\nLakhani, K. R., Jeppesen, L. B., Lohse, P. A., & Panetta, J. A. \n(2007). The value of openness in scientific problem solv-\ning (Working Paper 07-050). Retrieved from the Harvard \nBusiness  School  website:  https://www.hbs.edu/faculty/\nPublication%20Files/07-050.pdf\n\nLandry,  J.  (2000,  September/October).  Profiting  from  open \nsource. Harvard Business Review. Retrieved from https://\nhbr.org/2000/09/profiting-from-open-source\n\nLandy,  J.  F.,  Jia,  M.,  Ding,  I.  L.,  Viganola,  D.,  Tierney,  W., \nEbersole, C. R., . . . Uhlmann, E. L. (2018). Crowdsourcing \nhypothesis tests. Manuscript submitted for publication.\n\nLarrick, R. P., Mannes, A. E., & Soll, J. B. (2012). The social \npsychology  of  the  wisdom  of  crowds.  In  J.  I.  Krueger \n(Ed.),  Frontiers  in  social  psychology:  Social  judgment \nand  decision  making  (pp.  227\xe2\x80\x93242).  New  York,  NY: \nPsychology Press.\n\nLeBel, E. P., McCarthy, R., Earp, B., Elson, M., & Vanpaemel, \nW. (2018). A unified framework to quantify the credibility \nof scientific findings. Advances in Methods and Practices \nin Psychological Science, 1, 389\xe2\x80\x93402.\n\nLindwall,  M.,  Cimino,  C.  R.,  Gibbons,  L.  E.,  Mitchell,  M., \nBenitez,  A.,  Brown,  C.  L.,  .  .  .  Piccinin,  A.  M.  (2012). \nDynamic associations of change in physical activity and \nchange  in  cognitive  function:  Coordinated  analyses  of \nfour  longitudinal  studies.  Journal  of  Aging  Research, \n2012, Article 49359812. doi:10.1155/2012/493598\n\nList, B. (2017). Crowd-based peer review can be good and \n\nfast. Nature, 546, 9. doi:10.1038/546009a\n\nLittmann, M., & Suomela, T. (2014). Crowdsourcing, the great \nmeteor storm of 1833, and the founding of meteor sci-\nence. Endeavour, 38, 130\xe2\x80\x93138.\n\nLorenz, J., Rauhut, H., Schweitzer, F., & Helbing, D. (2011). \nHow  social  influence  can  undermine  the  wisdom  of \ncrowd  effect.  Proceedings  of  the  National  Academy  of \nSciences, USA, 108, 9020\xe2\x80\x939025.\n\nLuborsky,  L.,  Diguer,  L.,  Seligman,  D.  A.,  Rosenthal,  R., \nKrause, E. D., Johnson, S., . . . Schweizer, E. (1999). The \nresearcher\xe2\x80\x99s  own  therapy  allegiances:  A  \xe2\x80\x9cwild  card\xe2\x80\x9d  in \ncomparisons  of  treatment  efficacy. Clinical Psychology: \nScience and Practice, 6, 95\xe2\x80\x93106.\n\nMakel, M. C., Plucker, J. A., & Hegarty, B. (2012). Replications \nin psychology research: How often do they really occur? \nPerspectives in Psychological Science, 7, 537\xe2\x80\x93542.\n\nMannes, A. E., Soll, J. B., & Larrick, R. P. (2014). The wis-\ndom of select crowds. Journal of Personality and Social \nPsychology, 107, 276\xe2\x80\x93299.\n\nManzoli,  L.,  Flacco,  M.  E.,  D\xe2\x80\x99Addario,  M.,  Capasso,  L., \nDeVito,  C.,  Marzuillo,  C.,  .  .  .  Ioannidis,  J.  P.  (2014). \nNon-publication and delayed publication of randomized \ntrials on vaccines: Survey. British Medical Journal,  348, \nArticle g3058. doi:10.1136/bmj.g3058\n\nMarsh,  H.  W.,  Jayasinghe,  U.  W.,  &  Bond,  N.  W.  (2008). \nImproving the peer-review process for grant applications: \nReliability, validity, bias, and generalizability. American \nPsychologist, 63, 160\xe2\x80\x93168.\n\nMavor, D., Barlow, K., Thompson, S., Barad, B. A., Bonny, \nA. R., Cario, C. L., . . . Fraser, J. S. (2016). Determination \nof ubiquitin fitness landscapes under different chemical \nstresses in a classroom setting.  eLife,  5, Article e15802. \ndoi:10.7554/eLife.15802\n\nMcCarthy, R. J., Skowronski, J. J., Verschuere, B., Meijer, E. H., \nJim, A., Hoogesteyn, K., . . . Yildiz, E. (2018). Registered \nreplication report on Srull & Wyer (1979). Advances in \nMethods and Practices in Psychological Science, 1, 321\xe2\x80\x93\n336.\n\nMellers, B., Ungar, L., Baron, J., Ramos, J., Gurcay, B., Fincher, \nK., & Murray, T. (2014). Psychological strategies for win-\nning a geopolitical forecasting tournament. Psychological \nScience, 25, 1106\xe2\x80\x931115.\n\n\x0cCrowdsourcing Science \n\n731\n\nMerton, R. K. (1968). The Matthew effect in science. Science, \n\n159, 56\xe2\x80\x9363.\n\nMetaSUB International Consortium. (2016). The Metagenomics \nand  Metadesign  of  the  Subways  and  Urban  Biomes \n(MetaSUB)  International  Consortium  inaugural  meeting \nreport.  Microbiome,  4,  Article  24.  doi:10.1186/s40168-\n016-0168-z\n\nMogil, J. S., & Macleod, M. R. (2017). No publication without \n\nconfirmation. Nature, 542, 409\xe2\x80\x93411.\n\nMonin, B., & Oppenheimer, D. M. (2014). The limits of direct \nreplications and the virtues of stimulus sampling. Social \nPsychology, 45, 299\xe2\x80\x93300.\n\nMonin, B., Pizarro, D., & Beer, J. (2007). Deciding vs. react-\ning: Conceptions of moral judgment and the reason-affect \ndebate. Review of General Psychology, 11, 99\xe2\x80\x93111.\n\nMoshontz,  H.,  Campbell,  L.,  Ebersole,  C.  R.,  IJzerman,  H., \nUrry, H. L., Forscher, P. S., . . . Chartier, C. R. (2018). The \npsychological science accelerator: Advancing psychology \nthrough  a  distributed  collaborative  network.  Advances \nin  Methods  and  Practices  in  Psychological  Science,  1, \n501\xe2\x80\x93515.\n\nMueller-Langer, F., Fecher, B., Harhoff, D., & Wagner, G.\xc2\xa0G. \n(2019).  Replication  studies  in  economics\xe2\x80\x94How  many \nand which papers are chosen for replication, and why? \nResearch Policy, 48, 62\xe2\x80\x9383.\n\nMuffatto,  M.  (2006).  Open  source:  A  multidisciplinary \n\napproach. London, England: Imperial College Press.\n\nMynatta,  C.  R.,  Dohertya,  M.  E.,  &  Tweneya,  R.  D.  (1977). \nConfirmation bias in a simulated research environment: \nAn experimental study of scientific inference. Quarterly \nJournal of Experimental Psychology, 29, 85\xe2\x80\x9395.\n\nNorenzayan, A., & Heine, S. J. (2005). Psychological univer-\nsals: What are they and how can we know? Psychological \nBulletin, 135, 763\xe2\x80\x93784.\n\nNosek,  B.  A.  (2017,  July  14). Are  reproducibility  and  open \nscience starting to matter in tenure and promotion review? \nRetrieved  from  the  Center  for  Open  Science  website: \nhttps://cos.io/blog/are-reproducibility-and-open-science-\nstarting-matter-tenure-and-promotion-review\n\nNosek, B. A., Alter, G., Banks, G. C., Borsboom, D., Bowman, \nS. D., Breckler, S. J., . . . Yarkoni, T. (2015). Promoting \nan  open  research  culture.  Science,  348,  1422\xe2\x80\x931425. \ndoi:10.1126/science.aab2374\n\nNosek,  B.  A.,  &  Bar-Anan,  Y.  (2012).  Scientific  utopia:  I. \nOpening scientific communication. Psychological Inquiry, \n23, 217\xe2\x80\x93223.\n\nNosek, B. A., Ebersole, C. R., DeHaven, A., & Mellor, D.\xc2\xa0M. \n(2018).  The  preregistration  revolution.  Proceedings  of \nthe National Academy of Sciences, USA, 115, 2600\xe2\x80\x932606. \ndoi:10.1073/pnas.1708274114\n\nNosek,  B.  A.,  &  Errington,  T.  M.  (2017).  Reproducibility  in \ncancer  biology:  Making  sense  of  replications. eLife,  6, \nArticle e23383. doi:10.7554/eLife.23383\n\nNosek,  B.  A.,  &  Lakens,  D.  (2014).  Registered  reports:  A \nmethod to increase the  credibility of  published results. \nSocial Psychology, 45, 137\xe2\x80\x93141.\n\nNosek, B. A., Spies, J. R., & Motyl, M. (2012). Scientific uto-\npia: II. Restructuring incentives and practices to promote \n\ntruth  over  publishability.  Perspectives  on  Psychological \nScience, 7, 615\xe2\x80\x93631.\n\nO\xe2\x80\x99Donnell, M., Nelson, L. D., Ackermann, E., Aczel, B., Akhtar, \nA., Aldrovandi, S., . . . Zrubka, M. (2018). Registered rep-\nlication report: Dijksterhuis & van Knippenberg (1998). \nPerspectives  on  Psychological  Science,  13,  268\xe2\x80\x93294. \ndoi:10.1177/1745691618755704\n\nOlmsted, D. (1834). Observations on the meteors of November \n13th,  1833.  American  Journal  of  Science  and  Arts,  26, \n354\xe2\x80\x93411.\n\nOpen  Science  Collaboration.  (2015).  Estimating  the  repro-\nducibility  of  psychological  science. Science,  349(6251), \nArticle aac4716. doi:10.1126/science.aac4716\n\nPan,  R.  K.,  Petersen,  A.  M.,  Pammolli,  F.,  &  Fortunato,  S. \n(2016). The memory of science: Inflation, myopia, and \nthe  knowledge  network.  Journal  of  Informetrics,  12, \n656\xe2\x80\x93678.\n\nPanagiotou, O. A., Willer, C. J., Hirschhorn, J. N., & Ioannidis, \nJ. P. (2013). The power of meta-analysis in genome-wide \nassociation  studies.  Annual  Review  of  Genomics  and \nHuman Genetics, 14, 441\xe2\x80\x93465.\n\nPetersen,  A.  M.,  Jung,  W.-S.,  Yang,  J.-S.,  &  Stanley,  H.  E. \n(2011). Quantitative and empirical demonstration of the \nMatthew effect in a study of career longevity. Proceedings \nof the National Academy of Sciences, USA, 108, 18\xe2\x80\x9323.\n\nPiccinin, A. M., Muniz, G., Clouston, S. A., Reynolds, C. A., \nThorvaldsson,  V.,  Deary,  I.,  .  .  .  Hofer,  S.  M.  (2013). \nIntegrative  analysis  of  longitudinal  studies  on  aging: \nCoordinated analysis of age, sex, and education effects \non  change  in  MMSE  scores.  Journal  of  Gerontology: \nPsychological Sciences, 68, 374\xe2\x80\x93390. doi:10.1093/geronb/\ngbs077\n\nPoetz, M. K., & Schreier, M. (2012). The value of crowdsourc-\ning: Can users really compete with professionals in gen-\nerating new product ideas? Journal of Product Innovation \nManagement, 29, 245\xe2\x80\x93256.\n\nPolymath, D. H. J. (2012). A new proof of the density Hales-\nJewett theorem. Annals of Mathematics, 175, 1283\xe2\x80\x931327.\nPolymath,  D.  H.  J.  (2014).  New  equidistribution  estimates \nof Zhang type. Algebra & Number Theory, 9, 2067\xe2\x80\x932199.\nPrice,  A.,  Turner,  R.,  Stencel,  R.  E.,  Kloppenborg,  B.  K.,  & \nHenden, A. A. (2012). The origins and future of the citi-\nzen sky project. Journal of the American Association of \nVariable Star Observers, 40, 614\xe2\x80\x93617.\n\nPrinz, F., Schlange, T., & Asadullah, K. (2011). Believe it or \nnot: How much can we rely on published data on poten-\ntial drug targets? Nature Reviews. Drug Discovery, 10, 712.\nRevolutionizing peer review? (2005). Nature Neuroscience, 8, \n\n397. doi:10.1038/nn0405-397\n\nSaez-Rodriguez, J., Costello, J. C., Friend, S. H., Kellen, M.\xc2\xa0R., \nMangravite,  L.,  Meyer,  P.,  .  .  .  Stolovitzky,  G.  (2016). \nCrowdsourcing biomedical research: Leveraging commu-\nnities  as  innovation  engines. Nature  Reviews  Genetics, \n17, 470\xe2\x80\x93486.\n\nSakaluk, J. K., Williams, A. J., & Biernat, M. (2014). Analytic \nreview as a solution to the misreporting of statistical results \nin  psychological  science. Perspectives  on  Psychological \nScience, 9, 652\xe2\x80\x93660.\n\n\x0c732 \n\nUhlmann et al.\n\nSalganik, M. J. (2017). Bit by bit: Social research in the digital \n\nage. Princeton, NJ: Princeton University Press.\n\nSalter, S. J., Cox, M. J., Turek, E. M., Calus, S. T., Cookson, \nW.\xc2\xa0O., Moffatt, M. F., . . . Walker, A. W. (2014). Reagent \nand  laboratory  contamination  can  critically  impact \nsequence-based microbiome analyses. BMC Biology, 12, \nArticle 87. doi:10.1186/s12915-014-0087-z\n\nSch\xc3\xb6nbrodt,  F.  (2018,  June  25).  Hiring  policy  at  the  LMU \nPsychology Department: Better have some open science track \nrecord. Nicebread. Retrieved from http://www.nicebread.de/\nopen-science-hiring-policy-lmu\n\nSchooler, J. (2014). Metascience could rescue the \xe2\x80\x98replication \n\ncrisis.\xe2\x80\x99 Nature, 515, 9.\n\nSchweinsberg,  M.,  Feldman,  M.,  Staub,  N.,  Prasad,  V., \nRavid, A., van den Akker, O., . . . Uhlmann, E. (2018). \nCrowdsourcing data analysis: Gender, status, and science. \nManuscript in preparation.\n\nSchweinsberg,  M.,  Madan,  N.,  Vianello,  M.,  Sommer,  S.  A., \nJordan, J., Tierney, W., . . . Uhlmann, E. L. (2016). The \npipeline  project:  Pre-publication  independent  replica-\ntions of a single laboratory\xe2\x80\x99s research pipeline. Journal \nof Experimental Social Psychology, 66, 55\xe2\x80\x9367.\n\nSchweinsberg,  M.,  Viganola,  D.,  Prasad,  V.,  Dreber,  A., \nJohannesson, M., Pfeiffer, T., . . . Uhlmann, E. L. (2018). \nThe pipeline project 2: Opening pre-publication indepen-\ndent replication to the world. Manuscript in preparation. \nRetrieved from https://osf.io/skq2b/\n\nSears,  D.  O.  (1986).  College  sophomores  in  the  labora-\ntory: Influences of a narrow data base on psychology\xe2\x80\x99s \nview of human nature. Journal of Personality and Social \nPsychology, 51, 515\xe2\x80\x93530.\n\nSeglen, P. O. (1997). Why the impact factor of journals should \nnot  be  used  for  evaluating  research.  British  Medical \nJournal, 314, 498\xe2\x80\x93502.\n\nSilberzahn,  R.,  Uhlmann,  E.  L.,  Martin,  D.,  Anselmi,  P., \nAust, F., Awtrey, E., . . . Nosek, B. A. (2018). Many ana-\nlysts,  one  data  set:  Making  transparent  how  variations \nin  analytic  choices  affect  results. Advances  in  Methods \nand  Practices  in  Psychological  Science,  1,  337\xe2\x80\x93356. \ndoi:10.1177/2515245917747646\n\nSimmons,  J.  P.,  Nelson,  L.  D.,  &  Simonsohn,  U.  (2011). \nFalse-positive  psychology:  Undisclosed  flexibility  in \ndata  collection  and  analysis  allows  presenting  any-\nthing as significant. Psychological Science,  22, 1359\xe2\x80\x93\n1366.\n\nSimons,  D.  J.  (2014).  The  value  of  direct  replication. \n\nPerspectives on Psychological Science, 9, 76\xe2\x80\x9380.\n\nSimonsohn,  U.  (2013).  Just  post  it:  The  lesson  from  two \ncases  of  fabricated  data  detected  by  statistics  alone. \nPsychological Science, 24, 1875\xe2\x80\x931888.\n\nSimonsohn,  U.,  Nelson,  L.  D.,  &  Simmons,  J.  P.  (2014). \nP-curve: A key to the file drawer. Journal of Experimental \nPsychology: General, 143, 534\xe2\x80\x93547.\n\nSobel, D. (2007). Longitude: The true story of a lone genius \nwho  solved  the  greatest  scientific  problem  of  his  time. \nLondon, England: Bloomsbury Publishing.\n\nS\xc3\xb8rensen,  J.  J.,  Pedersen,  M.  K.,  Munch,  M.,  Haikka,  P., \nJensen, J. H., Planke, T., & Sherson, J. F. (2016). Exploring \nthe quantum speed limit with computer games. Nature, \n532, 210\xe2\x80\x93213.\n\nSrinarayan, S., Sugumaran, V., & Rajagopalan, B. (2002). A \nframework for creating hybrid-open source software com-\nmunities. Information Systems Journal, 12, 7\xe2\x80\x9325.\n\nStanley, T. D., Carter, E. C., & Doucouliagos, H. (2018). What \nmeta-analyses  reveal  about  the  replicability  of  psycho-\nlogical research. Psychological Bulletin, 144, 1325\xe2\x80\x931346.\nSteward, O., Popovich, P. G., Dietrich, W. D., & Kleitman, \nN. (2012). Replication and reproducibility in spinal cord \ninjury research. Experimental Neurology, 233, 597\xe2\x80\x93605.\n\nStewart, N., Chandler, J., & Paolacci, G. (2017). Crowdsourcing \nsamples in cognitive science. Trends in Cognitive Sciences, \n21, 736\xe2\x80\x93748.\n\nStolovitzky, G. A., Monroe, D., & Califano, A. (2007). Dialogue \non  reverse-engineering  assessment  and  methods:  The \nDREAM  of  high-throughput  pathway  inference. Annals \nof the New York Academy of Sciences, 1115, 1\xe2\x80\x9322.\n\nSurowiecki, J. (2005). The wisdom of crowds. New York, NY: \n\nAnchor Books.\n\nTao, T., Croot, E., & Helfgott, H. (2012). Deterministic meth-\nods  to  find  primes.  Mathematics  of  Computation,  81, \n1233\xe2\x80\x931246.\n\nThelen, B. A., & Thiet, R. K. (2008). Cultivating connection: \nIncorporating meaningful citizen science into Cape Cod \nNational  Seashore\xe2\x80\x99s  estuarine  research  and  monitoring \nprograms. Park Science, 25, 74\xe2\x80\x9380.\n\nTierney, W., Schweinsberg, M., Jordan, J., Kennedy, D. M., \nQureshi, I., Sommer, S. A., . . . Uhlmann, E. L. (2016). \nData from a pre-publication independent replication ini-\ntiative examining ten moral judgment effects. Scientific \nData, 3, Article 160082. doi:10.1038/sdata.2016.82\n\nTierney,  W.,  Schweinsberg,  M.,  &  Uhlmann,  E.  L.  (2018). \nMaking  prepublication  independent  replication  main-\nstream  [Commentary]. Behavioral & Brain Sciences,  41, \nArticle e153. doi:10.1017/S0140525X18000894\n\nValderas, J. M., Buckley, R., Wray, K. B., Wuchty, S., Jones, \nB. F., & Uzzi, B. (2007). Why do team authored papers \nget cited more? Science, 317, 1496\xe2\x80\x931498.\n\nVisscher,  P.  M.,  Brown,  M.  A.,  McCarthy,  M.  I.,  &  Yang,  J. \n(2012). Five years of GWAS discovery. American Journal of \nHuman Genetics, 90, 7\xe2\x80\x9324. doi:10.1016/j.ajhg.2011.11.029\nWagenmakers, E.\xe2\x80\x93J., Wetzels, R., Borsboom, D., van der Maas, \nH.  L.  J.,  &  Kievit,  R.  A.  (2012).  An  agenda  for  purely \ncon\xef\xac\x81rmatory  research.  Perspectives  on  Psychological \nScience, 7, 627\xe2\x80\x93633.\n\nWagge, J. R., Baciu, C., Banas, K., Nadler, J. T., Schwarz, S.,  \nWeisberg, Y., . . . Grahe, J. (2019). A demonstration of \nthe  Collaborative  Replication  and  Education  Project: \nReplication attempts of the red-romance effect. Collabra, \n5(1), Article 5. doi:10.1525/collabra.177\n\nWahls,  W.  P.  (2018).  High  cost  of  bias:  Diminishing  mar-\nginal  returns  on  NIH  grant  3  funding  to  institutions. \nUnpublished manuscript.\n\nWells, G. L., & Windschitl, P. D. (1999). Stimulus sampling \nin social psychological experimentation. Personality and \nSocial Psychology Bulletin, 25, 1115\xe2\x80\x931125.\n\nWenger,  E.  (1998).  Communities  of  practice:  Learning, \nmeaning, and identity. Cambridge, England: Cambridge \nUniversity Press.\n\nWenneras,  C.,  &  Wold,  A.  (1997).  Nepotism  and  sexism  in \n\npeer-review. Nature, 387, 341\xe2\x80\x93343.\n\n\x0cCrowdsourcing Science \n\n733\n\nWestra, H.-J., Jansen, R. C., Fehrmann, R. S. N., te Meerman, \nG. J., van Heel, D., Wijmenga, C., & Franke, L. (2011). \nMixupMapper:  Correcting  sample  mix-ups  in  genome-\nwide  datasets  increases  power  to  detect  small  genetic \neffects. Bioinformatics, 27, 2104\xe2\x80\x932111.\n\nWilliamson,  I.  O.,  &  Cable,  D.  M.  (2003).  Predicting  early \ncareer  research  productivity:  The  case  of  management \nfaculty. Journal of Organizational Behavior, 24, 25\xe2\x80\x9344.\n\nWolfman-Arent, A. (2014, June 5). Frustrated scholar creates \nnew way to fund and publish academic work. Chronicle of \nHigher Education. Retrieved from https://www.chronicle \n.com/blogs/wiredcampus/frustrated-scholar-creates-new-\nroute-for-funding-and-publishing-academic-work/53073\n\nWuchty,  S.,  Jones,  B.  F.,  &  Uzzi,  B.  (2007).  The  increasing \ndominance  of  teams  in  the  production  of  knowledge. \nScience, 316, 1036\xe2\x80\x931038.\n\n\x0c'",Uhlmann et al. - Scientific Utopia III Crowdsourcing Science.pdf
"b' \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\nOpen Science in Developmental Science \n\nLisa A. Gennetian \n\nDuke University \n\nMichael Frank \n\nStanford University \n\n \n\n \n\nCatherine S. Tamis-LeMonda \n\nNew York University \n\nAcknowledgements \n\nWe thank Ryder Buttry and Taylor Cole for their assistance in compiling some of the background \n\ninformation and data for this paper. CTL acknowledges funding from the National Institutes of \n\nChild Health and Development (Grant number: R01HD094830).  \n\n \n\n \n\n1 \n\n\x0cAbstract \n\nOpen science policies have proliferated in the social and behavioral sciences in recent \n\nyears, including sharing study designs, protocols, and data, and pre-registering hypotheses. \n\nDevelopmental research has moved more slowly than some other disciplines in adopting open \n\nscience practices, in part because developmental science is often descriptive, and does not always \n\nstrictly adhere to a confirmatory approach. We assess the state of open science practices in \n\ndevelopmental science and offer a broader definition of open science that includes replication, \n\nreproducibility, data reuse, and global reach.  \n\n \n\n \n\n \n\n2 \n\n\x0c\xe2\x80\x9cProgress in science is marked by reducing uncertainty about nature.\xe2\x80\x9d (Nosek et al., 2018) \n\nDevelopmental science seeks to understand how children learn and change across the \n\nenvironments of everyday life. Towards this goal, the field aims to accumulate increasingly \n\nprecise observations and integrate them into theories that expand the breadth, depth, and \n\npredictive accuracy of knowledge (Munaf\xc3\xb2 et al., 2017). However, findings in developmental \n\npsychology \xe2\x80\x93 as many other fields \xe2\x80\x93 do not always conform to a model of cumulative science \n\n(Ioannidis, 2005; Moody et al., 2022). \n\n Many findings may not be replicable in new samples, meaning that new data yield results \n\ninconsistent with the original (Nosek et al., 2022; Open Science Collaboration, 2015). Perhaps \n\neven more troublingly, some scientific findings may not be computationally reproducible, \n\nmeaning that their numerical conclusions cannot be duplicated by a new researcher starting from \n\nthe same dataset (Hardwicke, 2019; Hardwicke et al., 2021; Stodden et al., 2018). Data reuse \xe2\x80\x93 in \n\nwhich researchers share their data to foster new questions and analyses \xe2\x80\x93 is also rare, leading to \n\nreduced impact of scientific investments. Finally, the lens of developmental science tends to be \n\nnarrow in global reach, with most studies conducted by scientists in North America and Europe \n\non children from a handful of countries (Kidd & Garcia, 2021; Nielsen et al., 2017). These four \n\nRs \xe2\x80\x93 broad goals for an open science movement \xe2\x80\x93 are summarized in Table 1. \n\nResearch practices that interfere with these four goals are exacerbated in child \n\ndevelopment research (Davis-Kean & Ellis, 2019; Frank et al., 2017). Developmental studies \n\noften have low statistical power due to small samples of convenience that arise from the costs and \n\nchallenges of recruiting and studying minors. Children are not always compliant participants, and \n\ncoupled with typical challenges related to time, scheduling, and life circumstances that interrupt \n\n \n\n \n\n \n\n3 \n\n\x0cadults and caregivers, high attrition rates magnify the complexities of securing valid and unbiased \n\ndata. Furthermore, because direct instruction is not possible with young participants (e.g., \n\ninfants), measurements may be inconsistent, and researchers are left to infer the meaning of \n\nhighly variable responses (e.g., looking time, object manipulation, heart rate, or head-turns). And, \n\nbecause much of developmental science relies on video-recorded observations, sharing of raw \n\ndata requires careful ethical consideration because faces and names are identifiable. Finally, \n\ndisproportionate resources and access to research by investigators from different regions of the \n\nworld limits the reach of developmental science.  \n\nOpen science practices are designed to address these challenges, but scientific disciplines \n\ndiffer in the extent to which they value and adopt such practices. Moreover, within-discipline \n\ndifferences may exist depending on research design (e.g., pre-registration is more widely used and \n\naccepted in randomized control trials). Thus, given the wide range of what falls under the \n\numbrella of developmental science, variation among researchers in open science practices is no \n\nsurprise. The adoption of open science is not all or none, however. Instead, each additional \n\npractice moves the field incrementally towards the broader goals of reproducibility, replicability, \n\nreuse, and global reach (Klein et al., 2018). \n\nOpen science practices are, if anything, even more critically central to developmental \n\nscience than to other subfields. Children as research participants are a vulnerable population of \n\ngreat societal importance and investment. And although public and private stakeholders uniformly \n\nagree on the need to protect children\xe2\x80\x99s well-being, there is no consensus on how best to do so, \n\neven within the scientific community. Thus, cultivating public trust in the scientific enterprise is \n\nparamount. Furthermore, the substantial financial and time investments required to study \n\nchildren\xe2\x80\x99s learning and development (including costs of equipment and time burden to parents \n\n \n\n \n\n4 \n\n\x0cand children) mean that researchers have an ethical obligation to maximize the value of public \n\ninvestment and to accelerate scientific discoveries through open science practices.  \n\nIn this review we present key goals for the broad application of open science practices in \n\ndevelopmental science; address the challenges in doing so; and survey the current state of the \n\nfield. Our review complements existing publications (Asarnow et al., 2018; Branney et al., 2019; \n\nGehlbach & Robinson, 2021; Klein et al., 2018; Nosek et al., 2022; Tackett et al., 2019) and adds \n\nto the wealth of available resources on open science practices by addressing issues specific to \n\ndevelopmental science. Notably, we move beyond narrow and rigid interpretations of open \n\nscience that have likely impeded its wider acceptance and adoption: Open science is not, and \n\nshould not be, narrowly defined by any specific policy, whether it be data sharing, preregistration, \n\nor badges rewarding such practices (Kidwell et al., 2016). A broad and flexible view of open \n\nscience must include practices that increase transparency at all stages of the scientific enterprise \n\n(Soska et al., 2021).  \n\nThe Goals of Open Science: The 4 R\xe2\x80\x99s \n\nThe open science movement pursues scientific transparency policies to enhance \n\nreplicability, improve reproducibility, accelerate discovery through reuse, and broaden global \n\nreach. In this section, we briefly review policies and practices in the service of each of these \n\ngoals. Box 1 provides select examples of precedent-setting initiatives in developmental science \n\nthat work towards these goals.  \n\nEnhancing the Replicability of Results in New Samples \n\nOne major catalyst for the open science movement has been the failure of independent \n\nresearchers to replicate the findings of many published studies (Moody et al., 2022; Nosek et al., \n\n2022; Open Science Collaboration, 2015). Much ink has been spilled around what precisely \n\n \n\n \n\n5 \n\n\x0cconstitutes replication and when such a study should be considered successful (Nosek & \n\nErrington, 2020; Zwaan et al., 2018). In developmental science, not every finding should be \n\nreplicable across samples and contexts. Yet failure of replication due to statistical issues in the \n\noriginal study is preventable: If a reported result is due to random sampling variation rather than \n\ntrue signal in the data, it is by definition un-replicable \xe2\x80\x93 leading to wasted efforts by teams that \n\nfollow up on it. The practice of \xe2\x80\x9cp-hacking\xe2\x80\x9d \xe2\x80\x93 taking advantage of undisclosed analytic flexibility \n\nto report statistically significant findings \xe2\x80\x93 may be a major source of replication failures (Nosek et \n\nal., 2022; Open Science Collaboration, 2015).  \n\nPreregistration \xe2\x80\x93 traditionally defined as specifying outcomes and analyses in an open and \n\nindependent repository before collecting data \xe2\x80\x93 is one response to the challenge of p-hacking. By \n\nreporting all conditions and measures in a study and planning hypothesis tests, experimenters can \n\nremove extra \xe2\x80\x9cdegrees of freedom\xe2\x80\x9d that may lead to spurious discoveries (Nosek et al., 2018; \n\nSimmons et al., 2011). Preregistration is mandated in clinical trials (e.g., via clinicaltrials.gov) to \n\navoid incorrect conclusions that could incur harm to patients; empirically, this policy has reduced \n\npublication and reporting bias (Kaplan & Irvin, 2015). Registered reports, in which manuscripts \n\nundergo peer review prior to data collection, are one especially promising form of preregistration \n\nfor some study types (Nosek & Lakens, 2014). \n\nNot all research in developmental science adheres to a hypothesis testing approach, \n\nhowever. Much research is descriptive or exploratory, with researchers annotating events based \n\non video or audio recordings or language transcriptions, and sometimes live observations, in the \n\nabsence of any expectations about what they will find. Such work provides critical insights into \n\ndevelopmental processes and can spur new hypotheses that are grounded in careful \n\ndocumentation of behavior. Although descriptive and exploratory research (rather than \n\n \n\n \n\n6 \n\n\x0cconfirmatory experiments) may not fit with current norms of pre-registration, many aspects of \n\nresearch plans can be registered before embarking on an exploratory or descriptive study. For \n\nexample, researchers can pre-register (1) the source of data, study design, methods, and \n\nstimuli/materials; (2) behaviors they plan to code from video-recorded observations or language \n\ntranscripts; (3) plans for calculating reliability, (4) approaches to data quality assurance; and (5) \n\ntentative plans for analysis, perhaps accompanied by a note that additional analyses will be \n\nconducted based on what researchers find (Kosie & Lew-Williams, 2022). Researchers who are \n\nunsure about certain components of their research plans can engage in sequential pre-registration \n\n(Nosek et al., 2018). Such steps increase transparency about pre-planned aspects of a study while \n\nmaintaining the flexibility necessary for description and discovery in rich datasets. \n\nIncreasing Analytic Reproducibility \n\nDevelopmental research is rich in methodological and analytic breadth that continues to \n\nexpand. For example, developmental research uses neuroscience, physiology, and computational \n\nmodels; the great reliance on analytic code in such work amplifies concerns about computational \n\nreproducibility. Because developmental psychology as a field has a newer culture around \n\ncomputational methods, cultural transfer of good coding practices such as the use of version \n\ncontrol and code review may be less common than in other subfields (Poldrack et al., 2017). \n\nThus, open science practices that seek to decrease errors and increase reproducibility are critical \n\nfor developmental research.  \n\nUnfortunately, errors in data analysis and reporting are common. An analysis of research \n\ntexts found that approximately half of published psychology articles contain at least one statistical \n\nreporting error (defined as a set of reported statistical values that are internally inconsistent with \n\none another), an estimate that is likely conservative (Nuijten et al., 2016). Furthermore, when \n\n \n\n \n\n7 \n\n\x0cteams of analysts download data from papers (which presupposes that the data are available), only \n\naround a third of findings can be fully reproduced without assistance from the authors (Chang & \n\nLi, 2017; Hardwicke et al., 2021). These findings suggest that when errors exist, they typically \n\ncannot be detected and corrected.  \n\nTransparency \xe2\x80\x93 sharing both analytic code and the data on which it operates \xe2\x80\x93 is both a \n\nmotivator for better organization and documentation by the original authors and a step that allows \n\nothers to identify errors or verify results (Klein et al., 2018). The creation and availability of \n\ncomputational technical tools that assist with reproducing analyses have facilitated the sharing of \n\ncode and data together (Aust & Barth, 2018; Kluyver et al., 2016). In some cases, requiring \n\nauthors to make data available as a condition of publication acceptance or the receipt of study \n\nfunding has also helped researchers incorporate time and resources costs as part of the production \n\nof research output.  \n\nThe availability of code and data not only facilitates the exact reproduction of a finding, it \n\nalso allows analysts to check the robustness of the finding. Robustness in this case is a term used \n\nto describe how sensitive a finding is to specific analytic decisions (including, for example, \n\ncoding of the dependent variable, inclusion or exclusion of certain independent variables or \n\ncovariates, and related modeling decisions). A finding can be analytically reproducible but \n\nnevertheless of limited utility if it only emerges when certain control variables are entered into a \n\nregression model (Duncan, 2014). Increasingly, when their findings depend on a complex analytic \n\nmodel, researchers are encouraged to include robustness checks and perhaps even report a \n\n\xe2\x80\x9dmultiverse\xe2\x80\x9d analysis in which they conduct a large set of different analyses to examine \n\nsensitivity to certain modeling or analytic decisions more formally (Steegen et al., 2016). \n\n \n\n \n\n8 \n\n\x0cAccelerating Discovery Through Reuse \n\nA third goal of open science is to maximize the value of research and enable new \n\ndiscoveries by leveraging prior research investments. A lack of transparency and sharing of \n\nresearch methods and raw data creates considerable inefficiencies in the scientific enterprise by \n\nbenefitting individual investigators while decreasing value for the broader scientific community. \n\nThe rich data and methods employed by developmental scientists may make reuse opportunities \n\neven more important in developmental science than in other fields (Gilmore & Qian, 2022). As \n\none example, video recordings of children and/or their interactions with caregivers are a \n\ncornerstone of many developmental studies that capture behaviors in the whole child and \n\nsurrounding context. Their wide availability can foster video-based behavioral coding and \n\nscientific insights beyond the original motivations or primary research questions (Adolph, 2020). \n\nSimilarly, readily available and publicly accessible data from resources such as the National \n\nLongitudinal Survey of Youth, the child supplement of the Panel Study of Income Dynamics, the \n\nNICHD Study of Child Care and Youth Development and the National Longitudinal Study of \n\nAdolescent Health have also been fundamental to accelerating discovery about the roles of \n\nchildren\xe2\x80\x99s broader family and social environments in development.  \n\nOpen science practices also allow researchers to aggregate data across children and \n\ndevelopmental contexts by combining available datasets to better understand mechanisms of \n\ndevelopmental change (Frank et al., 2021). Among its many possible contributions, aggregation \n\nallows researchers to build a sufficient sample to investigate developmental processes that affect \n\nnarrow populations (e.g., children with impaired vision or hearing). Policies that require \n\nresearchers to share materials, data, and code in findable, accessible, interoperable, and reusable \n\n(FAIR) repositories have been much more successful and effective thansharing \xe2\x80\x9con demand\xe2\x80\x9d or \n\n \n\n \n\n9 \n\n\x0c\xe2\x80\x9con request\xe2\x80\x9d (Wicherts et al., 2006). Such sharing maximizes the scientific value of a dataset, a \n\ngoal that is relevant for both research participants and funders (Brakewood & Poldrack, 2013).  \n\nFinally, repeating and building on a study may require recreating materials developed by \n\nanother investigator. Because developmental research can be sensitive to the nuances of particular \n\nstimuli, dissemination of materials can play an outsized role in guiding new experiments. \n\nAlthough proprietary attitudes about stimuli are common \xe2\x80\x93 perhaps because of a desire for \n\nexclusive access to stimuli that were costly to create or to avoid questions about original results \n\n(e.g., Phillips et al., 2015) \xe2\x80\x93 sharing of stimulus materials can be done in ways that credit creators \n\nand substantially increase the impact of published work. For example, all materials shared on sites \n\nlike Databrary and OSF contain a Digital Object Identifier (DOI) that should be cited to credit the \n\nresearcher who has shared resources (Simon et al., 2015).  \n\nExpanding Reach and Building Global Capacity \n\nA final objective of open science practices is to expand the reach and inclusivity of \n\ndevelopmental studies to represent researchers and children from regions across the globe. \n\nVariety in contexts, perspectives, and backgrounds is critical to building a global representative \n\ndevelopmental science. However, most developmental samples are typically drawn from a narrow \n\nrange of cultures, populations, and languages, which stymies  efforts to draw generalizable \n\nconclusions about the nature of developmental variation and change (Kidd & Garcia, 2021; \n\nNielsen et al., 2017). Nurturing scientific studies from a broad range of scholars and contexts can \n\nensure the inclusion of different populations of children through their knowledge of and \n\nsensitivity toward their cultural community. Perhaps even more importantly, researchers from \n\ndifferent cultural communities contribute new perspectives and questions that may be important \n\nin their context but overlooked in others. Practices that expand the reach of developmental science \n\n \n\n \n\n10 \n\n\x0cinclude open access to scholarly products, open sharing of research materials (from protocols to \n\nraw data of all kinds), and democratization of research evaluation.  \n\nOpen access to scholarly products means that researchers around the globe can access \n\nscientific research in a timely fashion. Much literature is published behind \xe2\x80\x9cpaywalls\xe2\x80\x9d, requiring \n\nan expensive institutional subscription for individuals to browse the literature freely. Individuals \n\nwithout institutional affiliations or those from less-resourced institutions often cannot access the \n\nmost recent literature, a clear obstacle to their scientific contributions.  \n\nOpen sharing of research products (e.g., protocols, stimuli, behavioral coding manuals, \n\nanalytic code, video recordings, and raw and processed flat-tile data) reduces the cost of research \n\nfor individuals in lower-resource contexts. Open sharing allows researchers to leverage existing \n\nmaterials and data in repositories for study replication, analytic reproducibility, and data reuse \n\n(e.g., coding new behaviors from video recordings of infants or by applying new analyses to \n\nlongitudinal datasets). Furthermore, large collaborative projects and repositories can bring \n\nprominence to the contributions of researchers from across the globe. \n\nFinally, open science practices that focus on transparency can shift emphasis away from \n\nthe evaluation of research products based on institutional and individual reputation towards \n\nevaluation of the contribution of research products. For example, by providing third-party \n\nendorsement of quality standards met by the study, clearinghouses and \xe2\x80\x9cwhat works\xe2\x80\x9d registries \n\n(Hill & Buckley, 2021; Home Visiting Evidence of Effectiveness Review, n.d.) ease demands on \n\nusers of research and incentivize transparency.  \n\nThe State of Open Science in Developmental Science \n\nGiven the benefits of open science practices and many recent papers on the topic, where \n\ndoes the field of developmental science currently stand? In 2016, the Society for Research in \n\n \n\n \n\n11 \n\n\x0cChild Development (SRCD, home to flagship journals of Child Development and Child \n\nDevelopment Perspectives) convened a committee on Open Science Practices, charged with \n\nassessing the state of the field and issuing a consensus statement on behalf of the society. Three \n\nyears later, the society ratified a policy suggesting that authors \xe2\x80\x9cnote if and where\xe2\x80\x9d their materials \n\nare shared, and that society publications consider the future and role of pre-registered and \n\nreplication studies in their flagship journals. The society\xe2\x80\x99s reluctance to mandate open-science \n\npractices reflected their sensitivity to variation in attitudes among society members and concerns \n\nthat open science practices can impede rather than generate and accelerate scientific progress \n\n(Gennetian et al., 2020; Gilmore et al., 2020). How has adoption of these practices progressed \n\nsince? In this section, we review uptake of open science policies in developmental science.  \n\nProgress toward replicability  \n\nFunders, journals, and professional societies have increasingly encouraged researchers to \n\nembrace practices around transparency and open sharing of all forms \xe2\x80\x93 including study methods \n\nand materials, the use of video for demonstrating study procedures, and the pre-registration of \n\nstudy questions and hypotheses. Such practices promise to increase the replicability of research. \n\nChanging expectations, attitudes, and behaviors around pre-registration offers one example. \n\nResearchers engaged in NIH funded clinical and behavioral interventions must now pre-register \n\ntheir research questions, hypotheses, analysis plans, etc. in clinicaltrials.org. Other research \n\nfunders (e.g., the Arnold Foundation) have followed this lead. Still, much basic developmental \n\nresearch \xe2\x80\x93 which largely does not entail randomized trials \xe2\x80\x93 is not subject to such mandates. \n\nNevertheless, researchers interested in preregistration can use independent registry sites such as \n\nAsPredicted.org or the Open Science Framework (OSF).  \n\nOSF represents one of the largest pre-registration portals available for developmental \n\n \n\n \n\n12 \n\n\x0cresearchers: It is host to over 10,000 projects identified under search terms \xe2\x80\x9cchild development,\xe2\x80\x9d \n\n\xe2\x80\x9ccognitive development,\xe2\x80\x9d \xe2\x80\x9cbrain development,\xe2\x80\x9d and \xe2\x80\x9csocio-emotional health.\xe2\x80\x9d Figure 1 shows for \n\nexample that the number of registered studies through OSF when searching terms based on child \n\ndevelopment epochs \xe2\x80\x9cinfant\xe2\x80\x9d or \xe2\x80\x9cadolescent\xe2\x80\x9d has increased seven-fold from 2012 to 2021. \n\nAlthough most preregistration templates focus on hypothesis testing for randomized experiments, \n\nguidelines on pre-registration of secondary data analyses (Akker et al., 2021), longitudinal \n\nresearch (Petersen et al., 2022), and descriptive, discovery science (Kosie & Lew-Williams, 2022) \n\nare emerging. Norms are slowly shifting with increases to preregistration of articles published in \n\nmany flagship journals of developmental science. We offer a glance at this progress within a set \n\nof 20 peer-reviewed journals in Table 1.  \n\nReproducibility and reuse: Data sharing policies \n\nData sharing is increasing in psychology. More than half of psychology researchers \n\nreported sharing their data, although the quality and reusability of shared datasets is often low \n\n(Borghi & Gulick, 2021; Hardwicke, 2019; Towse et al., 2021). Moreover, what constitutes \n\nsharing and what is shared (e.g., raw data vs. processed data; tabular data vs. video recordings) \n\ndiffers enormously across researchers.  \n\nAs the primary mechanism for production of scientific scholarship, peer reviewed journals \n\nplay a substantive role in shaping policies around data sharing and transparency. Yet journals \n\nhave been reluctant to mandate data sharing or to enforce data-sharing policies (Wicherts et al., \n\n2006). The Transparency and Openness Promotion (TOP) guidelines attempted to create uniform \n\nstandards around data sharing practices (Nosek et al., 2015), but their adoption has been \n\npiecemeal (see Table 1). In contrast, funder mandates for data sharing have moved more quickly. \n\nSince 2011, the US National Science Foundation has required its grantees to submit a data \n\n \n\n \n\n13 \n\n\x0cmanagement plan, and the US National Institutes of Health mandated open data sharing for its \n\ngrantees starting in 2023. Other major funders of developmental science have similarly adopted \n\ndata sharing mandates, including the Wellcome Trust (2016) and the Robert Wood Johnson \n\nFoundation (2019). The next frontier for such policies will be to ensure that data are shared in a \n\nway that maximizes their value so that they are discoverable and can be combined with other data \n\nto accelerate scientific discovery. \n\nExpanding global reach \n\nGrowing awareness of the need for diversity, equity, and inclusion in psychological \n\nstudies has spurred efforts to broaden the representation of underrepresented groups around the \n\nworld among both the scientists conducting research and the children participating in research. \n\nSuch efforts include open access to publications; global access and contributions to the scientific \n\ndatabase on children\xe2\x80\x99s development; transparency and thoroughness in reporting sample \n\ndemographics in published works; and mechanisms for scientists to broadly collaborate on study \n\ndesign, implementation, and sharing of research products.  \n\nOpen access to research products. Despite widespread endorsement of open access to \n\npublished studies \xe2\x80\x93 with many researchers viewing open access as an ethical imperative \xe2\x80\x93 the \n\nadoption of such practices has been inconsistent and rife with challenges. Progress towards open \n\naccess in psychology has been considerable, though far from perfect. By one estimate, more than \n\n50% of published psychology articles are available through some form of open access. Within \n\ndevelopmental science, \xe2\x80\x9cgold\xe2\x80\x9d (paid) open access is an option for many journals (Table 1). For \n\nopen access advocates, the key challenge has been engaging funders and stakeholders to avoid \n\nexacerbating disparities by burdening authors with extra publication fees (Brainard, 2021) in what \n\nrisks becoming a \xe2\x80\x9cpay to publish\xe2\x80\x9d system.  \n\n \n\n \n\n14 \n\n\x0cYet the biggest change in the open access landscape has been the rise of \xe2\x80\x9cgreen\xe2\x80\x9d open \n\naccess. Fueled in part by the need to disseminate research rapidly during the covid-19 pandemic \n\n(Watson, 2022), preprint servers like arXiv, bioArXiv, and psyArXiv have grown tremendously. \n\nAt the time of writing, psyArXiv hosts more than 22,000 psychology papers and is growing by \n\naround 20 papers per day, with authors typically posting papers before submitting for publication \n\nat a peer-reviewed journal.  \n\nPreprints serve several purposes. First, they guarantee access to published work in un-\n\ntypeset form even if the eventual published article is inaccessible. Second, and perhaps more \n\nimportant to their adoption, they allow authors to share their work when it is completed, \n\naddressing the career hurdles that may arise when papers are under review for months or even \n\nyears. By listing preprints on a CV or in a grant application, early career researchers can gain \n\ncredit for what they have accomplished to date, rather than only at the end of a long and uncertain \n\npublication process. Finally, by making research results available regardless of publication \n\noutcomes, preprints can help counter biases in the literature caused by the reluctance of some \n\njournals to publish null findings, failed replications, or applications of unconventional theories or \n\nmethods (Wingen et al., 2022).  \n\nTransparency about samples. In 2020, the Society for Research on Child Development \n\nenacted policies on the reporting of sample characteristics, a direction spearheaded by former \n\nEditor in Chief Cynthia Garcia Coll. Specifically, SRCD\xe2\x80\x99s policy requires authors to clearly \n\nspecify recruitment procedures and sampling decisions; participant characteristics including \n\nrace/ethnicity, socioeconomic status, native language; and contextual information to frame the \n\nstudy, including the country, region, city, and so on where data were collected. Full transparency \n\non the characteristics and contexts of developmental studies serves the dual goals of increasing \n\n \n\n \n\n15 \n\n\x0cdiversity in the participants of research and advancing efforts towards the replicability of findings.  \n\nAccess to and contributing data to the knowledge base. The broad and open sharing of \n\nresearch methods, behavioral coding manuals, analytic code, and raw and processed data of all \n\nforms \xe2\x80\x93 in repositories such as OSF and Databrary \xe2\x80\x93 supports diversity, equity, and inclusion by \n\nallowing researchers from around the world to both access and contribute to the knowledge base \n\non child development. Researchers only require an internet connection to download \n\ndevelopmental data of all forms to ask new questions on existing data or to leverage research \n\nstimuli, methods, etc. to replicate and extend existing studies to new samples. Moreover, the \n\nability of researchers to contribute data openly provides a valuable mechanism for expanding the \n\nknowledge base on child development \xe2\x80\x93 from the overly narrow samples of children represented \n\nin research today to samples drawn from around the world in future studies.  \n\nCreating spaces for collaboration. Cross-lab collaboration among researchers from \n\ndifferent countries or regions of a country (e.g., rural and urban) allows investigators from \n\ndifferent backgrounds, languages, and cultures to share their expertise, samples, and resources to \n\naddress topics of mutual interest (Frank et al., 2017). Such collaborations in turn help ensure that \n\ndevelopmental theories and findings are grounded in the experiences of children from a variety of \n\nbackgrounds. Collaborative science also allows researchers to pool resources across sites to \n\nrecruit a larger sample than would be possible by a single lab, an approach that is particularly \n\nimportant when recruiting children from special populations where generating a sample of \n\nsufficient size is a challenge (e.g., children with disabilities). Exciting new initiatives around \n\nconsortia and data repositories, described next, provide platforms to facilitate collaboration and \n\nextend the reach of developmental science.  \n\n \n\n16 \n\n \n\n\x0cConsortia and Repositories: New Drivers of Open Science \n\nAs reviewed, the adoption of open science practices in developmental science has been \n\nuneven. At the same time, the field is steadily expanding. One of the most exciting trends in the \n\nopen science landscape is the rise of open, collaborative mega-projects. These are sometimes \n\ndescribed under the umbrella of \xe2\x80\x9cbig team science\xe2\x80\x9d (Coles et al., 2022), in which difficult \n\nresearch challenges \xe2\x80\x93 data collection, behavioral coding, transcriptions, materials creation, and \n\nanalyses \xe2\x80\x93 are distributed across a community, creating a larger and more impactful product by \n\nvirtue of the collaboration. In developmental science, collaborative projects and big team science \n\nhave typically taken the form of either data collection consortia or collaborative data repositories. \n\nHere we highlight a few select examples to illustrate how such mechanisms can drive progress \n\ntowards open science goals.  \n\nConsortium research \n\nSeveral developmental science consortia have adopted collaborative research initiatives \n\nthat are grounded in an open-science framework. For example, ManyBabies (MB) is a research \n\nconsortium for pursuing multi-lab collaborative projects. The first ManyBabies project was a \n\nreplication of the well-known phenomenon of an infant-directed speech preference. The goal was \n\nto measure variability in findings across infant populations and experimental methods \n\n(ManyBabies Consortium, 2020); with 67 labs contributing data, this project represents one of the \n\nlargest experimental investigations of infancy to date. Initially devised as a way of pursuing best-\n\npractices replications of important phenomena (Frank et al., 2017), the consortium has \n\nincreasingly taken on new projects that require coordinated effort to solve a difficult theoretical or \n\nempirical challenge. Critically, open science practices are woven throughout ManyBabies \n\nprojects: All projects are preregistered (typically as Registered Reports); all materials, analytic \n\n \n\n \n\n17 \n\n\x0ccode, and data are released as part of the project; and participation is open to the entire research \n\ncommunity. By actively pursuing global collaborations and by lowering the barriers to \n\nparticipation, ManyBabies aims to broaden the set of labs and investigators contributing to cutting \n\nedge experimental work in developmental science, with a special focus on training graduate \n\nstudents and providing mentorship on open science practices.  \n\nThe NIH-funded Play and Learning throughout A Year (PLAY) project describes a \n\ncomplementary but distinct model for pursuing open science goals within consortium research \n\n(play-project.org). Through a model of synergistic, collaborative science among 72 \n\ndevelopmental researchers, PLAY will gather 1000+ hours of video of 12\xe2\x80\x9324 month-old children \n\nand their mothers engaged in everyday activities in the home environment across 30 \n\ngeographically distributed sites in the United States; transcribe language interactions between \n\ninfants and mothers; and collaboratively annotate videos for behaviors of locomotion, emotion, \n\nobject play, and language pragmatics (across 48 behavioral coding labs) following consensus \n\nstandards created by subgroups of participating domain experts. The goal of PLAY is to create an \n\nopenly available video corpus with annotations and associated metadata for authorized \n\ninvestigators on Databrary who can address new questions about learning and development \n\nduring the second postnatal year. As with ManyBabies, PLAY openly shares consensus best-\n\npractice methods and guides that can be applied beyond the original study, including behavioral \n\ncoding manuals, transcription guidelines, and newly created survey instruments for Spanish-\n\nEnglish dual-language learners. Most centrally, PLAY uses video to demonstrate all aspects of the \n\nstudy protocol (e.g., recruitment of families; interviews with mothers about children\xe2\x80\x99s vocabulary, \n\ntemperament, locomotor skills; how to conduct video tours of participants\xe2\x80\x99 homes, and so on).  \n\nThese two projects \xe2\x80\x93 and others in psychology broadly (e.g., the Psychological Science \n\n \n\n \n\n18 \n\n\x0cAccelerator; Moshontz et al., 2018) \xe2\x80\x93 share a commitment to open science practices with goals to \n\nbroadly disseminate science practices and build capacity for researchers who likewise seek to \n\nengage in open science. For example, participants in the first ManyBabies study noted in an \n\ninformal survey that they were much more likely to adopt open science practices in their own labs \n\nhaving tried them in a supportive context (Byers-Heinlein et al., 2020). In addition, use of the \n\nopen standards, tools, and materials created by these projects provide illustrative models on the \n\nvalue of transparency for the next generation of student trainees and non-participating labs. \n\nOpen data repositories \n\nData repositories and platforms both promote open science and allow for generative \n\nresearch beyond the boundaries of original data collection goals or investigators. Large data \n\nrepositories such as the Inter-University Consortium for Political and Social Research (ICPSR) \n\nprovide a database of longitudinal and national studies with information about children\xe2\x80\x99s \n\ndevelopmental environments and outcomes (e.g., the National Survey of Children\xe2\x80\x99s Health).   \n\nIndeed, some of these repositories have arrangements with professional organizations to also \n\nserve as co-hosts for data deposits of published research articles (e.g. OpenICPSR collaborates \n\nwith, for example, the American Education Research and American Economic Associations).   \n\nBelow, we describe three successful examples specific to developmental research that have \n\novercome the perceived barriers of broad data availability related to working with vulnerable \n\npopulations and collecting sensitive data.  \n\nTalkBank is an open-access repository of naturally occurring spoken language \n\n(MacWhinney, 2019). The TalkBank corpora currently contains 122 million words across 44 \n\nlanguages, and it has generated over 8000 publications across thousands of users. Developmental \n\nresearchers have leveraged the openly shared transcripts of TalkBank to tackle questions about \n\n \n\n \n\n19 \n\n\x0clanguage learning and speech production in monolingual and bilingual populations from diverse \n\ncultural communities, individuals experiencing language disfluencies (e.g., stuttering), persons \n\nwith autism spectrum disorder, and so on, and it even contains transcripts from classroom \n\nsettings. TalkBank\xe2\x80\x99s reliance on an openly shared uniform transcription format (CHAT), \n\nfacilitates combining language data across studies, and thus allows researchers to analyze various \n\ncomponents of language \xe2\x80\x93 phonology, lexicon, syntax, and discourse \xe2\x80\x93 across larger samples than \n\nwould be possible in a single lab. \n\nDatabrary (databrary.org) is a restricted access video data library specialized for sharing, \n\nstoring, and streaming video-based data in the form of inherently identifiable video and audio \n\nrecordings of children with and without caregivers or demonstrating research methods. The \n\ndatabase allows researchers to ask new questions by accessing video recordings of children of \n\ndifferent ages, in different settings, engaged in a rich variety of activities (e.g., bookreading with \n\ncaregivers, natural play in the home environment). Currently, Databrary has over 1,500 \n\ninvestigators at 700+ institutions across 5 continents, and contains over 90,000+ hours of video \n\nrecordings. Databrary consent forms offer participants options for different release levels for data \n\nstored in the database, including sharing with authorized researchers, learning audiences (e.g., \n\ntalks and classes), or the broader public.  \n\nWordbank is an open database of children\xe2\x80\x99s vocabulary development, archiving data from \n\na specific parent report instrument for measuring children\xe2\x80\x99s early language, the MacArthur-Bates \n\nCommunicative Development Inventory (Frank et al., 2016). The repository contains data from \n\nmore than 78,000 children and 39 languages, allowing for a rich characterization of how language \n\nlearning varies across cultures and contexts (Frank et al., 2021). Unlike broader repositories such \n\nas OSF and Databrary, Wordbank takes advantage of the uniform format of its data to allow for \n\n \n\n \n\n20 \n\n\x0caccess and browsing of the data through both interactive online visualizations and a programmatic \n\ninterface.  \n\nChallenges for Open Science in Developmental Science \n\nAlthough developmental science has made substantial progress in embracing open science \n\nduring the past ten years, many obstacles remain. In this section, we discuss three perceived \n\nchallenges for open science in developmental science: tensions between exploration and \n\nconfirmation, the potential for career risks, and how open science practices intersect with the goal \n\nof fostering an inclusive science. Each of these critiques is important to understand in its own \n\nright, and also because critiques of specific, narrowly construed policies can unproductively \n\nundermine perceptions of the value of open science more broadly.  \n\nDoes open science limit exploration and discovery?  \n\nInitial open science reforms focused on correcting perceived issues in experimental \n\npsychology, for example by reducing the risk of p-hacking. Thus, many policies initially assumed \n\nthat preregistration simply entailed registering a hypothesis about condition differences and \n\ndepositing a single tabular data file in a repository. Although this narrow conceptualization may \n\nbe a useful starting point, it does not conform to many developmentalist scientists\xe2\x80\x99 vision of their \n\nwork, which can involve observational designs, rich behavioral and/or physiological measures, \n\nlongitudinal observations, and deep appreciation of the limitations of generalizing across samples \n\nwith different characteristics.  \n\nRecent reforms have been increasingly sensitive to the broader set of research goals and \n\nactivities that define developmental science. For example, preregistrations may productively take \n\nmany forms that promote transparency about the research process without a strict focus on \n\nhypothesis testing, as reflected in the breadth of preregistration templates available on the Open \n\n \n\n \n\n21 \n\n\x0cScience Framework. Furthermore, initiatives like ManyBabies recognize that the combination of \n\nrich data and privacy concerns may lead to the need for more careful data sharing policies (e.g., \n\nsharing tabular data via an open channel like OSF and sharing raw video data through a more \n\nrestricted access channel such as Databrary).  \n\nDoes open science pose risks to career enhancement and development?  \n\nIn a casual interaction between one of the authors of this paper and an early career scholar \n\nin psychology, a query about pre-registering new exploratory work was met with dismay by the \n\nearly career scholar who declared, \xe2\x80\x9cif I pre-register this pilot work, I\xe2\x80\x99ll get yelled at by everyone \n\nfor what I did not get right.\xe2\x80\x9d Transparency in research is not always comfortable! Sharing data \n\nand analysis code can lead researchers to worry that mistakes in their work will be identified. Yet \n\nthis worry may be misplaced: Researchers tend to be judged by whether they are open to \n\ncorrecting issues or errors in their work, not whether they are correct to begin with (Ebersole et \n\nal., 2016). \n\nSharing resources prior to publication can also lead researchers to worry that their work \n\nwill be \xe2\x80\x9cscooped\xe2\x80\x9d or their ideas will be co-opted. Both might have more severe consequences for \n\nearly career researchers. Collision of ideas is a part of science, especially in fast-moving fields \n\n(Kim & Corn, 2018), but \xe2\x80\x93 when used appropriately \xe2\x80\x93 open science practices can be part of the \n\nsolution, not part of the problem. For example, posting preprints is an important way to establish \n\nprecedence independent of the vagaries of peer review (Kriegeskorte, 2016). And while scooping \n\nis a real issue, its risks can be mitigated by strategic decision-making about which data to release \n\nand when (Popkin, 2019). For example, a researcher might decide to make a public release of \n\nonly enough data from a larger dataset to ensure reproducibility of the key results in a paper, \n\nembargoing the full raw dataset for a set period (e.g., two years) to allow for completion of a \n\n \n\n \n\n22 \n\n\x0cpublication(s). This step would signal to observers that the researcher values openness and \n\nsharing, while also protecting the researcher\xe2\x80\x99s future interest.  \n\nAlthough it can be challenging for researchers to navigate the changing landscape of open \n\nscience practices, many of these practices can have positive career value \xe2\x80\x93 by allowing greater \n\ntransparency in understanding a researcher\xe2\x80\x99s contributions and by signaling a commitment to \n\nshared scientific values. Senior researchers should encourage doctoral and postdoctoral \n\nresearchers in their labs \xe2\x80\x93 the next generation of developmental scientists \xe2\x80\x93 to list open-science \n\npractices on their resumes (e.g., publications for which they share video, stimuli, methods, \n\nbehavioral coding manuals, data, analytic code, etc.) so they can be properly credited for their \n\ncommitment to scientific transparency and integrity.  \n\nDoes open science raise barriers for inclusion and diversity in who conducts science? \n\nMany open science policies are compatible with \xe2\x80\x93 and in some cases, designed to \n\n\xe2\x80\x93 decrease historical barriers to involvement in science. Open access policies, especially preprint \n\nposting, democratize access to the scientific literature, removing the barrier of access to \n\ninstitutional journal subscriptions. Data, code, and materials sharing policies allow researchers \n\nwithout personal access to a scientific network to access experimental materials and analytic \n\ntechniques to the same extent as those who occupy positions of privilege in the social network of \n\nscience. Such changes are necessary to decrease inequities in science \xe2\x80\x93 but they are not in \n\nthemselves sufficient. To diversify the research community in developmental science beyond \n\nmajority groups in a small number of nations, developmental scientists must engage in active \n\noutreach, seeking to build networks that broaden participation and construct training pipelines and \n\ncollaborative ventures that lead scholars from a wide range of backgrounds into the field. Such \n\ncollaborations must be reciprocal in nature to ensure they are grounded in community input (i.e., \n\n \n\n \n\n23 \n\n\x0cparticipatory research) at all stages of the research process \xe2\x80\x93 from questions, to design and \n\nmethods, to implementation of the study, to analyses and dissemination of findings.  \n\nWhen scientists engage with complex social issues, the chance of skepticism and criticism \n\nis high. Motivated individuals may seek to undermine findings that do not align with their \n\npersonal beliefs, subjecting evidence to increased scrutiny based on desired outcomes rather than \n\nthe quality of research. In the face of such scrutiny, researchers may be tempted to withhold \n\ndetails and to shy away from transparency. But researchers should do precisely the opposite. \n\nWhen work is conducted with full transparency, critics and advocates can assess the strength of \n\nthe evidence and researchers can easily refute charges of selective or biased reporting. \n\nFurthermore, because controversial areas of research are precisely where public trust in science is \n\nlikely to be lowest, researchers should be as scrupulous as possible in following policies that are \n\ndesigned to maximize credibility.  \n\nSuggestions for the Future \n\nThe 4Rs \xe2\x80\x93 replication, reproducibility, reuse, and reach \xe2\x80\x93 offer an organizing action-\n\noriented framework for ongoing adoption and adaptation of open science practices in \n\ndevelopmental science. As described, funder, journal, and institutional policies are key \n\ninstruments for encouraging the adoption of open science practices. Yet, implemented in \n\nisolation, many policies meet with resistance and avoidance. Policy alone is not enough to \n\nproduce concrete changes to behavior without educating stakeholders about why policies exist \n\nand how to follow them, fostering changes in social norms, creating incentives for compliance, \n\nand building capacity. We discuss each of these directions in turn.  \n\nEducating stakeholders \n\nPolicy interventions are most effective when the people they affect understand them \n\n \n\n \n\n24 \n\n\x0c(Weisman & Markman, 2017), especially when compliance with a policy requires learning new \n\ntools or thinking through new issues. Thus, for open science policies to be maximally effective, \n\nresearchers need to be educated about how and why to carry them out. First, the whys and hows \n\nof open-science practices should be included in curricula of research methods classes so that \n\nstudents understand the potential pitfalls of closed science and learn reproducible, transparent \n\nworkflows from the outset (Frank & Saxe, 2012; Hawkins* et al., 2018). Educating students \n\nabout open science also helps to create a pipeline of researchers who are passionate about \n\ncollaborative, open research, and helping to change norms (Tackett et al., 2019). Second, beyond \n\nthe classroom, researchers need access to educational resources that help them to understand and \n\neasily deploy open sciences practices. Professional societies must play a role by offering pre-\n\nconferences, webinars, and workshops to facilitate adoption of open science practices. Funders \n\nshould incorporate budget lines for open sharing practices (e.g., open access to publications that \n\narise from a grant; funds to prepare data so they are easily findable and usable by others). \n\nFurthermore, organizations and consortia such as ManyBabies and the PLAY project can take \n\nresponsibility for educating participants about the values and reasons for adopting open science \n\nAs social scientists know, behavior change is difficult, especially when researchers may \n\nperceive costs or risks in adopting new practices \xe2\x80\x93 as is precisely the situation for open science \n\nreforms. In the face of adoption challenges, one of the most effective tools for behavior change is \n\nthrough a change in social norms (Sunstein, 2019). If researchers perceive that the norm in their \n\nfield is to post preprints, share data and materials, and to preregister the details of their studies, \n\nthey will be more likely to adopt such practices, even if they are costly. To change norms \xe2\x80\x93 and to \n\npractices. \n\nChanging norms \n\n \n\n \n\n25 \n\n\x0cchange researchers\xe2\x80\x99 perceptions of norms \xe2\x80\x93 the open science movement needs to make visible the \n\nrapid adoption of open science practices over the past ten years. Such visibility can be especially \n\npowerful when social referents, scientists, and organizations with high status within their \n\ncommunity advocate the practices (Prentice & Paluck, 2020). Thus, individual scientists \xe2\x80\x93 \n\nespecially more prominent or senior ones \xe2\x80\x93 have the responsibility to adopt open science practices \n\nand endorse and advocate for those practices. Even an act as simple as highlighting policy \n\ncompliance during peer review (\xe2\x80\x9cI commended the authors for sharing their code and data to \n\nallow me to reproduce their results\xe2\x80\x9d) can signal shifting norms. Indeed, communicating values \n\nand norms around open science through modeling, direct encouragement, and explanations about \n\nthe value of full transparency may be more powerful than signaling style strategies such as \n\nawarding badges to researchers who engage in open science. While 4 of the 20 journals that we \n\nreviewed give badges, this practice has engendered some mixed feelings in the community and \n\nhas received limited support as an intervention (Rowhani-Farid et al., 2020).  \n\nCreating incentives \n\nCrediting open science practices through informal and formal mechanisms can go a long \n\nway in incentivizing researchers to engage in such practices. Informally, individual lab policies \n\ncan endorse open science practices, and encourage and train students to pre-register and openly \n\nshare their research questions, hypotheses, protocols, behavioral coding, videos, raw data, analytic \n\ncode, and so on in existing repositories. Course assignments that encourage students to access and \n\nuse openly available data can highlight the value of open science by making them beneficiaries of \n\nthe research investments made by others, and to one day pay it forward themselves. Such \n\ngrassroots efforts will go a long way in producing the next generation of advocates for open \n\nscience practices in developmental research.  \n\n \n\n \n\n26 \n\n\x0cFormal incentives for open science practices can be instituted through a variety of \n\nmechanisms, including creation of DOIs associated with shared stimuli, procedures, and data to \n\nbe cited by researchers who access shared resources; consideration of open science practices in \n\npersonnel and tenure reviews of faculty hires and promotions; required sections of grant \n\napplications stating how and when research products will be shared; and funding streams for the \n\npreparation of data for open sharing and the reuse of existing datasets to maximize investments. \n\nBuilding capacity \n\nOne of the most common responses to policy shifts around data sharing is grumbling \n\naround \xe2\x80\x9cunfunded mandates.\xe2\x80\x9d To recognize the importance of open science, institutions and \n\nfunders must be willing to direct resources towards building and sustaining these practices. For \n\nexample, the new NIH data sharing policy explicitly lists data sharing costs as allowable budget \n\nitems; other funders would do well to make this norm explicit. The creation and maintenance of \n\ndata sharing resources like Talkbank, Databrary, and Wordbank is also extremely resource \n\nintensive; although all three were created with US federal funding, few programs exist for funding \n\nthe continued maintenance of such resources.   \n\nFinally, global collaboration requires mechanisms for creating capacity across institutions, \n\nespecially outside the United States. As an example, the first ManyBabies study received a \n\n$50,000 grant from the Association for Psychological Science, which was regranted to \n\nparticipating labs around the world in small amounts from $500 - $2,500. Under most funding \n\nmodels, such small grants are impossible \xe2\x80\x93 for example, creating US federal subcontracts for \n\nthese amounts would require dozens of hours of paperwork and paying overhead to two \n\ninstitutions \xe2\x80\x93 yet they create a substantial incentive to participate in an open, global network.  \n\n \n\n \n\n27 \n\n\x0c     Conclusion \n\nA narrow conception of open science can lead to a perceived mismatch between overly \n\nspecific policies and the breadth of research that falls under the umbrella of developmental \n\nscience. This conception is mistaken. We argue for a broad view of open science that revolves \n\naround four goals: increasing replicability, ensuring reproducibility, facilitating reuse, and \n\nbroadening global reach. Despite impressive progress towards each of these goals in the last ten \n\nyears, challenges remain. Nonetheless, there is reason to be optimistic: new practices like \n\npreprinting are expanding access to the scientific literature; new policy guidelines by funders, \n\njournals, and so on are ensuring greater levels of data sharing; and new consortia and repositories \n\nare leading the way towards a representative and robust research. These exciting new \n\ndevelopments illustrate the ways that open and transparent science practices can be adapted to the \n\nrich variety of perspectives, methods, samples, and goals of developmental science. \n\n \n\n \n\n28 \n\n\x0cReferences \n\nAdolph, K. E. (2020). Oh, Behave! PRESIDENTIAL ADDRESS, XXth International Conference \n\non Infant Studies New Orleans, LA, US May 2016. Infancy, 25(4), 374\xe2\x80\x93392. \n\nAkker, O. R. van den, Weston, S., Campbell, L., Chopik, B., Damian, R., Davis-Kean, P., Hall, \n\nA., Kosie, J., Kruse, E., Olsen, J., Ritchie, S., Valentine, K. D., Veer, A. van \xe2\x80\x99t, & Bakker, \n\nM. (2021). Preregistration of secondary data analysis: A template and tutorial. Meta-\n\nPsychology, 5. https://doi.org/10.15626/MP.2020.2625 \n\nAsarnow, J., Bloch, M. H., Brandeis, D., Alexandra Burt, S., Fearon, P., Fombonne, E., Green, J., \n\nGregory, A., Gunnar, M., & Halperin, J. M. (2018). Special editorial: Open science and \n\nthe Journal of Child Psychology & Psychiatry\xe2\x80\x93next steps? In Journal of Child Psychology \n\nand Psychiatry (Vol. 59, Issue 7, pp. 826\xe2\x80\x93827). Wiley Online Library. \n\nAust, F., & Barth, M. (2018). papaja: Create APA manuscripts with R Markdown. \n\nBorghi, J. A., & Gulick, A. E. V. (2021). Data management and sharing: Practices and \n\nperceptions of psychology researchers. PLOS ONE, 16(5), e0252047. \n\nhttps://doi.org/10.1371/journal.pone.0252047 \n\nBrainard, J. (2021). Open access takes flight. Science, 371(6524), 16\xe2\x80\x9320. \n\nhttps://doi.org/10.1126/science.371.6524.16 \n\nBrakewood, B., & Poldrack, R. A. (2013). The ethics of secondary data analysis: Considering the \n\napplication of Belmont principles to the sharing of neuroimaging data. Neuroimage, 82, \n\nBranney, P., Reid, K., Frost, N., Coan, S., Mathieson, A., & Woolhouse, M. (2019). A context-\n\nconsent meta-framework for designing open (qualitative) data studies. Qualitative \n\n671\xe2\x80\x93676. \n\n \n\nResearch in Psychology. \n\n \n\n29 \n\n\x0cByers-Heinlein, K., Bergmann, C., Davies, C., Frank, M. C., Hamlin, J. K., Kline, M., ..., & \n\nSoderstrom, M. (2020). Building a collaborative Psychological Science: Lessons from \n\nManyBabies 1. Canadian Psychology, 61, 349\xe2\x80\x93363. https://doi.org/10.1037/cap0000216 \n\nChang, A. C., & Li, P. (2017). A preanalysis plan to replicate sixty economics research papers \n\nthat worked half of the time. American Economic Review, 107(5), 60\xe2\x80\x9364. \n\nColes, N. A., Hamlin, J. K., Sullivan, L. L., Parker, T. H., & Altschul, D. (2022). Build up big-\n\nteam science. Nature, 601(7894), 505\xe2\x80\x93507. https://doi.org/10.1038/d41586-022-00150-2 \n\nDavis-Kean, P. E., & Ellis, A. (2019). An overview of issues in infant and developmental \n\nresearch for the creation of robust and replicable science. Infant Behavior and \n\nDevelopment, 57, 101339. https://doi.org/10.1016/j.infbeh.2019.101339 \n\nDuncan, G. J. (2014). Replication and robustness in developmental research. Developmental \n\nPsychology, 50(11), 2417. https://doi.org/10.1037/a0037996 \n\nDuncan, G. J., & Brooks-Gunn, J. (1997). Consequences of growing up poor. Russell Sage \n\nFoundation. \n\nEbersole, C. R., Axt, J. R., & Nosek, B. A. (2016). Scientists\xe2\x80\x99 Reputations Are Based on Getting \n\nIt Right, Not Being Right. PLOS Biology, 14(5), e1002460. \n\nhttps://doi.org/10.1371/journal.pbio.1002460 \n\nFrank, M. C., Bergelson, E., Bergmann, C., Cristia, A., Floccia, C., Gervain, J., Hamlin, J. K., \n\nHannon, E. E., Kline, M., & Levelt, C. (2017). A collaborative approach to infant \n\nresearch: Promoting reproducibility, best practices, and theory-building. Infancy, 22(4), \n\nFrank, M. C., Braginsky, M., Yurovsky, D., & Marchman, V. A. (2016). Wordbank: An open \n\nrepository for developmental vocabulary data. Journal of Child Language. \n\n421\xe2\x80\x93435. \n\n \n\n \n\n30 \n\n\x0cFrank, M. C., Braginsky, M., Yurovsky, D., & Marchman, V. A. (2021). Variability and \n\nConsistency in Early Language Learning: The Wordbank Project. MIT Press. \n\nhttp://wordbank-book.stanford.edu \n\nFrank, M. C., & Saxe, R. (2012). Teaching replication. Perspectives on Psychological Science, 7, \n\n595\xe2\x80\x93599. https://doi.org/10.1177/1745691612460686 \n\nGehlbach, H., & Robinson, C. D. (2021). From old school to open science: The implications of \n\nnew research norms for educational psychology and beyond. Educational Psychologist, \n\n56(2), 79\xe2\x80\x9389. \n\nGennetian, L. A., Tamis-LeMonda, C. S., & Frank, M. C. (2020). Advancing transparency and \n\nopenness in child development research: Opportunities. Child Development Perspectives, \n\n14(1), 3\xe2\x80\x938. https://doi.org/10.1111/cdep.12356 \n\nGilmore, R. O., Cole, P. M., Verma, S., van Aken, M. A. G., & Worthman, C. M. (2020). \n\nAdvancing Scientific Integrity, Transparency, and Openness in Child Development \n\nResearch: Challenges and Possible Solutions. Child Development Perspectives, 14(1), 9\xe2\x80\x93\n\n14. https://doi.org/10.1111/cdep.12360 \n\nGilmore, R. O., & Qian, Y. (2022). An open developmental science will be more rigorous, robust, \n\nand impactful. Infant and Child Development, 31(1), e2254. \n\nhttps://doi.org/10.1002/icd.2254 \n\nHardwicke, T. E. (2019). Data availability, reusability, and analytic reproducibility: Evaluating \n\nthe impact of a mandatory open data policy at the journal Cognition | Royal Society Open \n\nScience. https://royalsocietypublishing.org/doi/full/10.1098/rsos.180448 \n\nHardwicke, T. E., Bohn, M., MacDonald, K., Hembacher, E., Nuijten, M. B., Peloquin, B. N., \n\ndeMayo5, B. E., Long, B., Yoon, E. J., & Frank, M. C. (2021). Analytic reproducibility in \n\n \n\n \n\n31 \n\n\x0carticles receiving open data badges at Psychological Science: An observational study. \n\nRoyal Society Open Science, 8. https://doi.org/10.1098/rsos.201494 \n\nHawkins*, R. X. D., Smith*, E. N., Students, P. 254, & Frank, M. C. (2018). Improving the \n\nreplicability of psychological science through pedagogy. Advances in Methods and \n\nPractices in Psychological Science. https://doi.org/10.1177/2515245917740427 \n\nHill, K. G., & Buckley, P. (2021). Blueprints for Healthy Youth Development. \n\nHome Visiting Evidence of Effectiveness Review: Brief - 2021. (n.d.). Retrieved May 19, 2022, \n\nfrom https://www.acf.hhs.gov/opre/report/home-visiting-evidence-effectiveness-review-\n\nbrief-2021 \n\nIoannidis, J. P. A. (2005). Why Most Published Research Findings Are False. PLOS Medicine, \n\n2(8), e124. https://doi.org/10.1371/journal.pmed.0020124 \n\nKaplan, R. M., & Irvin, V. L. (2015). Likelihood of null effects of large NHLBI clinical trials has \n\nincreased over time. PloS One, 10(8), e0132382. \n\nKidd, E., & Garcia, R. (2021). How diverse is child language acquisition? \n\nKidwell, M. C., Lazarevi\xc4\x87, L. B., Baranski, E., Hardwicke, T. E., Piechowski, S., Falkenberg, L.-\n\nS., Kennett, C., Slowik, A., Sonnleitner, C., & Hess-Holden, C. (2016). Badges to \n\nacknowledge open practices: A simple, low-cost, effective method for increasing \n\ntransparency. PLoS Biology, 14(5), e1002456. \n\nKim, J.-S., & Corn, J. E. (2018). Sometimes you\xe2\x80\x99re the scooper, and sometimes you get scooped: \n\nHow to turn both into something good. PLOS Biology, 16(7), e2006843. \n\nhttps://doi.org/10.1371/journal.pbio.2006843 \n\nKlein, O., Hardwicke, T. E., Aust, F., Breuer, J., Danielsson, H., Mohr, A. H., IJzerman, H., \n\nNilsonne, G., Vanpaemel, W., & Frank, M. C. (2018). A practical guide for transparency \n\n \n\n \n\n32 \n\n\x0cin psychological science. Collabra: Psychology, 4, 20. \n\nhttps://doi.org/10.1525/collabra.158 \n\nKluyver, T., Ragan-Kelley, B., P\xc3\xa9rez, F., Granger, B. E., Bussonnier, M., Frederic, J., Kelley, K., \n\nHamrick, J. B., Grout, J., & Corlay, S. (2016). Jupyter Notebooks-a publishing format for \n\nreproducible computational workflows. (Vol. 2016). \n\nKosie, J., & Lew-Williams, C. (2022). Open Science Considerations for Descriptive Research in \n\nDevelopmental Science. \n\nKriegeskorte, N. (2016). The selfish scientist\xe2\x80\x99s guide to preprint posting. The Winnower. \n\nhttps://doi.org/10.15200/winn.145838.88372 \n\nMacWhinney, B. (2000). The CHILDES Project: Tools for Analyzing Talk. Third Edition. \n\nLawrence Erlbaum Associates. \n\nMacWhinney, B. (2019). Understanding spoken language through TalkBank. Behavior Research \n\nMethods, 51(4), 1919\xe2\x80\x931927. https://doi.org/10.3758/s13428-018-1174-9 \n\nManyBabies Consortium. (2020). Quantifying sources of variability in infancy research using the \n\ninfant-directed speech preference. Advances in Methods and Practices in Psychological \n\nScience, 3(1), 24\xe2\x80\x9352. https://doi.org/10.1177/2515245919900809 \n\nMoody, J. W., Keister, L. A., & Ramos, M. C. (2022). Reproducibility in the Social Sciences. \n\nAnnual Review of Sociology, 48. \n\nMoshontz, H., Campbell, L., Ebersole, C. R., IJzerman, H., Urry, H. L., Forscher, P. S., Grahe, J. \n\nE., McCarthy, R. J., Musser, E. D., Antfolk, J., Castille, C. M., Evans, T. R., Fiedler, S., \n\nFlake, J. K., Forero, D. A., Janssen, S. M. J., Keene, J. R., Protzko, J., Aczel, B., \xe2\x80\xa6 \n\nChartier, C. R. (2018). The Psychological Science Accelerator: Advancing Psychology \n\nthrough a Distributed Collaborative Network. Advances in Methods and Practices in \n\n \n\n \n\n33 \n\n\x0cPsychological Science, 1(4), 501\xe2\x80\x93515. https://doi.org/10.1177/2515245918797607 \n\nMunaf\xc3\xb2, M. R., Nosek, B. A., Bishop, D. V., Button, K. S., Chambers, C. D., Du Sert, N. P., \n\nSimonsohn, U., Wagenmakers, E.-J., Ware, J. J., & Ioannidis, J. P. (2017). A manifesto \n\nfor reproducible science. Nature Human Behaviour, 1(1), 0021. \n\nNielsen, M., Haun, D., K\xc3\xa4rtner, J., & Legare, C. H. (2017). The persistent sampling bias in \n\ndevelopmental psychology: A call to action. Journal of Experimental Child Psychology, \n\n162, 31\xe2\x80\x9338. \n\nNosek, B. A., Alter, G., Banks, G. C., Borsboom, D., Bowman, S. D., Breckler, S. J., Buck, S., \n\nChambers, C. D., Chin, G., Christensen, G., Contestabile, M., Dafoe, A., Eich, E., Freese, \n\nJ., Glennerster, R., Goroff, D., Green, D. P., Hesse, B., Humphreys, M., \xe2\x80\xa6 Yarkoni, T. \n\n(2015). Promoting an open research culture. Science, 348(6242), 1422\xe2\x80\x931425. \n\nhttps://doi.org/10.1126/science.aab2374 \n\nNosek, B. A., Ebersole, C. R., DeHaven, A. C., & Mellor, D. T. (2018). The preregistration \n\nrevolution. Proceedings of the National Academy of Sciences, 115(11), 2600\xe2\x80\x932606. \n\nhttps://doi.org/10.1073/pnas.1708274114 \n\nNosek, B. A., & Errington, T. M. (2020). What is replication? PLoS Biology, 18(3), e3000691. \n\nNosek, B. A., Hardwicke, T. E., Moshontz, H., Allard, A., Corker, K. S., Dreber, A., Fidler, F., \n\nHilgard, J., Kline Struhl, M., & Nuijten, M. B. (2022). Replicability, robustness, and \n\nreproducibility in psychological science. Annual Review of Psychology, 73, 719\xe2\x80\x93748. \n\nNosek, B. A., & Lakens, D. (2014). Registered reports. In Social Psychology. Hogrefe Publishing. \n\nNuijten, M. B., Hartgerink, C. H., Van Assen, M. A., Epskamp, S., & Wicherts, J. M. (2016). The \n\nprevalence of statistical reporting errors in psychology (1985\xe2\x80\x932013). Behavior Research \n\nMethods, 48(4), 1205\xe2\x80\x931226. \n\n \n\n \n\n34 \n\n\x0cOpen Science Collaboration. (2015). Estimating the reproducibility of psychological science. \n\nScience, 349(6251), aac4716. \n\nPetersen, I. T., Apfelbaum, K. S., & McMurray, B. (2022). Adapting open science and pre-\n\nregistration to longitudinal research. Infant and Child Development, n/a(n/a), e2315. \n\nhttps://doi.org/10.1002/icd.2315 \n\nPhillips, J., Ong, D. C., Surtees, A. D. R., Xin, Y., Williams, S., Saxe, R., & Frank, M. C. (2015). \n\nA second look at automatic theory of mind: Reconsidering Kov\xc3\xa1cs, T\xc3\xa9gl\xc3\xa1s, and Endress \n\n(2010). Psychological Science, 26(9), 1353\xe2\x80\x931367. \n\nhttps://doi.org/10.1177/0956797614558717 \n\nPoldrack, R. A., Baker, C. I., Durnez, J., Gorgolewski, K. J., Matthews, P. M., Munaf\xc3\xb2, M. R., \n\nNichols, T. E., Poline, J.-B., Vul, E., & Yarkoni, T. (2017). Scanning the horizon: \n\nTowards transparent and reproducible neuroimaging research. Nature Reviews \n\nNeuroscience, 18(2), 115\xe2\x80\x93126. \n\nPopkin, G. (2019). Data sharing and how it can benefit your scientific career. Nature, 569(7756), \n\n445\xe2\x80\x93447. https://doi.org/10.1038/d41586-019-01506-x \n\nPrentice, D., & Paluck, E. L. (2020). Engineering social change using social norms: Lessons from \n\nthe study of collective action. Current Opinion in Psychology, 35, 138\xe2\x80\x93142. \n\nhttps://doi.org/10.1016/j.copsyc.2020.06.012 \n\nRowhani-Farid, A., Aldcroft, A., & Barnett, A. G. (2020). Did awarding badges increase data \n\nsharing in BMJ Open? A randomized controlled trial. Royal Society Open Science, 7(3), \n\n191818. https://doi.org/10.1098/rsos.191818 \n\nSimmons, J. P., Nelson, L. D., & Simonsohn, U. (2011). False-positive psychology: Undisclosed \n\nflexibility in data collection and analysis allows presenting anything as significant. \n\n \n\n \n\n35 \n\n\x0cPsychological Science, 22(11), 1359\xe2\x80\x931366. \n\nSimon, D. A., Gordon, A. S., Steiger, L., & Gilmore, R. O. (2015). Databrary: Enabling sharing \n\nand reuse of research video. Proceedings of the 15th Acm/Ieee-Cs Joint Conference on \n\nSlobin, D. I. (1985). The crosslinguistic study of language acquisition: Theoretical issues (Vol. \n\nDigital Libraries, 279\xe2\x80\x93280. \n\n2). Psychology Press. \n\nSoska, K., Xu, M., Gonzalez, S., Hertzberg, O., Tamis-LeMonda, C., Gilmore, R. O., & Adolph, \n\nK. E. (2021). (Hyper)active Data Curation: A Video Case Study from Behavioral Science. \n\nPsyArXiv. https://doi.org/10.31234/osf.io/89rcb \n\nSteegen, S., Tuerlinckx, F., Gelman, A., & Vanpaemel, W. (2016). Increasing Transparency \n\nThrough a Multiverse Analysis. Perspectives on Psychological Science, 11(5), 702\xe2\x80\x93712. \n\nhttps://doi.org/10.1177/1745691616658637 \n\nStodden, V., Seiler, J., & Ma, Z. (2018). An empirical analysis of journal policy effectiveness for \n\ncomputational reproducibility. Proceedings of the National Academy of Sciences, 115(11), \n\nSunstein, Cass. (2019). Conformity. NYU Press. https://nyupress.org/9781479867837/conformity \n\nTackett, J. L., Brandes, C. M., & Reardon, K. W. (2019). Leveraging the Open Science \n\nFramework in clinical psychological assessment research. Psychological Assessment, \n\nTowse, J. N., Ellis, D. A., & Towse, A. S. (2021). Opening Pandora\xe2\x80\x99s Box: Peeking inside \n\nPsychology\xe2\x80\x99s data sharing practices, and seven recommendations for change. Behavior \n\nResearch Methods, 53(4), 1455\xe2\x80\x931468. https://doi.org/10.3758/s13428-020-01486-1 \n\nWeisman, K., & Markman, E. M. (2017). Theory-based explanation as intervention. Psychonomic \n\n2584\xe2\x80\x932589. \n\n31(12), 1386. \n\n \n\n \n\n36 \n\n\x0cBulletin & Review, 24(5), 1555\xe2\x80\x931562. https://doi.org/10.3758/s13423-016-1207-2 \n\nWicherts, J. M., Borsboom, D., Kats, J., & Molenaar, D. (2006). The poor availability of \n\npsychological research data for reanalysis. American Psychologist, 61(7), 726. \n\nWingen, T., Berkessel, J. B., & Dohle, S. (2022). Caution, Preprint! Brief Explanations Allow \n\nNonscientists to Differentiate Between Preprints and Peer-Reviewed Journal Articles. \n\nAdvances in Methods and Practices in Psychological Science, 5(1), 25152459211070560. \n\nhttps://doi.org/10.1177/25152459211070559 \n\nZwaan, R. A., Etz, A., Lucas, R. E., & Donnellan, M. B. (2018). Making replication mainstream. \n\nBehavioral and Brain Sciences, 41. \n\n \n\n \n\n \n\n \n\n \n\n \n\n37 \n\n\x0cTable 1. The 4Rs: Values for a Broader Open Science. \n\nDescription \nMeasurement of the same effect in a new sample of participants \n\nValue \nReplication \nReproducibility  Calculation of the same numerical measures from the original dataset \nReuse \nReach \n  \n\nUse of shared code, data, or materials for new research   \nBroadening global access to scientific products  \n\n \n\n \n\n \n\n38 \n\n\x0cTable 2. Open Science Practices of Selected Journals in Developmental Research. \n\nDevelopmental Psychology* \n\nX \n\nX \n\n  \n\nAnnual Review of Developmental \nPsychology \n\nChild Development* \nChild Development Perspectives \nCurrent Directions in Psychological \nScience* \n\nDevelopmental Review* \n\nDevelopmental Science \n\nEducational Psychologist* \n\nEducational Psychology Review* \n\nHuman Development \n\nInfancy \n\nInfant Behavior and Development \n\nExceptional Children* \n\nX \n\nX \n\n \n\n \n \ns\ns\ne\nc\nc\na\nn\ne\np\no\nd\ne\ns\na\nb\n-\ne\ne\nF\n\n \n\n \ne\nl\nb\na\nl\ni\na\nv\na\n\n \ns\ne\ng\nd\na\nB\n\n \ne\nl\nb\na\nt\np\ne\nc\nc\na\n\n \ns\nt\nn\ni\nr\np\n-\ne\nr\nP\n\n \n\nX \nX \n\nX \n\nX \n\n \n\nX \n\nX \n\nX \n\n \n\n \n\nX \n\nX \n\n \n\nX \n\nX \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\nX \n\nX \n\nX \n\nX \nX \n\nX \n\nX \n\nX \n\nX \n\nX \n\nX \n\nX \n\nX \n\nX \n\nX \n\nX \n\nX \n\nX \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\nX \n\nX \n\nX \n\nX \n\nX \n\nX \n\n \n\n \ne\nc\nn\ne\ni\nc\ns\n \nn\ne\np\no\n\n \n\ng\nn\ni\nd\nr\na\ng\ne\nr\n \nt\nn\ne\nm\ne\nt\na\nt\nS\n\n \n \n\nn\no\ni\nt\np\ni\nr\nc\ns\ne\nd\n \ne\nl\np\nm\na\ns\n \nl\na\nr\nu\nt\nl\nu\nc\n-\no\ni\nc\no\nS\n\n \n2\ns\nt\nr\no\np\ne\nr\n \nd\ne\nr\ne\nt\ns\ni\ng\ne\nr\n \ns\nt\np\ne\nc\nc\nA\n\nX \n\nX \n\nX \nX \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\nX \n\nX \n\nX \n\nX \n\nX \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \ns\nn\no\ni\nt\na\nc\ni\nl\np\ne\nr\n \n\ng\nn\ni\nh\ns\ni\nl\nb\nu\np\n \ns\ne\ns\nr\no\nd\nn\nE\n\nX \n\nX \n\nX \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\nJ of Abnormal Psychology* \nJ of American Acad of Child and \nAdolescent Psychiatry* \n\nX \n\nJ of Child Psychology and Psychiatry \n\nJ of Cognition and Development  \n\nX \n\nX \n\nX \n\nX \n\nX \n\n \n\nData availability policy \n\nNone \nAuthors must state whether data, code, \nand materials are available, but \ninformation will not be used in review \nNone \n\nNone \nEncouraged to state data availbility, if \nunaccessible or unsuitable to post, you \ncan indicate why during the submission \nprocess \nMust state whether data and study \nmaterials are available and if so where to \naccess them - if not available, must state \nlegal or ethical reasons \nMust include a data availbility statement \nto confirm if data is available and if so \nwhere it can be accessed \nMust provide info about where the data \ncan be found & must deposit in \nrecognized repository \nEncourages authors where possible in \napplicable to deposit data that supports \nthe findings in a public repository \nMake the data for the study available for \nindependent review and provide access \nto procedures and code used in analysis \nStrongly encourage authors to make all \ndatasets available without unnecessary \nrestrictions, required to include \nstatement \nEncourage data sharing through NYU \nDatabrary, must include data availibility \nstatement and link to repository used \nEncourages data sharing when \nappropriate and state availability of data \nin submission \nMust state whether data and study \nmaterials are available and where to \naccess them - if not available, must state \nlegal or ethical reasons  \nDifferent requirements based on the type \nof study  \nEncourages authors to share the data and \nother artifacts supporting the results in \nan appropriate public repository  \nRequired to provide a data availability \nstatement detailing where associated \ndata can be found and how it can be \naccessed, if not available must state why \n\n \n\n39 \n\n\x0cPlease provide information about where \nthe data supporting the results or \nanalyses presented in the paper can be \nfound \n\n \nX \n\n \n \n\n \nX \nX \n\nJ of the Learning Sciences* \n\nX \n\n \n\nX \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\nX \n\nX \nX \n\nNone \nNone \n\nMonographs of the SRCD \nSRCD: Social Policy Report \n   \nNotes: X = Yes as of May 2022. Each of the journals included in this table mandates a conflict of \ndisclosure form.  Each of the journals also requires an indication from authors whether data used \nin the study are available and whether there are any restrictions.  No journal listed here requires \nthat data or analysis programs be made available as a condition of publication. * Journal \nidentified as top 10 in 2020 in Development and Educational Psychology.  Additional Js in the \nfield of developmental science that accept registered reports include British J of Developmental \nPsychology; Developmental Cognitive Neuroscience; Infant and Child Development; J of Child \nPsychology; J of Child Language. \n\n  \n\n \n\n \n\n \n\n \n\n40 \n\n\x0cFigure 1 \n\nNumber of studies registered on the Open Science Framework since 2012 for each of three \n\nkeywords corresponding to epochs of child development.  \n\n \n\n \n\n \n\n41 \n\n\x0cBOX 1: Examples of Pioneering Open Science Precedents \n\nReproducibility and Replicability. An influential edited volume about the impact of \n\npoverty on children\xe2\x80\x99s development (Duncan & Brooks-Gunn, 1997) also modeled an early vision \n\nof reproducibility in developmental science. Authors were invited to contribute to a chapter of the \n\nvolume with the condition that each chapter would begin with reporting on estimates that adhered \n\nto a regression model that followed an agreed-upon set of covariates as independent variables and \n\nan agreed-upon dependent variable. This practice resulted in a volume that could speak to the \n\nreproducibility and robustness of findings across data sets.  \n\nReuse. Developmental science as a field has long recognized the importance of sharing \n\ndatasets and technical tools that enable collaboration and reuse. For example, the Child Language \n\nData Exchange System (CHILDES) has been a pioneer in making transcripts of speech to \n\nchildren freely available to the full research community (MacWhinney, 2000) since long before \n\n\xe2\x80\x9cbig data\xe2\x80\x9d or \xe2\x80\x9copen science\xe2\x80\x9d were buzzwords.  \n\nReach. One important goal of research in language acquisition is to characterize the shared \n\nlearning mechanisms that \xe2\x80\x93 in combination with language specific environmental input \xe2\x80\x93 lead to \n\nthe acquisition of a child\xe2\x80\x99s native language(s). Pursuing this goal requires research with global \n\nreach, as recognized by a set of pioneering studies of cross-linguistic language acquisition that \n\nsought to decrease the English-centric perspective of the psycholinguistics field (Slobin, 1985). \n\n \n\n \n\n \n\n42 \n\n\x0c'",Gennetian et al. - 2022 - Open Science in developmental science.pdf
"b'886300 ADHXXX10.1177/1523422319886300Advances in Developing Human ResourcesTurner and Baker\n\nresearch-article2020\n\nArticle\n\nAdvances in Developing Human\nResources\n2020, Vol. 22(1) 72 \xe2\x80\x9386\n\xc2\xa9 The Author(s) 2020\nArticle reuse guidelines:\nsagepub.com/journals-permissions \nDOI: 10.1177/1523422319886300\nhttps://doi.org/10.1177/1523422319886300\njournals.sagepub.com/home/adh\n\nCollaborative Research: \nTechniques for Conducting \nCollaborative Research  \nFrom the Science of Team \nScience (SciTS)\n\nJohn R. Turner1 and Rose Baker1\n\nAbstract\nThe Problem\nThe  field  of  human  resource  development  (HRD)  is  a  multidisciplinary  field  of \nresearch and practice requiring collaboration. Unfortunately, the literature on how to \nconduct collaborative research is incomplete within HRD and other disciplines. Any \nbreakdown in the communication, exchange of ideas, agreed-upon methodologies, \nor  shared  credit  for  dissemination  has  the  potential  of  preventing  research  from \nmoving  forward.  Promotion  and  tenure  policies  also  hamper  collaborative  efforts \nin that these policies often reward individual initiative as opposed to collaborative \noutcomes. These behavioral patterns provide constraints to the improvement and \nbetterment of efforts to changing of the guard.\nThe Solution\nThis  article  highlights  new  and  improved  methods  for  working  in  collaborative \nenvironments. During an academic\xe2\x80\x99s transition and professional development, these \nmethods will help emerging scholars, new to collaborative research, when facing the \nteam science revolution.\nThe Stakeholders\nScholars and scholar-practitioners engaged in collaborative research. Emerging scholars \nwho are beginning their journey into collaborative research. Graduate students preparing \nfor a career in academia.\n\nKeywords\ncollaboration, multidisciplinary, team science\n\n1University of North Texas, Denton, TX, USA\n\nCorresponding Author:\nJohn R. Turner, University of North Texas, 3940 North Elm Street, G150, Denton, TX 76207, USA. \nEmail: john.turner@unt.edu\n\n\x0cTurner and Baker \n\n73\n\nScientific research today has become more of a collaborative process to the point that \nit is viewed as a \xe2\x80\x9cteam science revolution\xe2\x80\x9d (Bozeman & Youtie, 2017, p. 2). Others \nhave termed this revolution a new collaboration cosmopolitanism (Bozeman & Youtie, \n2017) to represent the fact that research is not only conducted by academics. It also \ninvolves practitioners from industry and stakeholders from the community. Scientific \ncollaboration provides the benefits of promoting synergy across disciplines, holds the \npotential of solving larger and more complex problems, and potentially increases the \nimpact of research outcomes due to its nature of being multidisciplinary (DeHart, \n2017). Here, scientific collaboration is defined as \xe2\x80\x9ca form of interaction among pro-\nducers of knowledge, allowing effective communication and exchange; sharing of \nskills,  competencies  and  resources;  working,  generating  and  reporting  findings \ntogether\xe2\x80\x9d (Ynalvez & Shrum, 2011, p. 205). Unfortunately, if any of these processes \nbreak down during the research effort (e.g., communication, sharing of skills), the \nscientific collaborative process could be hampered, or worse, could produce invalid \noutcomes.\n\nThe field of human resource development (HRD) is a multidisciplinary field of \nresearch  and  practice.  Unfortunately,  literature  on  how  to  conduct  collaborative \nresearch is lacking within HRD. Conducting multidisciplinary research is challeng-\ning. Each discipline adheres to its unique definitions, theories, processes, procedures, \nand  methodologies.  Reconciling  any  conflict  or  disagreement  between  disciplines \nmust be performed by the researchers during the initial stages of the study. Breakdowns \nin communication, exchange of ideas, agreed upon methodologies, and shared credit \nfor  dissemination  also  have  the  potential  of  preventing  the  research  from  moving \nforward.\n\nAcceptance of multidisciplinary research efforts still has its barriers in that promo-\ntion and tenure requirements within traditional academic institutions continue to favor \nindependent  research  over  collaborative  studies  (Zucker,  2012).  Multidisciplinary \nresearch presents a shift in traditional academic practices. Collaboration trumps indi-\nvidual achievement: \xe2\x80\x9cthe scientific community realizes that to truly understand com-\nplex phenomena, we must transcend disciplinary boundaries\xe2\x80\x9d (Fiore, 2008, p. 258). To \nsucceed  in  the  future,  researchers  will  need  to  transition  efforts  from  individual \nachievements for promotion and tenure to the new emerging culture of collaboration. \nThis transition can be difficult to achieve, both for the individual researcher and for \nadministrators. Scholars and scholar-practitioners are beginning to realize the need to \nembrace multidisciplinary research. New requirements from grant funding agencies \nand  the  complexity  involved  in  today\xe2\x80\x99s  research  require  \xe2\x80\x9cboth  broad  and  in\xe2\x80\x93depth \nexpertise\xe2\x80\x9d (Zucker, 2012, p. 780).\n\nPreparing the field of HRD for this new team science revolution is essential in pro-\nviding scholars and scholar-practitioners with the necessary resources to aid them in \nthis collaborative transition, which is innate to changing of the guard. This change will \nbe essential for the discipline moving forward in today\xe2\x80\x99s increasingly complex land-\nscape.  Complexity  requires  multiple  agents  (multidisciplinary)  coming  together  to \nresolve a complex problem that individual agents (interdisciplinary) could not accom-\nplish  on  their  own.  The  advantage  of  these  collaborative  methods,  when  practiced \n\n\x0c74 \n\nAdvances in Developing Human Resources 22(1)\n\ncorrectly, is that their results produce unexpected and astonishing outcomes\xe2\x80\x94known \nas the concept of emergence from complexity science. Emergent outcomes result from \ncollaborative practices by autonomous agents that are self-organizing and free of con-\nstraints\xe2\x80\x94the essence of changing of the guard. The current article focuses on the col-\nlaborative aspects that can lead to, or emerge into, successful outcomes when changing \nof the guard becomes essential.\n\nThe remainder of this article is organized by highlighting the burgeoning trends in \ncollaborative research, primarily multiauthored research by disparate researchers. This \narticle will also differentiate between the divergent types of cross-disciplinary collabo-\nration research models (unidisciplinary, multidisciplinary, interdisciplinary, transdisci-\nplinary). Teamwork and taskwork will be placed in context as collaborative research \ndemands effective teamwork skills and the coordination and management of taskwork. \nTechniques  on  how  to  manage  this  teamwork  along  with  evaluation  techniques  for \ncollaborative  research  proficiency  and  accomplishment  will  be  covered. Also  pro-\nvided will be recommendations for necessary changes to existing policies, funding, \nauthor contributions, and promotion and tenure guidelines. These recommendations \nshould help support scholars and scholar-practitioners in HRD as they begin to navi-\ngate this new collaboration cosmopolitanism.\n\nCollaborative Research\nAs researchers and practitioners are called upon to address increasing levels of com-\nplexity  in  their  daily  practices,  collaborations  will  be  necessary  rather  than  being \noptional. Addressing complex problems requires identifying relevant causal modali-\nties from multiple levels of analysis, requiring consideration of multiple environmen-\ntal  dimensions.  For  example,  social  ecology  considers  four  environmental  scales: \nnatural, built, sociocultural, and virtual environments (Stokols, 2018). Natural envi-\nronments  involve  nature,  animals,  and  plants;  built  involves  physical  environments \ndesigned and constructed by humans; sociocultural environments involve organiza-\ntional  and  institutional  entities;  and  virtual  environments  involve  computing  and \nmobile technologies, the internet, and virtual reality (Stokols, 2018). In considering \nmultiple environments to identify relevant causal mechanisms, collaborative research \nteams are essential. The expertise of these multidisciplinary teams must also be diverse \nenough to provide the requisite knowledge, skills, and experiences required to narrow \ndown the causal mechanisms from each environmental scale to only a few that may be \nrelevant to the phenomenon being researched. As problems become more complex, \ncollaborative endeavors require multiple research partners (e.g., academics, lay people \nor community members, politicians, practitioners).\n\nThe Multi-ness of Cross-Disciplinary Research\nIn most disciplines, multiple-authorship research articles have been increasing (Huang, \n2015; Katz & Martin, 1997) and the number of single-authored research articles have \nbeen decreasing (Huang, 2015). Earlier appraisals of authorship in the basic sciences \n\n\x0cTurner and Baker \n\n75\n\n(chemistry,  mathematics,  physics)  showed  a  decline  in  single-author  papers  and  an \nincrease  in  two-author  papers.  Following  this  trend  over  time,  the  number  of  two-\nauthor  papers  began  to  decline  and  the  number  of  three-author  papers  began  to \nincrease. Currently, this trend has continued in each of the basic sciences in which \nfour- and five-author papers are becoming the norm, above and beyond single-, two-, \nand three-author papers (Huang, 2015). Research of authorship has concluded that the \nincrease in the number of co-authors is \xe2\x80\x9congoing and follows an exponential growth\xe2\x80\x9d \n(Huang, 2015, p. 2146) and can be attributed to \xe2\x80\x9cthe explosion of knowledge in vari-\nous fields\xe2\x80\x9d (Stokols, 2018, p. 330). These basic science findings are similar to what \nhas been found in other disciplines. For example, in coronary heart disease (CHD) \nresearch, the number of multiauthored papers have increased \xe2\x80\x9cfrom 4.2 in 1981 to 6.4 \nin 2010\xe2\x80\x9d (Yu et al., 2013, p. 632), placing the average number of multiauthors per \npaper for this field around six. Also, in CHD research, it has been common to find that \nnot only has the number of multiauthored research papers increased, but it also has \nbeen documented that the number of multi-institutional collaboration efforts have also \nincreased, from 23% in 1981 to 56% in 2010 (Yu et al., 2013). In addition, the number \nof multinational (global) collaboration efforts have also increased, from 2% in 1981 to \n19% in 2010 (Yu et al., 2013).\n\nSimilar trends have been realized within the HRD academic journals (see Figure 1). \nThe number of authors per published research article (not including editorials, reviews, \nreaction  articles,  etc.)  were  coded  for  each  of  the  four  HRD  journals  as  shown  in \nFigure  1.  The  number  of  single-authored  research  papers  for  Human  Resource \nDevelopment Quarterly (HRDQ) decreased from 87 articles between 1990 and 1999 \nto 57 between 2000 and 2009, and down again to 25 between 2010 and 2018. This \njournal  increased  the  number  of  articles  with  more  than  three  authors  substantially \nafter 2000 and first published articles with six and eight authors within the time frame \nof  2010  to  2018.  Human  Resource  Development  Review  (HRDR)  showed  similar \ntrends  with  a  total  of  73  single-authored  articles  between  2002  and  2009  with  a \ndecrease to 51 single-authored articles from 2010 to 2018. This journal, HRDR, had \npublished  articles  with  one  to  five  authors  during  the  period  of  2002  to  2009  and \nexpanded to include articles with six and seven authors from 2010 to 2018. Human \nResource Development International (HRDI) decreased single-authored articles from \n122 between 1998 and 2009 to 68 between 2010 and 2018. Authorship in the time \nframe between 1998 and 2009 included articles with one through five authors and one \narticle with eight authors. Between 2010 and 2018, more articles were published with \nmultiple authors with a range between one and nine authors with one article having 11 \nauthors. As a special issue journal and one that promotes collaboration, Advances in \nDeveloping  Human  Resources  (ADHR)  showed  similar  trends.  Single  authorship \nreduced from 192 published articles between 1999 and 2008 to 119 published articles \nbetween 2009 and 2018. Both time frames represented multiple-authored articles with \nas  many  as  six  authors,  and  one  article  included  nine  authors  in  the  earlier  period \nbetween 1999 and 2008. The trends for each of the four journals highlight the point \nthat single-authored research is becoming less common and that the trend of multiple-\nauthored research has impacted the field of HRD and is expected to continue to grow.\n\n\x0c76 \n\nAdvances in Developing Human Resources 22(1)\n\nFigure 1.  Authorship trends in HRD journals.\nNote. HRD = human resource development; HRDQ = Human Resource Development Quarterly;  \nHRDI = Human Resource Development International; HRDR = Human Resource Development Review; \nADHR = Advances in Developing Human Resources.\n\nThis increasing trend in multiauthor, multi-institutional, and multinational research \ncollaborations has been found in other disciplines (Rahman et al., 2017; Yu et al., \n2013). This increasing trend is beginning to raise some concerns as well. For exam-\nple,  as  the  number  of  multiauthor  research  studies  increase,  researchers  should  be \ntrained on how to collaborate more effectively, especially when working across other \n\n\x0cTurner and Baker \n\n77\n\ndisciplines, across different institutions, and across various countries and language \nbarriers. This increasing trend falls under the umbrella of cross-disciplinary research \nin which a shift in traditional academic practices involving collaboration trumps indi-\nvidual achievement, creating a culture of collaboration, a collaboration cosmoplitan-\nism, or a team science revolution (Bozeman & Youtie, 2017).\n\nCross-Disciplinary Collaboration\nWhen dealing with cross-disciplinary research, multiple agents from differing disci-\nplines  are  involved  in  exchanging  multiple  perspectives,  philosophies,  and  theories \naimed toward the phenomenon and research problem. Some of the benefits of conduct-\ning cross-disciplinary collaboration research follow:\n\n\xe2\x80\xa2\xe2\x80\xa2 Complex modern problems such as climate change and resource security are \n\nnot amenable to single-discipline investigation.\n\n\xe2\x80\xa2\xe2\x80\xa2 Discoveries are said to be more likely on the boundaries between fields.\n\xe2\x80\xa2\xe2\x80\xa2 These encounters with others benefit single disciplines, extending their hori-\n\nzons (Rylance, 2015, p. 313).\n\nHowever,  even  with  the  numerous  benefits,  cross-disciplinary  research  comes \nwith many challenges as well. Addressing these challenges and issues when conduct-\ning cross-disciplinary research is the focus of the field of the Science of Team Science \n(SciTS; Falk-Krzesinski et al., 2010). Building upon knowledge gained from the field \nof  Team  Science  (TSci),  SciTS  has  concentrated  efforts  primarily  on  conducting \ncross-disciplinary research. The field of SciTS is \xe2\x80\x9cconcerned with understanding and \nmanaging  circumstances  that  facilitate  or  hinder  the  effectiveness  of  collaborative \ncross-disciplinary  science,  and  the  evaluation  of  collaborative  science  outcomes\xe2\x80\x9d \n(Falk-Krzesinski  et  al.,  2010,  p.  263).  TSci  views  cross-disciplinary  research  as  a \n\xe2\x80\x9cprocess of teamwork to be mastered\xe2\x80\x9d (Fiore, 2008, p. 256).\n\nDepending on the level of complexity in a research problem, there are generally \nfour different types of cross-disciplinary collaboration research models (i.e., uni-\ndisciplinary, multidisciplinary, interdisciplinary, transdisciplinary). Unidisciplinary \nincludes  research  that  is  conducted  within  a  single  discipline.  Multidisciplinary \nincludes  research  that  involves  contributions  from  multiple  disciplines  (DeHart, \n2017), defined as the \xe2\x80\x9ccoordinated efforts of some set of disciplines designed \nto  achieve  some  common  goal  or  goals\xe2\x80\x9d  (Fiore,  2008,  p.  254).  Interdisciplinary \ninvolves integrative contributions from multiple disciplines; this integration is nec-\nessary as no single discipline can address the issue alone (DeHart, 2017). The out-\ncome of interdisciplinary research often involves new ways of understanding, new \nlexicon to describe an emerging phenomena, and new techniques in developing new \ntheories (Fiore, 2008).\n\nTransdisciplinary research, in contrast, involves multiple disciplines bringing novel \ncontributions to a study (DeHart, 2017). One example of transdisciplinary is when a \nnew method needs to be developed to address a problem because existing methods are \n\n\x0c78 \n\nAdvances in Developing Human Resources 22(1)\n\nnot adequate for the stated problem. Transdisciplinary also involves, at times, stake-\nholders and community members as part of the research effort (DeHart, 2017).\n\nThe following sections identify what teamwork and taskwork entail along with an \nintroduction of the critical components that a researcher should concentrate on when \nworking in a team setting. Then, team contracts are discussed along with collaborative \nresearch agreements that need to be decided in advance of beginning research. Last, an \nintroduction of contextual factors for conducting cross-disciplinary collaborations is \npresented.\n\nTeamwork and Taskwork\nFor teams, processes are divided into two types, teamwork and taskwork. Teamwork \nidentifies the interactions required among team members to achieve their tasks, whereas \ntaskwork involves the activities that must be completed to complete the team\xe2\x80\x99s task or \nobjective. Teamwork relates to team member interactions, the sharing of information \nrequired at any point in time for team members to complete their tasks, the knowledge \nof who has what skills so that they can easily be identified when required, and basic com-\nmunication among team members throughout the process. Taskwork refers to the actual \nactivities (technical, physical, procedural) involved in completing a task or subtask.\n\nThe field of team science has identified nine core processes that make up team-\nwork. These nine core processes have been categorized as being either an emerging \nstate (coaching, cognition, conflict, cooperation, and coordination) or an influencing \ncondition (context, composition, and culture; Dihn & Salas, 2017; Turner et al., 2018). \nEmerging states relate to internal team dynamics that can be managed, whereas influ-\nencing conditions are external dynamics in which the team has little control.\n\nIdentifying the conditions that can be managed compared with those that cannot be \nmanaged would support the efforts of scholars and scholar-practitioners in any col-\nlaborative endeavor. For example, it is important for new scholars to identify what the \ncontext is for their new academic home. Comparing a department\xe2\x80\x99s research interest \nwith one\xe2\x80\x99s own research agenda would aid new scholars to identify better with their \ncolleagues and the department. In the short-term, the new scholar would not be able to \nmake changes if their research agenda was not in line with their new academic home. \nThis new scholar would most likely have to alter an existing research agenda to meet \nthe department\xe2\x80\x99s needs. This is an example of having little to no control over the influ-\nencing  conditions.  Scholar-practitioners  would  benefit  from  identifying  with  the \nemerging states that they can contribute to in a positive manner. By better contributing \nto some of these emerging states will place a scholar-practitioner in the role of a team \nplayer and potentially as an appointed team leader. This is just one example of how \nscholar-practitioners can influence teamwork by identifying with the emerging states.\n\nTechniques for Collaborating\nTeamwork  activities  requires  each  team  member  to  work  both  independently  and \ninterdependently. Team members are required to work interdependently to \xe2\x80\x9cthe extent \n\n\x0cTurner and Baker \n\n79\n\nto  which  a  job  is  contingent  on  others\xe2\x80\x99  work\xe2\x80\x9d  (Cordery  &  Tian,  2017,  p.  111). \nResulting  in  team  members  being  dependent  on  other  team  members  completing \ntheir tasks. Task interdependence requires decisions at the team level involving \xe2\x80\x9cthe \nspecialization  and  differentiation  of  activities  and  tasks  within  teams,  where  the \nseparation  of  activity  elements  generates  explicit  coordination  requirements,  or \nthrough the application of formalized routines that specify particular task and work \nflows\xe2\x80\x9d (p. 111). Essentially, successful teaming requires monitoring and managing \nthese emergent states (coaching, cognition, conflict, cooperation, and coordination) \nby all team members.\n\nPlanning  a  team\xe2\x80\x99s  activities/goals/tasks  takes  work.  One  common  mistake  that  is \noften realized when beginning cross-disciplinary collaborations is in underestimating \nthe level of commitment and personal relationships required in such an effort (Ledford, \n2015). Each of the emerging states of teamwork must be managed throughout the pro-\ncess. One tool for managing these emerging states can be found in Figure 2. Figure 2 \nprovides a brief example of a team agreement contract that each team member contrib-\nutes to. A team or collaborative agreement should cover a team\xe2\x80\x99s goals and schedule, \ninformation  pertaining  to  who  will  perform  which  tasks  and  when,  credit  for  task \naccomplishment acknowledged by peers (other team members), contingencies around \ncommunicating, and any conflicts of interests (Bennett & Gadlin, 2012).\n\nThe team agreement contract in Figure 2 provides information on who is on the \nteam along with the team\xe2\x80\x99s name. When completing this form, team members will \nhave an opportunity to get to know the other team members as well as their knowl-\nedge, skills, experiences, and area of expertise (discipline). The action of includ-\ning a name for the team helps to begin forming a sense of team identify among the \nteam members. These initial activities will help in developing transactive memory \nsystems for the team members, identifying who knows what and who has which \nskills.  This  exercise  also  begins  to  develop  the  team\xe2\x80\x99s  cognition,  the  organized \nunderstanding  of  collective  knowledge  among  team  members  (Mohammed  & \nDumville, 2001).\n\nThe second section requires group members to identify what the team\xe2\x80\x99s task goals \nand process goals are. During this stage, if this was being used for a multidisciplinary \nresearch effort, it would be recommended to include a row on problem identification. \nIdentifying the problem is a critical step prior to assigning tasks and process goals. \nThis step initiates the communication and coordination steps of teamwork by requiring \neach team member to identify and agree on the listed goals. The following section of \nthe form identifies who will do what along with who is appointed as the team leader. \nNot all teams operate with a team leader, some utilize shared leadership models or \nincorporate a blend of coaching and leadership. Either is fine; it is more a matter of \nwhat works best for the team. Also, this section identifies who is to complete what \nassignment or task.\n\nNext in the form is a section on the team\xe2\x80\x99s rules of conduct. The team decides on \nhow they wish to conduct business. The section on evaluation includes a debriefing \nsession after a milestone or after a set of subtasks have been completed. This debrief-\ning is part of a continuous learning cycle; team members identify what worked and \n\n\x0c80 \n\nAdvances in Developing Human Resources 22(1)\n\nFigure 2.  Team agreement contract.\n\n\x0cTurner and Baker \n\n81\n\nwhat  did  not  work  as  well  as  planned,  then  discuss  options  to  make  the  teamwork \nprocesses work better during the next cycle. The last section involves each team mem-\nber agreeing to the information on the form and willing to adopt the changes recom-\nmended by the team for the next cycle.\n\nThis process of having the team agree to the team\xe2\x80\x99s goals, coordination, communi-\ncation, and leadership begins the process of managing the emerging states of team-\nwork. Scholar-practitioners can utilize this form when working in team settings with \ntheir customer, whereas scholars can easily modify this form for use in any type of \ncross-disciplinary research activity. Furthermore, this form has been used successfully \nin a number of team tasks in the classroom, which can benefit both scholars and stu-\ndents. Utilization of this form addresses each of the critical techniques of strengthen-\ning teamwork to (Bennett & Gadlin, 2012)\n\n\xe2\x80\xa2\xe2\x80\xa2 Foster an environment that is collegial and nonthreatening.\n\xe2\x80\xa2\xe2\x80\xa2 Openly recognize strengths of all members of the team and discuss how these \n\ndifferent strengths contribute to advancing the project.\n\n\xe2\x80\xa2\xe2\x80\xa2 Take a few minutes at regularly scheduled group meetings to do a check-in. Ask \n\nhow everyone is doing.\n\n\xe2\x80\xa2\xe2\x80\xa2 Encourage open and honest discussion by establishing trust.\n\xe2\x80\xa2\xe2\x80\xa2\n\nJointly  develop  a  process  for  bringing  issues  and  disagreements  forward  for \nearly resolution.\n\n\xe2\x80\xa2\xe2\x80\xa2 Assure that when decisions are being made, which require everyone\xe2\x80\x99s input, \nthat each person has an opportunity and understands the process for providing \ncomment.\n\n\xe2\x80\xa2\xe2\x80\xa2 Schedule periodic assessments and feedback, including opportunities for col-\nlaborators  to  discuss  what  is  going  well,  what  is  not,  and  what  needs  to  be \nimproved.\n\nImplications for HRD Practitioners\nPractitioners will need to be even more aware of how to gain credit when contributing \nto  a  collaborative  research  project,  how  to  gain  funding  for  collaborative  research \nprojects. In addition, scholar-practitioners will also need to know how to show impact \nfrom their collaborative efforts to better advance one\xe2\x80\x99s self for promotion and tenure. \nThese items are discussed in the following sections.\n\nAuthor\xe2\x80\x99s Contribution\nAdditional questions have been raised as to how to account for each author\xe2\x80\x99s contribu-\ntion  to  a  multiauthor  research  paper.  This  collaborative  contribution  record  aids  in \ndisplaying an individual author\xe2\x80\x99s impact to their field, typically by citation counts, but \nit also contributes to the author\xe2\x80\x99s evaluation for promotion and tenure. Contribution \nshould be decided upon the following criteria set by the Committee on Publication \nEthics (COPE):\n\n\x0c82 \n\nAdvances in Developing Human Resources 22(1)\n\n1.  Substantial contributions to conception and design, or acquisition of data, or \n\nanalysis and interpretation of data;\n\n2.  Drafting the article or revising it critically for important intellectual content; \n\nand\n\n3.  Final approval of the version to be published (Albert & Wager, 2003).\n\nAuthorship contribution should be based on each author meeting all three conditions \nthroughout  the  research  and  writing  processes.  Discussions  relating  to  the  order  of \nauthorship should begin during the start of any research project rather than later as \nwell  as  publication  and  conference  intentions  of  the  final  output  (Albert  & Wager, \n2003).\n\nContribution can be evaluated in two general methods: the full counting approach \nand the fractional counting approach (Rahman et al., 2017). Full counting provides \ncredit to each author for an article\xe2\x80\x99s citation, whereas fractional counting partitions \nthe citation into a part per each author such that each author would receive one (cita-\ntion) divided by the number of authors (n; 1/n). Different variations of this fractional \napproach can be found in the literature (see Rahman et al., 2017). The point remains, \nhowever, that attribution for an author\xe2\x80\x99s contribution in multiauthored papers is cur-\nrently  being  considered  within  academia.  These  discussions  have  the  potential  of \nnegatively impacting one\xe2\x80\x99s academic career if, for example, one\xe2\x80\x99s contribution was \nunfairly measured using a fractional measure that assigned the researcher little to no \ncredit for participation in the multiauthored contribution. This could deeply impact \none\xe2\x80\x99s chances of being promoted to a tenured faculty position, further highlighting \nthe  need  for  scholars  and  students  (emerging  scholars)  to  be  able  to  counter  such \npractices.\n\nFunding\nOther problems that accompany this trend of cross-disciplinary research is that national \nand international funding agencies are beginning to require that research be conducted \nby cross-disciplinary teams, typically of the variety of multidisciplinary or transdisci-\nplinary  research  teams  due  to  the  complexity  involved  in  today\xe2\x80\x99s  environment  and \nproblems (Zucker, 2012).\n\nFunding  agencies  have  been  encouraged  to  support  cross-disciplinary  efforts. \nOne example of this is in the Global Research Council\xe2\x80\x99s (GRC) report on interdisci-\nplinarity. Regarding recommendations to funding agencies, the GRC provided the \nfollowing:\n\n\xe2\x80\xa2\xe2\x80\xa2 Our research and much of the literature suggest that top-down thematic funding \nprograms  are  one  of  the  most  common  approaches  adopted  by  funders  to \nencourage interdisciplinarity.\n\n\xe2\x80\xa2\xe2\x80\xa2 There  is  also  a  consensus  that  researcher-led  \xe2\x80\x9cbottom-up\xe2\x80\x9d  approaches  are \nrequired  and  funding  agencies  should  support  such  approaches  despite  the \npotential risks associated with the most innovative ideas.\n\n\x0cTurner and Baker \n\n83\n\n\xe2\x80\xa2\xe2\x80\xa2 At the same time, interdisciplinary research should be viewed as a means to an \nend and not an end in itself. Several funding agencies emphasized that practices \nand  policies  toward  interdisciplinarity  should  be  driven  by  the  required  out-\ncomes and scientific demand (Gleed & Marchant, 2016).\n\nPromotion and Tenure Practices\nTypically,  academic  institutions\xe2\x80\x99  promotion  and  tenure  policies  still  favor  individual \nresearch (Rylance, 2015). Also, current bibliometrics favor individual efforts as opposed \nto providing collaborative measures. This results, in many cases for early career aca-\ndemics, in advisement against cross-collaborative research in favor of concentrating \none\xe2\x80\x99s time and effort on research that is individual, unidisciplinary, or both (Rylance, \n2015). Future changes in promotion and tenure practices will need to be considered, \nespecially with current and future changes to the requirements from grant-funding insti-\ntutions. Scholars and scholar-practitioners will be better served to collaborate, joining \nacademia with practice, to gain future funding for research. Receipt of such funding \ncould potentially be the catalyst to change the requirements for promotion and tenure, \nat least at one\xe2\x80\x99s current institution.\n\nEvaluation Techniques\nThere are various evaluation measures for collaboration and transdisciplinary research \nprojects.  One  such  tool  is  the  research  orientation  scale  provided  by  the  National \nCancer Institute (NCI; Fload Expert, 2010). This tool is available free and is included \nas part of the Team Science Toolkit sponsored by NCI (https://www.teamsciencetool-\nkit.cancer.gov/Public/Home.aspx). Another evaluation tool is the transdisciplinary ori-\nentation scale provided by Mirsa et al. (2015). The transdisciplinary orientation scale \nprovides \xe2\x80\x9ca reliable and preliminarily validated tool to measure scientists\xe2\x80\x99 and schol-\nars\xe2\x80\x99 personal disposition toward transdisciplinary research\xe2\x80\x9d (Mirsa et al., 2015, p. 6).\n\nThese  evaluation  tools  could  be  tested  and  validated  for  collaborative  research \nwithin the HRD community. Future research could entail utilizing either of these eval-\nuation tools to identify the level of collaboration among collectives along with extend-\ning HRD\xe2\x80\x99s knowledge base by introducing new evaluation tools that could be used by \nscholars and scholar-practitioners within the field of HRD.\n\nConclusion\nResearch is a collaborative process. Advances in science rely partially on social inter-\naction among other scientists (Katz & Martin, 1997). These interactions have, in some \ncases, formed into larger, organized, collaborative networks such as the Academy of \nHuman Resource Development (AHRD) and the SciTS. Funding agencies have also \nbegun to require applicants to show collaborative efforts when documenting research \ncompared with single-discipline research. Part of this drive for more cross-disciplinary \nresearch is because \xe2\x80\x9cit has a key role to play in addressing the grand challenges that \n\n\x0c84 \n\nAdvances in Developing Human Resources 22(1)\n\nsociety faces\xe2\x80\x9d (Gleed & Marchant, 2016, p. 5). Multiauthorship research is more com-\nmon in many disciplines and has been shown to already be acceptable in HRD.\n\nThis shift toward collaborative research identifies with the theme of this special \nissue in that it identifies the shifting roles that scholars and scholar-practitioners will \nface shortly regarding cross-disciplinary research. The potential changes in policies \nrequired  from  the  new  demands  of  cross-disciplinary  research  to  gain  funding  will \nindeed impact the future of current and emerging scholars.\n\nManaging and adjusting the collaborative processes, current practices, and policies \nwill take time. These items are critical and must be realized for scholars and scholar-\npractitioners  to  advance  into  the  future  of  collaborative  research. Achieving  emer-\ngence,  through  collaborative  research  practices,  is  essential  for  those  who  practice \nHRD as it is a multidisciplinary field of study. This emergence addresses the theme of \nthis special issue, Changing of the Guard. Scholars and scholar-practitioners need to \nbe responsive and adaptable to this transformation. They must be capable of participat-\ning in managing and evaluating new collaborative efforts. These efforts will benefit \nHRD scholars and scholar-practitioners and could begin a new field of study within \nHRD, TSci, and Cross-Disciplinary Research.\n\nDeclaration of Conflicting Interests\nThe author(s) declared no potential conflicts of interest with respect to the research, authorship, \nand/or publication of this article.\n\nFunding\nThe author(s) received no financial support for the research, authorship, and/or publication of \nthis article.\n\nReferences\nAlbert, T., & Wager, E. (2003). How to handle authorship disputes: A guide for new research-\n\ners: The COPE report. https://doi.org/10.24318/cope.2018.1.1\n\nBennett,  M.  L.,  &  Gadlin,  H.  (2012).  Collaboration  and  team  science:  From  theory  to  prac-\ntice.  Journal  of  Investigative  Medicine,  60(5),  768\xe2\x80\x93775.  https://doi.org/10.2310/\nJIM.0b013e318250871d\n\nBozeman, B., & Youtie, J. (2017). The strength in numbers: The new science of team science. \n\nPrinceton University Press.\n\nCordery, J. L., & Tian, A. W. (2017). Team design. In E. Salas, R. Rico & J. Passmore (Eds.), \nThe Wiley Blackwell handbook of the psychology of team working and collaborative pro-\ncesses (pp. 103\xe2\x80\x93128). John Wiley.\n\nDeHart, D. (2017). Team science: A qualitative study of benefits, challenges, and lessons learned. \n\nThe Social Science Journal, 54, 458\xe2\x80\x93467. https://doi.org/10.1016/j.soscij.2017.07.009\n\nDihn,  J.  V.,  &  Salas,  E.  (2017).  Factors  that  influence  teamwork.  In  E.  Salas,  R.  Rico  &  J. \nPassmore (Eds.), The Wiley Blackwell handbook of the psychology of team working and \ncollaborative processes (pp. 15\xe2\x80\x9341). John Wiley.\n\nFalk-Krzesinski,  H.  J.,  Borner,  K.,  Contractor,  N.  S.,  Fiore,  S.  M.,  Hall,  K.  L.,  Keyton,  J., \n. . . Uzzi, B. (2010). Advancing the science of team science. Clinical and Translational \nScience, 3, 263\xe2\x80\x93266. https://doi.org/10.1111/j.1752-8062.2010.00223.x\n\n\x0cTurner and Baker \n\n85\n\nFiore, S. M. (2008). Interdisciplinarity as teamwork: How the science of teams can inform team sci-\nence. Small Group Research, 39(3), 251\xe2\x80\x93277. https://doi.org/10.1177/1046496408317797\nFload Expert. (2010). Research Orientation Scale (ROS). https://www.teamsciencetoolkit.can-\n\ncer.gov/public/TSResourceMeasure.aspx?tid=2&rid=35\n\nGleed,  A.,  &  Marchant,  D.  (2016,  May).  Interdisciplinarity:  Survey  report  for  the  Global \nResearch Council 2016 annual meeting. https://www.globalresearchcouncil.org/fileadmin/\ndocuments/GRC_Publications/Interdisciplinarity_Report_for_GRC_DJS_Research.pdf\n\nHuang, D.-W. (2015). Temporal evolution of multi-author papers in basic sciences from 1960 to \n\n2010. Scientrometrics, 105, 2137\xe2\x80\x932147. https://doi.org/10.1007/s11192-015-1760-x\n\nKatz, S. J., & Martin, B. R. (1997). What is research collaboration? Research Policy, 26, 1\xe2\x80\x9318. \n\nhttps://doi.org/10.1016/S0048-7333(96)00917-1\n\nLedford, H. (2015). How to solve the world\xe2\x80\x99s biggest problems: Interdisciplinarity has become \nall  the  rage  as  scientists  tackle  climate  change  and  other  intractable  issues.  But  there  is \nstill  strong  resistance  to  crossing  borders.  Nature,  525(7569),  308\xe2\x80\x93311.  https://doi.org \n/10.1038/525308a\n\nMirsa, S., Stokols, D., & Cheng, L. (2015). The transdisciplinary orientation scale: Factor struc-\nture and relation to the integrative quality and scope of scientific publications. Journal of \nTranslational Medicine & Epidemiology, 3(2), 1042.\n\nMohammed, S., & Dumville, B. C. (2001). Team mental models in a team knowledge frame-\nwork:  Expanding  theory  and  measurement  across  disciplinary  boundaries.  Journal  of \nOrganiztional Behavior, 22, 89\xe2\x80\x93106. https://doi.org/10.1002/job.86\n\nRahman, M. T., Regenstein, J. M., Kassim, N. L. A., & Haque, N. (2017). The need to quantify \nauthors\xe2\x80\x99 relative intellectual contributions in a multi-author paper. Journal of Informatics, \n11, 275\xe2\x80\x93281. https://doi.org/10.1016/j.joi.2017.01.002\n\nRylance,  R.  (2015).  Global  funders  to  focus  on  interdisciplinarity  [Commentary].  Nature, \n\n525(7569), 313\xe2\x80\x93315. https://doi.org/10.1038/525313a\n\nStokols, D. (2018). Social ecology in the digital age: Solving complex problems in a globalized \n\nworld. Elsevier.\n\nTurner, J. R., Baker, R., & Morris, M. (2018). Complex adaptive systems: Adapting and man-\naging teams and team conflict. In A. V. Boas (Ed.), Organizational conflict (pp. 65\xe2\x80\x9394). \nINTECH Open Science. https://doi.org/10.5772/intechopen.72344\n\nYnalvez, M. A., & Shrum, W. M. (2011). Professional networks, scientific collaboration, and \npublication  productivity  in  resource-constrained  research  institutions  in  a  developing \n country. Research Policy, 40, 204\xe2\x80\x93216. https://doi.org/10.1016/j.respol.2010.10.004\n\nYu,  Q.,  Shao,  H.,  He,  P.,  &  Duan,  Z.  (2013).  World  scientific  collaboration  in  coronary \nheart  disease  research.  International  Journal  of  Cardiology,  167,  631\xe2\x80\x93639.  https://doi.\norg/10.1016/j.ijcard.2012.09.134\n\nZucker, D. (2012). Developing your career in an age of team science. Journal of Investigative \n\nMedicine, 60, 779\xe2\x80\x93784. https://doi.org/10.231/JIM.0b013e3182508317\n\nAuthor Biographies\nJohn  R.  Turner,  PhD,  is  an  assistant  professor  at  the  University  of  North  Texas  for  the \nDepartment of Learning Technologies in the College of Information. He currently serves as the \neditor-in-chief for the Performance Improvement Quarterly (PIQ) journal. His research inter-\nests  are  in  team  science,  team  cognition,  leadership,  performance  improvement,  knowledge \nmanagement, theory building, complexity theory, multilevel model development, and analysis \ntechniques. He is the co-creator of The Toyota Flow System. He has published a number of book \n\n\x0c86 \n\nAdvances in Developing Human Resources 22(1)\n\nchapters and research articles in Advances in Developing Human Resources; Human Resource \nDevelopment Review; European Journal of Training & Development; International Journal of \nTechnology,  Knowledge,  &  Society;  Journal  of  Information  and  Knowledge  Management; \nJournal  of  Manufacturing  Technology  Management;  Journal  of  Knowledge  Management; \nPerformance Improvement; Performance Improvement Quarterly; and Systems.\n\nRose Baker, PhD, is an assistant professor in the Learning Technologies Department in the \nCollege of Information at the University of North Texas. She researches open learning, manage-\nment techniques and statistical applications for operations and performance improvement, the-\nory development, survey and evaluation design, and impact and prevention of substance use. \nShe holds a PhD in instructional systems from Penn State and is a certified PMP\xc2\xae by PMI.\n\n\x0c'",1523422319886300.pdf
"b'Scientific Teams and Scientific Laboratories \nAuthor(s): Alvin M. Weinberg \nSource: Daedalus, Fall, 1970, Vol. 99, No. 4, The Making of Modern Science: Biographical \nStudies (Fall, 1970), pp. 1056-1075\nPublished by: The MIT Press on behalf of American Academy of Arts & Sciences \n\n \n\nStable URL: https://www.jstor.org/stable/20023981\n\nJSTOR is a not-for-profit service that helps scholars, researchers, and students discover, use, and build upon a wide \nrange of content in a trusted digital archive. We use information technology and tools to increase productivity and \nfacilitate new forms of scholarship. For more information about JSTOR, please contact support@jstor.org. \n \nYour use of the JSTOR archive indicates your acceptance of the Terms & Conditions of Use, available at \nhttps://about.jstor.org/terms\n\nAmerican Academy of Arts & Sciences and The MIT Press are collaborating with JSTOR to \ndigitize, preserve and extend access to Daedalus\n\nThis content downloaded from \n(cid:0)198.51.92.3 on Mon, 06 Feb 2023 21:21:35 UTC(cid:0)\n(cid:0) \n\nAll use subject to https://about.jstor.org/terms\n\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n\x0c ALVIN M. WEINBERG\n\n Scientific Teams and Scientific Laboratories1\n\n Scientific truths discovered in one age are essential for scientific prog\n ress in another: the laws of thermodynamics, discovered in the nine\n teenth century, will remain relevant and necessary for the scientist of\n the twenty-second century. Similarly scientific truth discovered in one\n place is required for scientific progress elsewhere: Lord Rutherford\'s ex\n periments at Manchester on the scattering of alpha particles led even\n tually to the prolific investigation of nuclear phenomena throughout the\n world. To paraphrase Alfred Korzybski, man the scientist is both a time\n binder and a space-binder.\n\n In this sense science has always been a cumulative, team activity,\n more than, say, the arts or literature.2 To be sure, great individual\n geniuses, like Newton or Maxwell or Darwin, create the revolutions that\n punctuate scientific progress. (T. S. Kuhn, in his The Structure of Sci\n entific Revolutions, calls these turning points in science ""paradigm-break\n ing.""3 I shall refer to them, along with the more modest ""important dis\n coveries,"" simply as ""breakthroughs."") Yet the connections of even such\n individual geniuses with their predecessors and their contemporaries\n are surely more direct and demonstrable than is the connection between\n Beethoven and Mozart, or Picasso and Renoir. As Newton wrote to Rob\n ert Hooke, ""If I have seen further (than you and Descartes) it is by\n standing upon the shoulders of giants.""4\n\n Nineteenth-century science was mainly conducted by geographically\n isolated, though intellectually interacting, individuals; much of today\'s\n science is conducted by large interdisciplinary teams. These teams often\n center around pieces of expensive equipment and are then said to be part\n of ""big science."" Team science is characteristically conducted in the\n large multipurpose scientific laboratory, an institution that is predom\n inantly a phenomenon of World War II and after. My purpose will be\n first to trace the origins of big team science and to examine its multipur\n pose institutions, second to estimate the capacity of this new scientific\n style to launch and carry off the scientific breakthroughs so necessary\n 1056\n\nThis content downloaded from \n(cid:0)198.51.92.3 on Mon, 06 Feb 2023 21:21:35 UTC(cid:0)\n(cid:0) \n\nAll use subject to https://about.jstor.org/terms\n\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n\x0c Scientific Teams and Scientific Laboratories\n\n for the progress of science, and finally to speculate on the future of team\n research and its institutions.\n\n I. The Origins of Big Team Science\n\n The emergence of the large interdisciplinary scientific team as the\n landmark of science can be traced to at least three separate develop\n ments. First is the extraordinary growth of science and the resulting in\n crease in the amount of scientific information produced; second is the\n emergence and institutionalization of applied science; third, and pos\n sibly most important, is the increasing complexity of scientific machinery.\n\n A. The Information Crisis and the Rise of Team Science\n\n The scientific information explosion has caused scientists to become\n more specialized. Some scientists respond to the information crisis by\n confining their range of scientific undertakings to those over which they\n can still retain command of the relevant information sources. Others\n form interdisciplinary teams in which are represented different though\n overlapping ranges of expertise or technique. In principle, the problems\n that can be tackled successfully by such teams ought to be more com\n plex than those tackled by individuals.\n\n This trend in the sociology of science was foreshadowed in an essay,\n\n ""The Limits of Science,"" by Eugene Wigner in 1950.5 Wigner argued\n that, for the reason I have mentioned, team research in which individual\n scientists are orchestrated into a productive whole by a scientific leader\n would become more common. He then asked how this new social struc\n ture would change the course of science. Could the theory of relativity\n or the Schrodinger equation have been discovered by an interdisciplinary\n team? Or, for that matter, could the mysteries of the ""omega-minus""\n particle and violation of charge-parity invariance (both discoveries of\n teams of high-energy physicists ) have been unearthed by the typical in\n dividual scientist of the nineteenth century? I shall return to these ques\n tions later.\n\n The information explosion has been the subject of many essays and\n\n studies. Here I will mention only how the spawning of the scientific in\n formation specialist has affected the organization of scientific research.\n In previous generations the scientist gathered his information more or\n less on his own and rather haphazardly. Today scientists of course con\n tinue to browse in this manner, but they are now backed by a host of\n information services, ranging from libraries to abstract services and\n specialized information centers.\n\n Already one can see the considerable influence of the information\n\n 1057\n\nThis content downloaded from \n(cid:0)198.51.92.3 on Mon, 06 Feb 2023 21:21:35 UTC(cid:0)\n(cid:0) \n\nAll use subject to https://about.jstor.org/terms\n\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n\x0c D/EDALUS\n\n specialists on those fields of science such as nuclear physics and high\n energy physics where the spectroscopy0 has become so elaborate as to\n outrun any single individual\'s capacity to hold all the relevant data in\n his mind. As a result, much of the output of the nuclear or high-energy\n spectroscopist goes to a secondary source?such as K. Way\'s or A. H.\n Rosenfeld\'s centers?where the data are compiled and collated. But in\n the process the role of the individual scientist who first made the mea\n surements is weakened; the citation now often tends to be to the sec\n ondary source rather than to the original experimenter. Could this mean\n that one of the delicious joys and motivations of science?recognition\n and approbation by one\'s peers?will be attenuated? Parts of basic sci\n ence have already acquired some of the facelessness that characterizes\n applied science, and this trend, it seems to me, will continue as the in\n formation crisis deepens.7\n\n B. The Emergence of Applied Science and\n the Large Industrial Laboratory\n\n A second source of the trend toward team science is the rise of ap\n plied science and particularly the growth of the large industrial labora\n tory. Here the interdisciplinary team has predominated from the first,\n for reasons that are implicit in the strategy of science?that is, the way\n that scientists choose what they do. To make this point clearer, I shall\n digress to consider the strategy of scientific research.\n\n Science is the ""art of the soluble"" according to Peter Medawar.8 What\n\n a scientist does is largely determined by what he thinks he can do suc\n cessfully. According to this view, science is a meandering stream that\n pushes salients out wherever the bank is weak and can be conquered;\n that such meandering may lengthen and make more tortuous the path\n to the sea is somewhat irrelevant. The river valley ( to push the metaphor )\n is irrigated more heavily, and becomes greener, as a consequence of the\n meandering.\n\n Insofar as Medawar is referring to basic science, his view of science\n as the art of the soluble contains much truth. In basic science, the sci\n entist\'s criteria for deciding what he ought to try are usually internally\n generated; that is, they derive from the internal logic of the sp?cific field\n in which he works and from his assessment of how soluble the problem\n is. Moreover, in basic science success is achieved if one solves the prob\n lem he sets out to solve, if he solves a different problem, or even if he\n can show that a particular approach is unfruitful. For all these reasons,\n in basic research it is acceptable to tailor problems to one\'s capacity for\n solving them. An expert in nuclear magnetic resonance can confine his\n researches to that segment of the field of nuclear magnetic resonance\n 1058\n\nThis content downloaded from \n(cid:0)198.51.92.3 on Mon, 06 Feb 2023 21:21:35 UTC(cid:0)\n(cid:0) \n\nAll use subject to https://about.jstor.org/terms\n\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n\x0c Scientific Teams and Scientific Laboratories\n\n over which he can comfortably retain command. Thus basic science ( at\n least before the advent of the big machine) with its internally generated\n problems, can be pursued adequately within a narrow discipline. If the\n problem takes the researcher out of his specialty, he is still observing the\n canons of pure science if he turns to a different problem that is more\n easily accommodated by his interest and competence. It is for this rea\n son that much of basic science can remain disciplinary and little: the\n interdisciplinary team is not its social characteristic.\n\n Of course, even as a description of basic little science this is over\n simplified; to characterize science as the art of the soluble tells only part\n of the story. Basic science, at its best, is the art of the soluble and the\n important (as Medawar himself recognizes). Researchers, even poor\n ones, usually have more ideas than they have resources with which to\n pursue them; their research strategy always amounts to choosing, from\n among a variety of soluble problems, the ones they regard as important.\n What constitutes importance in science? One, though certainly not the\n only, criterion is the degree to which a given piece of science relates to\n neighboring sciences. Indeed, the motivation for much basic scientific\n activity originates outside that activity. Sometimes the motivation lies\n in a neighboring basic science. For instance, a nuclear physicist may\n study light-element reactions because these are needed by an astro\n physicist who wants to understand the mechanism of stellar evolution.\n Sometimes the motivation lies in technology: a physicist may investigate\n the basic properties of plasmas because of their relevance to the con\n trolled release of thermonuclear energy. But the main point is that as\n soon as a scientist ventures to deal with a question arising in a field\n outside his discipline he has less control over where to look for a solu\n tion. He no longer has the luxury of narrowing the problem to what is\n soluble with his own expertise. Externally motivated science tends to\n be interdisciplinary and therefore more of a team activity than internally\n motivated science.\n\n Applied science is externally motivated par excellence. Its questions\n are posed from without: from engineering, military, and even social\n demands. Such questions usually transcend the individual disciplines.\n The criterion of success in applied science is simply, ""Does it work?"" not\n ""Does it add to knowledge in a particular discipline?"" Thus applied sci\n ence is characteristically interdisciplinary; it lends itself to?in fact it\n almost requires?teams of interacting individuals, none of whom by him\n self commands all the knowledge necessary to make progress, but all of\n whom, when taken as a whole, hopefully do.\n\n It is therefore no accident that the great institutions of applied\n science in industry and in government are typically homes for inter\n disciplinary teams. The jobs of these institutions are set outside the\n 1059\n\nThis content downloaded from \n(cid:0)198.51.92.3 on Mon, 06 Feb 2023 21:21:35 UTC(cid:0)\n(cid:0) \n\nAll use subject to https://about.jstor.org/terms\n\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n\x0c D^DALUS\n\n disciplines, even outside science; in consequence their style is interdis\n ciplinary. Though the first of these laboratories, such as the General\n Electric Research Laboratory, Bell Telephone Laboratories, and the\n National Bureau of Standards, appeared around the turn of this century,\n there was an enormous development of them during and after World\n War II. The best known of the wartime laboratories were the Radiation\n Laboratory at the Massachusetts Institute of Technology, which devel\n oped radar, and the Metallurgical and Los Alamos laboratories of the\n Manhattan Project, which developed the atomic bomb. My own experi\n ence has been almost entirely confined to the atomic energy laboratories,\n and so I shall draw largely on them to illustrate some characteristics of\n the big applied scientific institutions.\n\n From its very beginning in late 1941, the Chicago Metallurgical\n Laboratory, at which the first fission chain reaction was established, was\n interdisciplinary. Arthur H. Compton, the director of the Metallurgical\n Project, realized that the technology of the chain reactor would require\n physicists, mathematicians, chemists, instrument experts, metallurgists,\n biologists, and the various engineers who could translate these scientists\'\n findings into practice. The chain reactor was much more than a nuclear\n physicist\'s experiment. Uranium to fuel the first reactor had to be purified\n and reduced to metal. Graphite of unprecedented purity was needed to\n moderate the neutrons. The chemistry of the new element plutonium\n was largely unknown. The production of plutonium was very hazardous,\n and the most sophisticated instruments were needed to keep everything\n under control. The biological effects of the radiation that would be re\n leased had to be assessed if not mitigated.\n\n The difference between most interdisciplinary engineering enterprises\n and the engineering at the Metallurgical Laboratory lay in the incredible\n speed with which the latest scientific findings at the Laboratory were\n converted into engineered chain reactors. Eugene P. Wigner, who headed\n the theoretical physics group, began to engineer the water-cooled Han\n ford reactors in early 1942, almost ten months before the first chain re\n action had been established.\n\n There was nothing very complicated or obscure about the function\n and purpose of the Metallurgical Laboratory. Its output was a specific\n gadget and a specific process: the nuclear chain reactor and the produc\n tion and extraction of plutonium. If the reactor succeeded, the Labora\n tory succeeded; if it failed, the Laboratory failed. Because of this single\n ness of purpose, which at least for the first two years was evident to all,\n there was remarkably little difficulty in forging the teams necessary to get\n on with the job.\n\n Like most institutions of this sort, the Laboratory was organized into\n\n divisions. Enrico Fermi and Eugene Wigner were in charge of the\n physicists; James Frank and then Sam Allison and Farrington Daniels,\n 1060\n\nThis content downloaded from \n(cid:0)198.51.92.3 on Mon, 06 Feb 2023 21:21:35 UTC(cid:0)\n(cid:0) \n\nAll use subject to https://about.jstor.org/terms\n\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n\x0c Scientific Teams and Scientific Laboratories\n\n in charge of the chemists; Charles Cooper, the engineers; and so on. But\n the over-all project overwhelmed the disciplinary divisions. This was\n relatively easy because everyone knew the stakes; one could readily\n submerge his personal aspirations for the sake of achieving the whole\n objective. This is not to say that there was no tension between the proj\n ect and the divisions ( that is, the disciplines ). Even in the dark days of\n 1943 one could find physicists at the Metallurgical Laboratory working\n on the spherical harmonic method of solution of the Boltzmann equation\n ( an activity that at the time seemed like an unjustified luxury ) instead\n of estimating more routinely the multiplication constant of the latest\n reactor design.\n\n This criss-cross organization?with each scientist having a permanent\n\n home in a division but being lent out temporarily to an interdisciplinary\n project?is the usual organization in applied laboratories. The project\n leaders generally control the funds; the division leaders, the people.\n The projects maintain pressure on the division managers t? keep their\n outlook and activities relevant as judged by the projects; the disciplinary\n divisions maintain pressure on the project managers to keep their ac\n tivities up to the standards of sophistication imposed by the divisions.\n It is hoped that out of this criss-cross tension between project and di\n vision there will come both relevance and sophistication.\n\n The Metallurgical Laboratory was hierarchical. Arthur Compton was\n boss, but there were many other managers at lower levels, each on top\n of a pyramid of lesser and usually younger scientists. This pyramidal\n structure gave very great power to the man on top: he could command\n information resources; he could order investigations in many directions\n that would be out of the question had he not had a team at his disposal.\n In such hierarchical scientific teams, the members lower down must sub\n merge their personalities and to some extent their scientific instincts to\n those of the boss. One therefore finds genius in such organizations less\n often than in the universities, where science is conducted more individ\n ually. On the other hand, a really good man in a position at the top of a\n pyramid obviously can get much more done than he can if he works in\n the usual university setting. Glenn Seaborg at the Metallurgical Labora\n tory had about thirty chemists working for him, and in only two years his\n group elucidated much of the chemistry of plutonium in addition to\n developing a process for extracting plutonium that was used successfully\n at Hanford!\n\n Though there were many scintillating talents around?Szilard and\n\n Fermi and Wigner and Seaborg?the decisions were finally made by\n Compton. Yet, as in any organization, those with enough energy, con\n fidence, and ability could impose their views in the face of official rejec\n tion. At the Metallurgical Laboratory, a showdown of this sort occurred\n during 1942. The issue was the coolant?and therefore the whole en\n\n 1061\n\nThis content downloaded from \n(cid:0)198.51.92.3 on Mon, 06 Feb 2023 21:21:35 UTC(cid:0)\n(cid:0) \n\nAll use subject to https://about.jstor.org/terms\n\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n\x0c DAEDALUS\n\n gineering design?of the Hanford plutonium-producing reactors. The\n prevailing view held that since helium absorbed no neutrons, helium\n should be used to cool the reactors. With this view Wigner disagreed\n vigorously; he wanted to cool the reactors with water. To him, the\n handling of hot and somewhat radioactive helium under pressure seemed\n much more serious than the loss of nuclear performance caused by the\n tendency of hydrogen to absorb neutrons. In arguing his case, Wigner\n commanded all the relevant elements of knowledge?the engineering,\n the chemistry, the metallurgy, and the physics. And, when the Hanford\n reactors were actually to be built, the DuPont engineers chose the water\n cooled pile rather than the original helium-cooled version.\n\n Writing about these events twenty-seven years after they occurred,\n I am struck not by their uniqueness but by their generality. The Metal\n lurgical Laboratory, in Anthony Downs\'s terminology,9 was a bureaucracy\n ?that is, a large organization that is not governed, or is only indirectly\n governed, by the feedback from the marketplace. In this sense almost\n every large laboratory, even if it is part of a big corporation, is a bureauc\n racy; its connection with the marketplace is usually tenuous. Many of\n the organizational features and sources of power in the large laboratory\n are not characteristic specifically of a scientific establishment but rather\n of any large nonmarket establishment. The hierarchical structure, the\n possibility of the energetic individual prevailing against the official\n position, the great logistic power of a big laboratory,10 above all, the\n urgent imperative of the various groups to survive and to expand?\n all these are obvious to students of large organizations, scientific or\n otherwise.\n\n C. The Influence of the Big Scientific Machine\n\n The third thread in the development of big team science goes back\n\n for some of its spirit to the explorations of the fifteenth and sixteenth\n centuries. To a degree, we would have to regard the great explorers as\n geographers, and hence scientists of sorts. Their enterprises were on a\n grand scale by the standards of their time; they required large teams\n and much money. And at least Columbus among them politicked with\n John of Portugal and Queen Isabella in much the same way that a\n promoter of a large accelerator must now politick with the Atomic En\n ergy Commission or the National Science Foundation, or even with the\n President himself, to sell his project.\n\n Many of today\'s explorations in basic science involve such elaborate\n and expensive pieces of hardware that the whole enterprise requires\n much the same mobilization of resources as was required by the ex\n plorers. The most extreme example today of huge mobilization of re\n 1062\n\nThis content downloaded from \n(cid:0)198.51.92.3 on Mon, 06 Feb 2023 21:21:35 UTC(cid:0)\n(cid:0) \n\nAll use subject to https://about.jstor.org/terms\n\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n\x0c Scientific Teams and Scientific Laboratories\n\n sources for a purpose that is at least partly scientific is the exploration\n of space. And even before we began to use rockets, earth-based astron\n omy had some of the attributes of modern big science: the 200-inch Hale\n telescope at Mount Palomar, completed in 1948, was one of the largest\n and most expensive pieces of scientific machinery until the advent of the\n large research reactors and large accelerators during and after the war.\n The new style of big science based on very large pieces of equipment\n\n is generally attributed to Ernest O. Lawrence. His 37-inch cyclotron at\n Berkeley was a monster for its time; this was followed by the 60-inch,\n the 184-inch, the synchrocyclotron, the proton synchrotron (Bevatron),\n in ever-increasing size and complexity. To be sure, there had been\n earlier scientific teams dominated by great leaders: J. J. Thomson and\n later Rutherford at the Cavendish Laboratory, Fermi and his neutron\n group in Rome, and, of course, the German institutes. But Lawrence\'s\n laboratory was probably the first in which the central piece of equip\n ment was so elaborate, and possibly so temperamental, as to require a\n more or less full-time engineering staff. The logistics of keeping the place\n going?whether this means the scientific machinery or the elaborate\n organization that tends the machinery?becomes an essential ingredient\n of the activity. There are engineers and instrument technicians and\n financial people and personnel experts, many of whom identify rather\n little with the purpose of the entire laboratory, but each of whom is\n valued for his specialized expertise.\n\n Thus the modern home of big basic science, especially the big ac\n celerator or reactor laboratory, acquires much of the flavor of the in\n dustrial laboratory. The time allotted for use of the machine is rigidly\n scheduled, and this imposes a regularity on the working habits at least\n of the technicians who tend the machine. There is a division of labor\n between those who are expert in electronics and computing and electrical\n engineering; and this requires coordination. The necessity for explicit\n planning is taken for granted, in much the same way as planning by a\n project manager is the accepted way of doing business in the applied\n laboratory.\n\n The typical home of massive basic science, like CERN in Geneva or\n the Stanford Linear Accelerator, is however more specialized than is\n the modern home of applied science; this goes back to the aforemen\n tioned distinction between basic science, which tends to be internally\n motivated and disciplinary, and applied science, which tends to be ex\n ternally motivated and interdisciplinary. The General Electric Research\n Laboratory covers a wider range of specialties than does the Stanford\n Linear Accelerator. The Argonne National Laboratory, with its experts\n ranging from biom?dical researchers and ecologists to high-energy physi\n cists, covers a wider range of specialties than does the nearby Fermi\n 1063\n\nThis content downloaded from \n(cid:0)198.51.92.3 on Mon, 06 Feb 2023 21:21:35 UTC(cid:0)\n(cid:0) \n\nAll use subject to https://about.jstor.org/terms\n\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n\x0c D^SDALUS\n\n National Accelerator Laboratory. I imagine that this greater specializa\n tion will in the long run pose some difficulties if the question of redeploy\n ing the large basic laboratories, like Fermi or SLAC, should ever arise.\n\n II. Individual Science and Team Science:\n Breakthroughs Versus Spectroscopy\n A. The Xenon Compounds: A Breakthrough by an Individual\n In 1962 Neil Bartlett, a young chemist at the University of British\n\n Columbia, stumbled onto the fact that oxygen could be oxidized by\n platinum hexafluoride. About the same time Bartlett had noticed, while\n browsing through a table of ionization potentials, that the energy re\n quired to strip an electron off xenon (to form Xe+) was about the same\n as that required to form the 0./ ion. He therefore concluded it was worth\n trying to oxidize xenon with PtF6. Almost on his first try he was success\n ful, and, in 1962, sixty-eight years of chemical dogma came to an end:\n the first stable compound of a noble gas, Xe(PtF6), was produced. The\n noble gases were no longer noble.11\n\n Immediately after Bartlett\'s discovery, a group of chemists at the\n Argonne National Laboratory plunged into the new chemistry of the\n noble gases. They came to this task well prepared: for many years they\n had been interested in the chemistry of the fluorine compounds of\n plutonium and other transuranics. Their laboratories were well equipped\n for handling treacherous, extremely toxic materials like elemental fluorine\n and PuF6. Almost immediately H. H. Claassen, J. G. Malm, and H. Selig\n discovered that xenon could be oxidized by fluorine alone, and within\n a year of frenzied activity many compounds of xenon and other noble\n gases were prepared and characterized. A blank page in inorganic chem\n istry had been expanded into a good-sized, well-filled book.12\n\n This incident serves to illustrate, in almost too perfect outline, the\n usually held stereotypes as to the strengths and the weaknesses of the\n traditional individual and the newer team styles of research. The brilliant\n initial stroke?Bartlett\'s crazy idea that xenon could be oxidized if only\n one chose a sufficiently strong oxidant?was very much the doing of an\n individual. Not that this idea was absolutely new: in 1933 Linus Pauling\n had suggested that stable xenon compounds exist, and D. M. Yost even\n tried, unsuccessfully, to prepare them at California. And, even closer to\n home, at Oak Ridge S. S. Kirslis, F. H. Blankenship, and W. R. Grimes,\n who were developing a reactor that was fueled with molten uranium\n fluoride, had noticed that the fission product xenon consistently was miss\n ing, whereas the fission product krypton was always present as expected\n in the gas phase. The question of whether the xenon could be disappear\n ing as a chemical compound did arise and was discussed but of course\n 1064\n\nThis content downloaded from \n(cid:0)198.51.92.3 on Mon, 06 Feb 2023 21:21:35 UTC(cid:0)\n(cid:0) \n\nAll use subject to https://about.jstor.org/terms\n\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n\x0c Scientific Teams and Scientific Laboratories\n\n was rejected, although, as it turned out, XeF4 was being produced. Sci\n entific dogmas of such strength as the nobility of the rare gases are hard\n to dethrone.\n\n But, once the brilliant, individual breakthrough had been made, the\n\n integrated team and the great logistic power of the National Laboratory\n moved in, massively and professionally, to fill in the spectroscopic de\n tails. A field that in earlier times would have remained fertile and excit\n ing for a decade or more was largely elucidated in little more than a\n year\'s time.\n\n In attacking the xenon compounds so massively, the Argonne Na\n tional Laboratory was working very much in the style of the applied\n laboratory. There was a group leader with a staff of highly professional\n people, each of whom was an expert. Bartlett with graduate students\n probably would have been no match for Argonne with its professionals.\n And indeed, this extraordinary elaboration of a field of chemistry in\n just a year has led some to suggest that at least in the field of chemistry\n the future belongs to the professional team supported with superb\n equipment and unencumbered by teaching commitments, rather than to\n the professor whose professionalism in research is diluted by teaching.13\n This view has been sharply criticized by representatives of the univer\n sity scientific community who insist that only individuals can achieve\n breakthroughs.\n\n In point of fact, the team can and has achieved breakthroughs, and\n it is by no means clear that the team will snuff out the fire of scientific\n revolution. In the table below, as an example, I list the Nobel Prizes\n in physics during the past twenty years, the time during which team\n physics has grown so markedly. Of course not every discovery that wins\n a Nobel Prize breaks a paradigm, but I believe most physicists will agree\n that these discoveries at the very least represent important breakthroughs.\n Though the individual winners exceed the team winners, the fact remains\n that team science has produced several Nobel Prizes in physics. Indeed,\n examples of teams achieving breakthroughs are not hard to find. I shall\n describe one that occurred in Oak Ridge in the past few years.\n\n B. Anomalous Losses in Channeling: A Breakthrough by a Team\n\n Charged particles in traversing crystals often become trapped in chan\n nels formed by rows of regularly spaced atoms. This phenomenon, called\n channeling, was predicted theoretically by Mark T. Robinson in 1962,\n and then was discovered experimentally. In 1964 a team at Oak Ridge,\n consisting of several nuclear physicists, solid state physicists, and a\n physical chemist, examined the energy loss of the channeled particles\n as they emerged from thin crystalline gold foils. They were astonished\n\n 1065\n\nThis content downloaded from \n(cid:0)198.51.92.3 on Mon, 06 Feb 2023 21:21:35 UTC(cid:0)\n(cid:0) \n\nAll use subject to https://about.jstor.org/terms\n\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n\x0c Nobel Prizes in physics, 1948-1968.\n\n Discovery\n\n Team Individual\n\n DJEDAIAJS\n\n Winner\n Year\n 1948 P. M. S. Blackett\n\n 1949 H. Yukawa\n 1950 C. F. Powell\n\n 1951 Sir J. D. Cockcroft\n\n E. T. S. Walton\n\n 1952 F. Bloch\n\n E. M. Purcell\n\n 1953 F. Zernike\n 1954 M. Born\n W. Bothe\n 1955 P. Kusch\n\n W. E. Lamb\n 1956 W. Shockley\n\n W. H. Brattain\n J. Bardeen\n 1957 C. N. Yang\n\n T. D. Lee\n\n 1958 P. A. Cerenkov\n\n I. Y. Tamm\n I. M. Frank\n 1959 E. G. Segr?\n\n O. Chamberlain\n\n 1960 D. A. Glaser\n 1961 R. Hofstadter\n\n R. L. M?ssbauer\n\n 1962 L. D. Landau\n 1963 E. P. Wigner\n\n Development of the Wilson\n method and discovery by this\n method of the r- and /x-mesons\n Prediction of mesons\n Development of the photographic\n method of the study of nuclear\n processes and discovery concern\n ing mesons\n Cockcroft-Wal ton accelerator\n and first disintegration\n Nuclear magnetic resonance\n\n Phase-contrast microscope\n Quantum mechanics; coincidence\n method\n Lamb shift; anomalous magnetic\n moment of electron\n Transistor\n\n Nonconservation of parity\n\n Cerenkov effect\n\n Antiproton\n\n Bubble chamber\n Electron nucle?n and nuclear\n interaction; M?ssbauer effect\n Liquid helium, etc.\n Shell theory; symmetry in physics\n\n Maria Goeppert-Mayer\n J. H. D. Jensen\n\n 1964 C. H. Townes\n\n N. Basov\n A. Prokhorov\n 1965 R. P. Feynman\n\n Julian S. Schwinger\n S. Tomonaga\n\n 1966 A. Kastler\n 1967 H. Bethe\n 1968 L. W. Alvarez\n\n Maser and laser\n\n Quantum electrodynamics\n\n Optical pumping\n Nuclear (astro) physics\n Giant bubble chamber and\n resonances obtained with it\n\n * Strongly influenced by the wartime teamwork on radar.\n ** Small team of students.\n\n 1066\n\nThis content downloaded from \n(cid:0)198.51.92.3 on Mon, 06 Feb 2023 21:21:35 UTC(cid:0)\n(cid:0) \n\nAll use subject to https://about.jstor.org/terms\n\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n\x0c Scientific Teams and Scientific Laboratories\n\n to find that the particles lost their energy in discrete jumps: the amount\n of energy a particle lost depended very sensitively on the angle with\n the channel axis at which the particle entered the channel. From this\n quite accidental discovery came a completely new and possibly quite\n powerful method of probing the details of the interatomic potential in\n certain crystals.\n\n I tell this story because it illustrates the unique power of an interdis\n\n ciplinary attack. Here is an instance in which the whole team is much\n more than the sum of its separate components, where a team as opposed\n to an individual (as in the case of Bartlett and xenon compounds)\n achieves a breakthrough. First, the experiments required very thin, per\n fect gold crystalline foils; these happened to be the specialty of T. S.\n Noggle, a metallurgist and electron microscopist. Next the 50-MeV\n (million electron volts) iodine ions had to be accelerated, and their\n energies after degradation had to be measured with precision; this re\n quired experts on Van de Graaff accelerators and particularly on so\n phisticated time-of-flight techniques. Once the phenomenon was dis\n covered, its full significance required the insight of a young solid state\n theorist, H. Lutz, as well as a variety of additional experiments that\n served to corroborate the theoretical predictions. And the team required\n orchestration: this was supplied by S. Datz, a chemist who had been\n concerned with the related phenomenon of sputtering.\n\n To be sure, elaborate equipment was needed?a time-of-flight Van\n de Graaff machine. Nowadays this is not so unusual; there are perhaps\n two dozen laboratories which possess such instruments. But the num\n ber having at the same time an electron microscopist who can make\n perfect gold crystals a few hundred angstroms thick, an expert on\n sputtering, and a solid state theorist capable of interpreting the experi\n ments is much smaller. It was very much more the style of research?\n the willingness of all parties to collaborate fully?that led to the break\n through. This willingness among professionals to collaborate is actually\n not to be taken for granted, especially in the academic world. In reading\n James Watsons The Double Helix14 one is constantly aware of the\n barriers that were placed between Watson and Francis Crick (who had\n the major idea about the structure of RNA) and Rosalind Franklin\n and Maurice Wilkins ( who had the means for making the measurements\n needed). I suppose it is for reasons such as this that I am convinced the\n success of team science depends on the institution. There must be a\n tradition of interdisciplinary collaboration between professionals. This\n is more likely to exist in an institute with hierarchical organization?\n such as one finds in the applied or project laboratories?than in the\n typical university.\n\n The example I have given would only marginally qualify as big\n 1067\n\nThis content downloaded from \n(cid:0)198.51.92.3 on Mon, 06 Feb 2023 21:21:35 UTC(cid:0)\n(cid:0) \n\nAll use subject to https://about.jstor.org/terms\n\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n\x0c D^DALUS\n\n science: the machines, though large, are not all that large. If one ex\n amines the more typical endeavors of big science, particularly those that\n require unique accelerators or unique reactors, one finds many examples\n of breakthroughs by teams. One of the most recent is the finding by\n Val Fitch and his collaborators (many of whom were students) of the\n nonconservation of charge parity in the decay of the K-meson or, per\n haps even more uniquely tied to the capacity of a single machine, the\n discovery by Segr? of the antiproton, at the Lawrence Radiation Lab\n oratory in Berkeley. In these cases I would argue that it was the ma\n chine properly used and good leadership more than the team. The\n team was needed mainly because the experiment was so complex. In a\n certain sense, the team tends to be incidental to the machine in very\n big science; by contrast, the team is central in those cases?such as the\n work on DNA and channeling?in which the means are more modest.\n Here what is important is a delicate balancing and interweaving of\n individual expertise.\n\n So we see that teams can achieve breakthroughs, operating either\n\n in the interdisciplinary mode or in the big science mode. Yet the power\n of the team seems to me to lie primarily in its ability to do spectroscopy;\n as team science becomes more and more common, so might the emphasis\n on the spectroscopic style of science. This trend may be accentuated by\n the weightiness and inertia of modern big science. Where scientific\n teams have mobilized around very big pieces of machinery there is an\n understandable incentive to exploit that machine. The path of de\n velopment, instead of following the logical demands of the discipline,\n tends to be constrained to directions that are made accessible by the\n machinery at hand.\n\n Something like this has always happened in science: one exploits\n\n whatever tools one has available. But scientists are naturally much less\n ready to scrap a 400-MeV proton-synchrocyclotron that costs several mil\n lion dollars, but which no longer can cut at the main edge of high\n energy physics, than they are to scrap, say, an optical microscope. The\n somewhat bureaucratic imperative to exploit expensive machinery cir\n cumscribes the direction of scientific growth. The spectroscopic filling\n in of details tends to crowd out the breakthroughs, simply because the\n number of breakthroughs possible with a particular machine is very\n small compared with the practically infinite spectroscopic detail the\n machine can generate.\n\n It would be foolish to underestimate the importance of spectroscopy\n\n in setting the groundwork for important discoveries and conceptual\n breakthroughs. Quantum mechanics would have been impossible with\n out its underlying detailed optical spectroscopy. Or, more recently, low\n energy physics has a strongly spectroscopic flavor. Most experiments\n 1068\n\nThis content downloaded from \n(cid:0)198.51.92.3 on Mon, 06 Feb 2023 21:21:35 UTC(cid:0)\n(cid:0) \n\nAll use subject to https://about.jstor.org/terms\n\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n\x0c Scientific Teams and Scientific Laboratories\n\n seek to measure, in various nuclides, specific properties that already\n fit into a general theoretical framework. Yet out of this elaborate spec\n troscopy (conducted, incidentally, by teams) has come a seemingly\n endless succession of breakthroughs: either in experimental techniques,\n as in the discovery of the lithium-drifted germanium-detector, or in\n new insights into nuclear structure, as in the discovery of isobaric\n analogue states and short-lived isomers.\n\n There is another side to the story which deserves mention. The team,\n\n especially around the big machine, is a powerful scientific device. More\n difficult experiments can be tried with a large team equipped with a\n unique facility than with a smaller outfit not so equipped. Thus, insofar\n as breakthroughs flow from difficult experiments, one might expect\n teams working with powerful and unique apparatus to continue to con\n tribute their share of important discoveries. For example, as soon as the\n high-flux isotope reactor became available, questions in the phonon dis\n tribution in solids that had plagued solid state physicists became an\n swerable.\n\n To make important breakthroughs in science will always require\n competent, imaginative leadership. But it seems reasonable to expect\n that the degree of insight required to make such discoveries may be\n somewhat less than it was in the day of individual science: the team,\n or the big machine, may offer elements of uniqueness that were formerly\n supplied by sheer intellectual power. And, since competence is so much\n more common than genius, the team may be spreading the possibility\n of significant scientific discovery to many more scientists than in former\n days. Perhaps this democratization will prove to be one of the main\n by-products of big team science.\n\n III. The Future of Team Research\n A. The Institutional Setting\n It seems clear to me that team science in the modern style is done\n better in the hierarchical, logistically strong institute than it is in the\n university. This, coupled with the unrest that wracks the university,\n suggests that we might see a gradual movement of modern science\n away from the university and toward the national institute?possibly\n even a growing separation between education and research. To most\n writers on this subject, especially since the Seaborg Report,15 the notion\n that research and education are inseparable and indissoluble, that\n the one cannot be done without the other, has acquired the ring of holy\n dogma. But the facts do not really bear this out: certainly insofar as\n one is elaborating a certain area, such as the chemistry of xenon, pro\n fessionals are better than students. For many years applied chemistry\n\n 1069\n\nThis content downloaded from \n(cid:0)198.51.92.3 on Mon, 06 Feb 2023 21:21:35 UTC(cid:0)\n(cid:0) \n\nAll use subject to https://about.jstor.org/terms\n\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n\x0c D^DALUS\n\n has been conducted to great advantage in the industrial laboratory\n without benefit of students. I know that at Oak Ridge some (though\n not all) of our division directors are convinced that they achieve results\n more quickly and more reliably with professionals than with students.\n The universities have responded to the trend toward team research\n\n by setting up institutes, interdisciplinary and logistically strong, where\n team research can be performed effectively, but largely by students.\n But on the average these institutes suffer from a mismatch between the\n social ethos of the university and the social ethos of the institute: the\n one is individual and democratic, the other collective and hierarchical.\n When the institute acquires a collective and hierarchical character,\n which I believe is necessary for its success, its tie with the university\n department becomes more tenuous.\n\n So we may be going full circle. Science in the seventeenth and\n eighteenth centuries was practiced predominantly in the academies,\n not the universities. It moved into the German and English universities\n in the nineteenth century. And perhaps, with the growth of the large\n team, it may gradually be moving out again, or at least it may not retain\n as intimate a connection with the university as it has had in the im\n mediate past.\n\n B. New Fields for Team Research: The Rise of Big Biology\n The big interdisciplinary team has generally been confined to the\n\n physical sciences and to engineering. The biological sciences have re\n mained the bastion of little, individualistic science, probably because\n the experimental tools needed to conduct biological experiments have\n typically been small and relatively inexpensive. Yet there are now im\n portant trends toward large interdisciplinary teams in the biom?dical\n sciences and, very recently, teams that include engineers as well as\n physical scientists.\n\n Part of this trend comes from the rise of molecular biology. In one\n\n sense the most important parts of molecular biology are really a branch\n of crystallography; the double helix model for DNA, for example, is\n based on a crystal structure deduced from X-ray diffraction data. Wat\n son in his book bemoans his lack of expertise in crystallography, a lack\n which was made up for him by Crick and by Wiltons\' group. It is\n highly significant that of the two men who made the most important\n discovery of modern biology, one (Crick) was originally a physicist.\n Again, the extraordinary elucidation of the working of peripheral nerve,\n for which A. L. Hodgkin and A. F. Huxley received the Nobel Prize,\n would have been impossible had it not been for the underlying work on\n electrical properties of nerves by the physicist K. S. Cole. Many biolo\n 1070\n\nThis content downloaded from \n(cid:0)198.51.92.3 on Mon, 06 Feb 2023 21:21:35 UTC(cid:0)\n(cid:0) \n\nAll use subject to https://about.jstor.org/terms\n\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n\x0c Scientific Teams and Scientific Laboratories\n\n gists, especially in the most active fields of biochemistry, feel it neces\n sary to rub shoulders with physicists and with physical chemists.\n\n The second trend discernible in biom?dical science is the rise of the\n very large-scale experiment. With our present concern with low-level\n insults to the biosphere (radiation, pesticides, smog), it becomes neces\n sary to conduct animal experiments on a scale far greater than had\n hitherto been customary in biology. The husband and wife team of\n William B. and Liane Russell at Oak Ridge maintains more than\n 100,000 mice in order to study the mutagenic effect of moderate levels\n of radiation. Such biological experimentation, immediately becomes a\n team activity: geneticists to manage the entire experiment; statisticians\n to scan the data for significance; veterinarians to manage the animals;\n pathologists to look for somatic effects; and, of course, the whole array\n of animal attendants, janitors, and cage-washers who are needed to keep\n 100,000 animals alive and thriving.\n\n I would expect this trend toward very large-scale animal experimenta\n\n tion to become increasingly prevalent as we become more sensitive to\n the widespread influence of seemingly subtle factors in our environment.\n A call for such experimentation has been made by Ren? Dubos, for\n example. Should this call be answered by the funding agencies, we may\n expect to see an increasing fraction of biology being conducted in the\n style of big science.\n\n Biologists, particularly biochemists, are beginning to learn how to\n employ engineers and other supporting scientists, notably analytical\n chemists. This represents a new trend since biological institutes tradi\n tionally have not crossed deeply into the physical sciences, even less\n into engineering. True, the National Institutes of Health is an enor\n mously large complex, but NIH has not had within it a strong tradition\n in the physical sciences or in process engineering. By contrast, the\n atomic energy laboratories have from the beginning spanned the bio\n logical sciences, the physical sciences, and the engineering sciences.\n This unusual juxtaposition has now begun to pay off?for example, in\n the brilliant development of zonal centrifuges under the leadership of\n Norman Anderson at the Oak Ridge complex. These centrifuges, which\n were first developed to separate uranium isotopes, have been modified\n by Anderson, together with a large team of engineers, to handle biolog\n ical materials. The centrifuges are now being used very widely to\n separate, on a large scale, various cell moieties; for instance, they have\n been used to purify flu vaccine of its antigenic protein impurities.\n Anderson is now exploiting in his Molecular Anatomy Program ( MAN )\n whatever relevant engineering and analytical expertise he can find in\n the Oak Ridge complex to systematically separate, and then prepare on\n large scale, the many cellular particles which now can only be seen in\n 1071\n\nThis content downloaded from \n(cid:0)198.51.92.3 on Mon, 06 Feb 2023 21:21:35 UTC(cid:0)\n(cid:0) \n\nAll use subject to https://about.jstor.org/terms\n\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n\x0c DJEDALVS\n\n the electron microscope. Anderson\'s success I believe is only the fore\n runner of future successes that biology will enjoy as it enlists the co\n operation of the engineering sciences.\n\n C. Redeployment of the Big Institutions\n\n The modern scientific team arose as an integral part of the great\n laboratories; whether in basic research or in applied research, the team\n style is the dominant mode in the modern big laboratory. It seems in\n evitable then that the future of team research will depend on the fate\n of the big laboratories. I shall therefore close with a few speculations\n on these institutions.\n\n Though it is evidently impossible to generalize, one can see limits\n to the prospects of the big laboratories. For example, in those institu\n tions devoted to high-energy physics, the future is limited by the sheer\n increase in expense of the necessary gadgets. The Alternating Gradient\n Synchrotron facility which was completed in 1960 cost $30,650,000; the\n Stanford Linear Accelerator, completed in 1966, cost $114,000,000; and\n the National Accelerator, a 200-GeV proton synchrotron, expected to be\n operating in 1972, is estimated to cost some $240,000,000. Presumably\n each of these devices will become obsolete, not because there is not\n always more spectroscopy to be done, but rather because people will\n eventually get bored with spectroscopy. Unless there are occasional\n stirring breakthroughs?perhaps a breakdown of quantum electrody\n namics?I cannot visualize these institutions forever sustaining them\n selves, or remaining immortal, by simply amassing spectroscopic details\n about elementary particles.\n\n The atomic energy laboratories must also face questions of redeploy\n\n ment, though for a different reason. True, the two central problems of\n nuclear energy?breeding and controlled fusion?have yet toi be solved.\n But even these are questions of finite dimensions; the first because it is\n not all that difficult, the second because it may prove so difficult that\n interest in it will wane. There is already evidence that the world\'s\n atomic energy laboratories are beginning to adjust to these facts, mainly\n by expanding their areas of concern beyond nuclear energy.\n\n By contrast, the future of the great biological laboratories seems\n clear enough: the questions which biom?dical science seeks to answer\n are urgent, massive, timeless until they are solved. It seems likely\n therefore that the need for redeployment will hardly arise for, say, the\n instrumentalities of the NIH. My guess is that these institutions will ac\n quire a more interdisciplinary flavor, especially by developing engineer\n ing skills, simply because the cross between the physical sciences and\n biom?dical research has been so fruitful.\n 1072\n\nThis content downloaded from \n(cid:0)198.51.92.3 on Mon, 06 Feb 2023 21:21:35 UTC(cid:0)\n(cid:0) \n\nAll use subject to https://about.jstor.org/terms\n\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n\x0c Scientific Teams and Scientific Laboratories\n\n Thus redeployment of at least some of the big laboratories is in the\n cards. This realization comes at a time when we hear much about the\n many social and socio-technological conflicts that plague our modern\n society?racial unrest, the decay of the city, pollution, overpopulation.\n John Platt sees modern society on the verge of crises so profound as to\n warrant launching wartime-like projects to resolve them.16 The Com\n mittee on Government Operations of the United States Senate has been\n holding hearings during the past couple of years under the chairmanship\n of Senator Edmund S. Muskie aimed at establishing a Select Committee\n on Technology and the Human Environment. Everywhere there is a\n restlessness and concern: the priorities our society has lived with in the\n postwar world need reassessment; we must abjure our preoccupation\n with hard science and address ourselves to these subtler, more difficult,\n and more important human problems.\n\n Whether science can help very much with these social questions is a\n moot point. Many of us scientists believe that science can help: that\n almost every one of the conflicts and problems that we face has some\n technological, as well as social, components, and that therefore science\n directed specifically at their resolution may be helpful. In this we may\n be displaying an uninformed na?vet?; perhaps racial conflict, urban\n decay, and overpopulation are beyond help even from science.\n\n Yet this much can be said: if science has something to offer toward\n\n resolving these questions, it surely will have to be a broadly inter\n disciplinary, team type of science. The social components of these prob\n lems are more obvious than are the technological ones, but there is\n always an interaction between the two aspects; it is quite natural to\n visualize interdisciplinary teams, ranging over social science as well as\n natural science and engineering, being mobilized to attack some of these\n desperately troublesome questions. As of now, however, such teams\n have no natural home: the university is unsuitable because of its prej\n udice against teams, the national laboratory because of its inexperience\n in social science. I have therefore suggested the creation of new entities,\n national socio-technological institutes. Some such institutes might be\n formed ab initio; others by co-opting experts in the social sciences to\n work in existing hardware-oriented laboratories.\n\n National socio-technological institutes at which one would apply\n the methods of science to our difficult socio-technological problems\n might also serve an entirely different purpose: a means of focusing the\n socially relevant energies of our young people. Many young students\n seem to be disillusioned with natural science: those who in the previous\n decades went into physics or chemistry now go into the more ""relevant""\n social sciences, and even the students of the natural sciences are acquiring\n a taste for socially relevant issues. Yet I can foresee this socially motivated\n 1073\n\nThis content downloaded from \n(cid:0)198.51.92.3 on Mon, 06 Feb 2023 21:21:35 UTC(cid:0)\n(cid:0) \n\nAll use subject to https://about.jstor.org/terms\n\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n\x0c DAEDALUS\n\n cohort of students being frustrated all over again if, once they are trained,\n once they are readied to do battle on behalf of society, they find no in\n strumentalities to which they can attach themselves to carry on their com\n mendable crusade. I should think that just as the institutions of big sci\n ence for several decades provided a home for the aspiring scientists of the\n 1950\'s and 1960\'s, so the socio-technological institutions might provide a\n home for the aspiring social engineers of the 1970\'s.\n\n There have been several suggestions by now for national socio\n technological institutes, most recently, in Senate Bill 3410 sponsored\n by Senators Howard Baker and Edmund Muskie to establish national\n environmental laboratories. It is premature to really assess such proposals.\n It could be that the difficulties we face go beyond resolution by the\n methods of science?hard analysis, empirical observation, engineering\n design. Yet, before taking so pessimistic a view of man\'s capacity for\n self-betterment, I would urge trying the interdisciplinary team attack?\n an approach that was so notably successful in the generation immediately\n following World War II and that just may help guide us during the\n coming generation.\n\n References\n\n 1. For alternative approaches to this topic see L. Kowarski, ""Team Work and In\n dividual Work in Research,"" in N. Kaplan, Science and Society (Chicago: Rand\n McNally, 1965), pp. 247-255; and Cecil F. Powell, ""Promise and Problems of\n Modern Science,"" Concluding Address, Maria Sklodowska-Curie: Centenary\n Lectures, Proceedings of a Symposium, Warsaw, October 17-20, 1967 (Vienna:\n International Atomic Energy Agency, 1968 ).\n\n 2. Dr. Saul Benison pointed out at the Bellagio conference that I may be over\n doing this distinction between art and science: Leonardo trained in the atelier\n of Verrochio, who influenced much of his early style; Melville was much in\n fluenced by the Bible and by Shakespeare. Yet the connection between, say, the\n physicist Hertz and his predecessor Maxwell is, to my mind, far more explicit\n and continuous than the connection between two artists. Hertz used Maxwell\'s\n equations precisely as Maxwell formulated them; his work flows from Maxwell\n with an inevitability and logic that can never be matched in the work of an\n artist who follows an illustrious predecessor. As Professor Edward Shils puts it,\n ""There is a coercive element in the tradition of the sciences that is absent in\n the arts.""\n\n 3. University of Chicago Press, 1962.\n\n 4. ""Letter to Robert Hooke, February 5, 1675/6,"" in John Bartlett, Familiar\n\n Quotations (Boston: Little, Brown and Company, 1968).\n\n 5. Proceedings of the American Philosophical Society, 94 (October 1950), pp. 422\n\n 427.\n\n 6. As will be apparent later in the discussion, I often extend and generalize the\n 1074\n\nThis content downloaded from \n(cid:0)198.51.92.3 on Mon, 06 Feb 2023 21:21:35 UTC(cid:0)\n(cid:0) \n\nAll use subject to https://about.jstor.org/terms\n\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n\x0c Scientific Teams and Scientific Laboratories\n\n word ""spectroscopy"" to mean both the activity and the results of filling in the\n scientific details after a major discovery has broken new ground.\n\n 7. The facelessness of big team research has been commented on by others?for\n example, Gerald Holton, ""Scientific Research and Scholarship, Notes Toward\n the Design of Proper Scales,"" Dsedalus (Spring 1962), pp. 362-399.\n\n 8. P. B. Medawar, The Art of the Soluble (London: Methuen & Company Ltd.,\n\n 1967).\n\n 9. inside Bureaucracy (Boston: Little, Brown and Company, 1967).\n\n 10. To anyone who has spent some time in a large laboratory, it must be perfectly\n\n clear what I mean by its great logistic power; but to\'those who are unfamiliar\n with such institutions, perhaps I can illustrate with the experience of a distin\n guished demographer who spent a summer at Oak Ridge National Laboratory\n studying urban decentralization. At the end of his stay I asked him what he\n thought of ORNL as a possible locale for demographic research. He replied:\n ""Demography would be revolutionized if it were conducted there. It would be\n converted from a small, rather individualistic enterprise into a big-scale, massive\n business. There would be huge computers with programmers and mathematicians\n to help one use them, experts of every sort available at the other end of the\n hall, as well as editorial assistants, draftsmen, travel agents; above all, they\n would be ready and willing to help you get on with the job."" If the large\n laboratory possesses so much logistic strength in the eyes of a demographer,\n one can imagine how much greater is its strength in the fields of science it\n was originally set up to exploit!\n\n 11. Neil Bartlett and N. K. Jha, ""The Xenon-Platinum Hexafluoride Reaction and\n Related Reactions,"" in H. H. Hyman, ed., Noble-Gas Compounds (Chicago:\n University of Chicago Press, 1963), pp. 23-30.\n\n 12. Hyman, ed., Noble-Gas Compounds; cf. J. H. Holloway, Noble-Gas Chemistry\n\n (London: Methuen & Company Ltd., 1968).\n\n 13. ""Basic Chemical Research in Government Laboratories,"" Report of the Panel\n on Basic Chemical Research in Government Laboratories of the Committee for\n the Survey of Chemistry, Division of Chemistry and Chemical Technology,\n National Academy of Sciences Report 1292-A (Washington, D.C., 1966).\n\n 14. James D. Watson, The Double Helix (New York: Atheneum, 1968).\n\n 15. ""Scientific Progress, the Universities, and the Federal Government,"" Statement\n by the President\'s Science Advisory Committee (Washington, D.C.: U.S. Gov\n ernment Printing Office, November 15, 1960).\n\n 16. John Platt, ""What We Must Do,"" Science, 166 (November 28, 1969), 1115\n\n 1121.\n\n 1075\n\nThis content downloaded from \n(cid:0)198.51.92.3 on Mon, 06 Feb 2023 21:21:35 UTC(cid:0)\n(cid:0) \n\nAll use subject to https://about.jstor.org/terms\n\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n(cid:0)\n\x0c'",20023981.pdf
"b""A\nu\n\nt\n\nh\no\nr\n \n\nM\na\nn\nu\ns\nc\nr\ni\np\n\nt\n\nA\nu\n\nt\n\nh\no\nr\n \n\nM\na\nn\nu\ns\nc\nr\ni\np\n\nt\n\nA\nu\n\nt\n\nh\no\nr\n \n\nM\na\nn\nu\ns\nc\nr\ni\np\n\nt\n\nA\nu\nt\nh\no\nr\n \n\nM\na\nn\nu\ns\nc\nr\ni\np\n\nt\n\nHHS Public Access\nAuthor manuscript\nBrain Imaging Behav. Author manuscript; available in PMC 2022 April 01.\n\nPublished in final edited form as:\nBrain Imaging Behav. 2021 April ; 15(2): 465\xe2\x80\x93474. doi:10.1007/s11682-021-00450-7.\n\nThe ENIGMA Brain Injury Working Group: Approach, Challenges, \nand Potential Benefits\n\nElisabeth A. Wilde1,2,3, Emily L. Dennis1,2,4,5, David F. Tate1,2,6\n1Department of Neurology, University of Utah School of Medicine, Salt Lake City, UT\n\n2George E. Wahlen VA Medical Center, Salt Lake City, UT\n\n3H. Ben Taub Department of Physical Medicine and Rehabilitation, Baylor College of Medicine, \nHouston, TX\n\n4Psychiatry Neuroimaging Laboratory, Brigham & Women\xe2\x80\x99s Hospital, Harvard Medical School, \nBoston, MA\n\n5Imaging Genetics Center, Stevens Neuroimaging & Informatics Institute, Keck School of \nMedicine of USC, Marina del Rey, CA\n\n6Missouri Institute of Mental Health, University of Missouri, St. Louis, MO\n\nAbstract\n\nThe Enhancing NeuroImaging Genetics through Meta-Analysis (ENIGMA) consortium brings \ntogether researchers from around the world to try to identify the genetic underpinnings of brain \nstructure and function, along with robust, generalizable effects of neurological and psychiatric \ndisorders. The recently-formed ENIGMA Brain Injury working group includes 10 subgroups, \nbased largely on injury mechanism and patient population. This introduction to the special issue \nsummarizes the history, organization, and objectives of ENIGMA Brain Injury, and includes a \ndiscussion of strategies, challenges, opportunities and goals common across 6 of the subgroups \nunder the umbrella of ENIGMA Brain Injury. The following articles in this special issue, including \n6 articles from different subgroups, will detail the challenges and opportunities specific to each \nsubgroup.\n\nIntroduction to ENIGMA\n\nThe Enhancing NeuroImaging Genetics through Meta-Analysis (ENIGMA; enigma.usc.edu) \nconsortium was formed in 2009 in an effort to increase power to detect associations between \n\nTerms of use and reuse: academic research for non-commercial purposes, see here for full terms. http://www.springer.com/gb/open-\naccess/authors-rights/aam-terms-v1\nPlease address correspondence to: Dr. Emily L Dennis, TBICC, Dept of Neurology, University of Utah School of Medicine, \nemily.dennis@hsc.utah.edu. \nConflicts of interest\nThe authors report no potential conflicts of interest related to this project.\nPublisher's Disclaimer: This Author Accepted Manuscript is a PDF file of a an unedited peer-reviewed manuscript that has been \naccepted for publication but has not been copyedited or corrected. The official version of record that is published in the journal is kept \nup to date and so may therefore differ from this version.\n\n\x0cA\nu\n\nt\n\nh\no\nr\n \n\nM\na\nn\nu\ns\nc\nr\ni\np\n\nt\n\nA\nu\n\nt\n\nh\no\nr\n \n\nM\na\nn\nu\ns\nc\nr\ni\np\n\nt\n\nA\nu\n\nt\n\nh\no\nr\n \n\nM\na\nn\nu\ns\nc\nr\ni\np\n\nt\n\nA\nu\nt\nh\no\nr\n \n\nM\na\nn\nu\ns\nc\nr\ni\np\n\nt\n\nWilde et al.\n\nPage 2\n\ngenetic variation and brain structure and function. ENIGMA has since expanded to examine \nalterations in brain structure and function across a number of disorders, with or without also \nincluding genetic data. The name ENIGMA, which Webster defines as \xe2\x80\x9cmysterious, \npuzzling, or difficult to understand or explain\xe2\x80\x9d, also invokes the endeavors of the British \nteam at Bletchley Park to decode highly sophisticated war-time communications during \nWorld War II; similarly, ENIGMA brings investigators together to decode the complex and \nmultifaceted factors that influence brain structure and function. At the time of its inception, \nthe focus on candidate genes in imaging genetics led to a crisis of reproducibility, but less \nbiased genome-wide association studies (GWAS) required tens or hundreds of thousands of \nparticipants to achieve significance. In 2014, ENIGMA was funded as an NIH Big Data to \nKnowledge (BD2K) Center of Excellence. ENIGMA has resulted in the largest-ever \nneuroimaging datasets of numerous disorders to date, including Major Depression, \nSchizophrenia, and Epilepsy. ENIGMA currently includes 30 disease working groups, 4 \ngroups on healthy variation over the lifespan, and 9 groups focused on methods \ndevelopment. There are currently over 1400 investigators from 40 countries participating in \nENIGMA activities (see Figure 1). ENIGMA has received funding through over 20 grants \nacross the United States, the European Union, and Australia. For a recent review of broader \nENIGMA activities, see (Thompson et al. 2020).\n\nIn the fall of 2016, the ENIGMA Brain Injury group was formed. From its inception, it was \nclear that the complexity of brain injury would necessitate specialized groups that could \nmore readily address unique features of the varying cohorts. Within the first year, multiple \nsubgroups were identified, including Pediatric Moderate/Severe Traumatic Brain Injury \n(TBI) (msTBI) (Dennis et al. 2020), Military-Relevant TBI (Tate et al. 2020), and Sport-\nRelated Head Injury (Koerte et al. 2020). Soon after, groups for Adult msTBI (Olsen et al. \n2020) and Acute Emergency Department (Civilian) Mild TBI were formed, followed by a \ngroup focusing on Intimate Partner Violence (Esopenko et al. 2020). The newest groups to \nbe formed are focused on emerging imaging methods that may have particular relevance in \nTBI, namely Magnetic Resonance Spectroscopy (MRS)(Bartnik-Olson et al. 2020), Arterial \nSpin Labeling (ASL), resting state fMRI (rsfMRI), and Cognitive Endpoints (see Figure 1). \nAdditional groups will likely be added in the future to address other aspects of \nmethodological and imaging development as well as other TBI-relevant patient populations. \nThe focus of this special issue is on the ENIGMA-Brain Injury working group\xe2\x80\x99s efforts to \nfacilitate research in TBI and concussion.\n\nFormation of ENIGMA Brain Injury\n\nGoals and Benefits\n\nThe overarching goal of the ENIGMA effort is to create a collaborative framework where \ninvestigators can work together to address questions and objectives that require large \namounts of data and to promote replication of preliminary findings through the use of \nmultiple and independent samples. Collaboration enables investigators to overcome common \nobstacles which often limit sample sizes in this area of research, including the expense of \nacquiring neuroimaging data and limited sample sizes. The intent of ENIGMA is to \naccelerate the pace of investigation through harnessing the enormous intellectual resources \n\nBrain Imaging Behav. Author manuscript; available in PMC 2022 April 01.\n\n\x0cA\nu\n\nt\n\nh\no\nr\n \n\nM\na\nn\nu\ns\nc\nr\ni\np\n\nt\n\nA\nu\n\nt\n\nh\no\nr\n \n\nM\na\nn\nu\ns\nc\nr\ni\np\n\nt\n\nA\nu\n\nt\n\nh\no\nr\n \n\nM\na\nn\nu\ns\nc\nr\ni\np\n\nt\n\nA\nu\nt\nh\no\nr\n \n\nM\na\nn\nu\ns\nc\nr\ni\np\n\nt\n\nWilde et al.\n\nPage 3\n\nand computational power that exists across the globe, not only with regard to a particular \ndisease entity, but also through interfacing with others possessing technical expertise in \nimaging, genetics, computational science, or with expertise in conditions that may be \ncomorbid (e.g., TBI and PTSD) or may modify disease outcome (e.g., developmental \nissues). Given many unique clinical and functional features of TBI (i.e., spatial and \nfunctional heterogeneity of injury, common comorbidities, etc), the ENIGMA model poses \nmany attractive solutions to addressing important clinical questions.\n\nImmediate goals of the ENIGMA Brain Injury working group are to conduct analyses using \nmultiple datasets to find robust effects of brain injury across samples, using mega-analysis \n(direct pooling of data points from different sources) when possible to answer questions that \nrequire larger samples. Meta-analysis (use of effect sizes from the existing studies or data to \nobtain an overall effect) can function as replication analyses, as effects that are only present \nin a minority of cohorts, or small cohorts that are not likely to survive multiple comparisons \ncorrections in the overall analysis. With an increase in statistical power, we can more \ndefinitively address major questions in the field, such as the existence and nature of sex-\nrelated differences after TBI, how different comparison group impact results (such as contact \nvs. non-contact controls in sports), how comorbid disorders interact with TBI to affect the \nbrain (such as PTSD or depression), and how differences in injury mechanisms may \nmanifest in the brain. Additionally, large sample sizes allow us to employ machine learning \napproaches to identify patient subgroups based on demographic, clinical, and imaging \nvariables, potentially with implications for prognostication and tailored treatment. ENIGMA \nworking groups are committed to publishing both positive and negative results, as \ntransparency is critical for advancing science and avoiding the \xe2\x80\x9cfile-drawer\xe2\x80\x9d problem \n(Duncan et al. 2018). In addition to an increase in statistical power, this collaboration leads \nto an increase in intellectual power by leveraging the collective expertise of a large network \nof scientists. Each researcher brings their own perspective, training background, experience, \nand interests, leading to a rich array of possible projects.\n\nBeyond the immediate goals, ENIGMA Brain Injury is meant to be hypothesis-generating \nfor future studies. Although large meta-analyses have increased power, this approach is not \nappropriate for all questions, so we consider it to be complementary to more in-depth \nindividual cohort studies. We hope that the findings that result from our efforts raise \nhypotheses that individual sites can interrogate in more depth within their cohorts. Our \nresults will hopefully serve as preliminary data to support future individual grant \nsubmissions by members of ENIGMA Brain Injury. Although the current ENIGMA Brain \nInjury activities will center largely on retrospective data analysis (with the exception of the \nIntimate Partner Violence group, Esopenko et al. 2020), this framework will lay the \nfoundation for future collaboration between teams. We hope that the exchange of ideas, \nmethodology, protocols, data, and analytic tools will lead to further attempts to harmonize \nprospective data collection and will synergize the development of new analytic pipelines and \ntechniques.\n\nBrain Imaging Behav. Author manuscript; available in PMC 2022 April 01.\n\n\x0cA\nu\n\nt\n\nh\no\nr\n \n\nM\na\nn\nu\ns\nc\nr\ni\np\n\nt\n\nA\nu\n\nt\n\nh\no\nr\n \n\nM\na\nn\nu\ns\nc\nr\ni\np\n\nt\n\nA\nu\n\nt\n\nh\no\nr\n \n\nM\na\nn\nu\ns\nc\nr\ni\np\n\nt\n\nA\nu\nt\nh\no\nr\n \n\nM\na\nn\nu\ns\nc\nr\ni\np\n\nt\n\nWilde et al.\n\nApproach\n\nPage 4\n\nENIGMA approaches team science in a unique way that differs both conceptually and \npractically from other consortium efforts. We recognize the hesitation that researchers may \nfeel in joining group science efforts as well as the logistical hurdles that can accompany data \nsharing, and we make every effort to \xe2\x80\x9cmeet groups where they are\xe2\x80\x9d so that researchers both \nfeel comfortable and invested. First, although there are advantages of an approach that favors \ncentralized data storage and analysis from the standpoint of quality control, this approach \nhas several notable disadvantages, particularly in a global forum. Regulatory mandates may \nprohibit or limit the transfer or sharing of some forms of health information and data, \nincluding neuroimaging and samples with genetic information. Additionally, centralized \nmodels may create logistical challenges for the institution where data reside, including \nissues related to the recurring personnel and infrastructure costs of storing and transferring \nlarge amounts of data. Centralized models also often create a situation where some \ninvestigators have more access to the data and resources than others, which may limit \nenthusiasm for contributing data. Centralized models may also lack incentives for sharing \nsince funding is often awarded to a primary site and publication credit may favor \ninvestigators at the primary sites. The ENIGMA approach circumvents each of these issues \nin an innovative approach.\n\nFirst, to accommodate data sharing issues, contribution of raw data is not required for \nparticipation; though processing support (at several levels) is available for groups that \nrequest it. Basic requirements for participation are the contribution of raw or summary \nmagnetic resonance imaging (MRI) data (e.g., T1-weighted imaging) and simple clinical and \ndemographic information. Additional imaging sequences, such as diffusion MRI (dMRI), \ntask-based and resting-state fMRI (tbfMRI and rsfMRI), magnetic resonance spectroscopy \n(MRS), and arterial spin labeling (ASL), and more detailed clinical and cognitive \ninformation allow for broader participation, but are not required. Sites will vary in both \nimaging acquisition parameters, but follow common, validated processing and analytic steps \nas part of the ENIGMA framework. Processing guidelines and scripts for subcortical \nvolume, cortical measures, and diffusion MRI measures can be found on the ENIGMA \nwebsite (http://enigma.ini.usc.edu/protocols/), and ENIGMA working groups are engaged in \ndeveloping and adapting pipelines for additional imaging modalities. There are numerous \nchallenges in combining and harmonizing distinct datasets, discussed in more detail in the \nLimitations and Challenges section below. Big data analytics is a rapidly advancing field, \nenabling more sophisticated modeling, but these approaches are suboptimal if the input data \nare not equivalent across sites.\n\nThe ENIGMA platform is intended to be flexible and expandable (see Figure 2). ENIGMA \nhas been successful in other areas of research in part because of the flexibility with data \nsharing: sharing raw data is never required, but a central site is available to provide \ncomputing support or analysis training, if necessary. Investigators may choose to process \nand analyze their data locally, sending only summary statistics to the lead investigator for an \nanalysis that has been proposed, thus maintaining the maximum amount of control over their \ndata. Sites that have a stronger emphasis on clinical expertise may opt to send raw imaging \ndata to a central site for a given investigation if they do not have the computational or \n\nBrain Imaging Behav. Author manuscript; available in PMC 2022 April 01.\n\n\x0cA\nu\n\nt\n\nh\no\nr\n \n\nM\na\nn\nu\ns\nc\nr\ni\np\n\nt\n\nA\nu\n\nt\n\nh\no\nr\n \n\nM\na\nn\nu\ns\nc\nr\ni\np\n\nt\n\nA\nu\n\nt\n\nh\no\nr\n \n\nM\na\nn\nu\ns\nc\nr\ni\np\n\nt\n\nA\nu\nt\nh\no\nr\n \n\nM\na\nn\nu\ns\nc\nr\ni\np\n\nt\n\nWilde et al.\n\nPage 5\n\npersonnel resources to process their own data locally. In all cases, analyses are opt in, \nmeaning that participation in one project does not assume participation in all projects. The \nresult is massively distributed computing, as processing is spread across many sites and \ninvestigators with different expertise. All working group members are encouraged to submit \nsecondary proposals to the group for projects they wish to lead. In this respect, ENIGMA is \na framework for collaboration, flexible to the interests and restrictions of each participating \nmember. While the initial analyses can be completed on existing datasets, the infrastructure \nis established for coordinating aspects of study protocols in new projects which have not yet \ncollected data. We encourage the participation of researchers at all levels, and aim to support \nearly career researchers through an expanded network of collaborators and access to larger \namounts of data than is commonly available.\n\nOf further note, there have been a number of recent consortia efforts in TBI and concussion \nneuroimaging. Most of these are multi-site studies with varying degrees of harmonization in \nstudy protocol. A number have been focused on brain injury in Military Service Members, \nincluding the Chronic Effects of Neurotrauma Consortium (CENC, Walker et al. 2016) and \nthe Long-term Impact of Military-relevant brain Injury Consortium (LIMBIC-CENC), the \nStudy of Brain Aging in Vietnam War Veterans (DoD ADNI, Weiner et al. 2014), the \nVietnam-Era Twin Study (VETSA, Kremen et al. 2013), and the Injury and Traumatic Stress \n(INTRuST, Lepage et al. 2018) study. Others have been focused on sports-related head \nimpacts, including the NCAA-DoD Grand Alliance Concussion Assessment, Research, and \nEducation (CARE, Broglio et al. 2017) consortium and the Big Ten-Ivy League Traumatic \nBrain Injury Research Collaboration (Putukian et al. 2019). Lastly, Translating Research and \nClinical Knowledge in TBI (TRACK-TBI, Yue et al. 2013) and Collaborative European \nNeuroTrauma Effectiveness Research in TBI (CENTER-TBI, Maas et al. 2015) are multi-\nsite studies recruiting from emergency departments (EDs), covering a wide range of injury \ntypes and severities. These large studies will significantly advance our understanding of \nfactors that influence outcome after TBI, but the large cost of collecting such large samples \nlimits participation of all interested investigators and requires dedicated funding \nopportunities. Some ENIGMA working groups are examining ways to work together to \nconverge data collection methods in studies that are just being designed or launched. \nHowever, historically, the main differences between these consortia and ENIGMA Brain \nInjury is the use of prospective vs. existing retrospective data harmonization and the degree \nof data centralization at a specific site. Each approach has benefits and drawbacks, and we \nbelieve there is a place for both in the field of TBI research. Studies that are prospectively \nharmonized obviously generate data that are more equivalent and simplify the harmonization \nsteps, but they require large amounts of funding, planning, and coordination across sites. \nWhile the ENIGMA model requires more effort to produce comparable data, using legacy \ndatasets represents a cost-effective way to gain further insight from completed projects. \nHarmonization can occur at multiple points during data processing, allowing multiple \ndatasets to be used in a unified approach. With the flexibility in data sharing, the ENIGMA \nmodel engages a larger group of researchers \xe2\x80\x93 data sharing regulations differ tremendously \nacross sites and across countries and clearly it is not possible to join a prospectively \nharmonized multi-site study after it has begun.\n\nBrain Imaging Behav. Author manuscript; available in PMC 2022 April 01.\n\n\x0cA\nu\n\nt\n\nh\no\nr\n \n\nM\na\nn\nu\ns\nc\nr\ni\np\n\nt\n\nA\nu\n\nt\n\nh\no\nr\n \n\nM\na\nn\nu\ns\nc\nr\ni\np\n\nt\n\nA\nu\n\nt\n\nh\no\nr\n \n\nM\na\nn\nu\ns\nc\nr\ni\np\n\nt\n\nA\nu\nt\nh\no\nr\n \n\nM\na\nn\nu\ns\nc\nr\ni\np\n\nt\n\nWilde et al.\n\nPage 6\n\nSuccesses in Other ENIGMA Groups\n\nPotential\n\nAmong ENIGMA working groups, the Brain Injury group is relatively young, allowing it to \nbenefit from the experiences of more established working groups. We highlight three \nworking groups here that have some comorbidity with TBI. The Major Depressive Disorder \n(MDD) was one of the first disease groups to be formed in 2014. To date, they have \npublished a large number of papers across a variety of modalities examining both broad \ndisease effects and more specific symptoms (Frodl et al. 2017; Kelly et al. 2017; Renter\xc3\xada et \nal. 2017; Schmaal et al. 2016, 2017; Tozzi et al. 2019). Additionally, this group has led the \ncreation of related focus groups, such as the Suicidal Thoughts and Behaviors (STB) \nworking group. While the MDD group was supported by the initial NIH BD2K Center of \nExcellence grant along with 6 other psychiatric working groups, this initial phase of funding \nhas been completed. One group that has been successful in receiving grant support is the \nPost-Traumatic Stress Disorder (PTSD) working group. The PTSD working group has \nconsiderable overlap in membership with the Military Brain Injury subgroup and has also \npublished papers on subcortical volume (Logue et al. 2017) and white matter microstructure \n(Dennis et al. 2019). The ENIGMA Addiction working group has similarly received grant \nsupport, and recently published a paper of 3,240 individuals examining multiple substances. \nThey found that alcohol abuse was associated with the most substantial alterations in cortical \nmeasures (Mackey et al. 2019). Depression, PTSD, and substance use disorders (SUDs) are \nall potentially comorbid with TBI, as either pre-injury and/or outcome so interfacing with \nthese groups will support important cross-disorder analyses.\n\nENIGMA has the potential to address many of the challenges listed above. There is \ntremendous heterogeneity in TBI and outcome is likely influenced by a large range of \ndemographic and clinical variables. When variability is high, large samples are necessary to \ndetect reliable effects. Through the increased sample size ENIGMA facilitates, there is \ngreater power to detect abnormalities that are consistent across patients, and also to perhaps \nidentify subgroups with distinct clinical prognoses. As discussed in more detail in the \nfollowing papers of this issue written by the leaders of each subgroup, there are a large \nnumber of potentially confounding variables when researching TBI. For example, with \nregard to the complex intersection of TBI and psychiatric disorders, TBI has been cited as \nboth a risk factor for subsequent development of post-injury psychopathology or \ndevelopmental disorder (e.g., ADHD), but a history of pre-existing psychopathology may \nalso increase the risk of sustaining a head injury. Therefore, comorbid disorders must be \ncarefully considered, as mentioned above. Moreover, some comorbid disorders are more \nprevalent in certain subgroups (e.g., ADHD in children, PTSD in Military Service Members \nand Intimate Partner Violence), while others are generally comorbid with TBI of any \npopulation or severity (e.g., MDD). Larger sample sizes made possible by ENIGMA allow \nconsideration of these confounds, and investigators will collaborate with existing ENIGMA \nworking groups dedicated to these potentially comorbid disorders. Through collaboration \nwith these groups, we endeavor to identify neural phenotypes that are distinct and also \nidentify common features that exist across disorders.\n\nBrain Imaging Behav. Author manuscript; available in PMC 2022 April 01.\n\n\x0cA\nu\n\nt\n\nh\no\nr\n \n\nM\na\nn\nu\ns\nc\nr\ni\np\n\nt\n\nA\nu\n\nt\n\nh\no\nr\n \n\nM\na\nn\nu\ns\nc\nr\ni\np\n\nt\n\nA\nu\n\nt\n\nh\no\nr\n \n\nM\na\nn\nu\ns\nc\nr\ni\np\n\nt\n\nA\nu\nt\nh\no\nr\n \n\nM\na\nn\nu\ns\nc\nr\ni\np\n\nt\n\nWilde et al.\n\nPage 7\n\nLimitations and Challenges\n\nA central aim of the ENIGMA Brain Injury group is identifying factors that affect outcome. \nSome of these may be variables that cannot be modified, such as gender/sex, age, or genetic \nvariability, but could warrant more targeted treatment. Others might be modifiable, and \namenable to treatment or intervention. There may be subgroups of individuals within the \nlarger patient group that manifest different patterns of dysfunction. The use of \xe2\x80\x9cbig data\xe2\x80\x9d \nmay thus allow us to more accurately predict future recovery or neurodegeneration. Another \ncentral aim of the ENIGMA Brain Injury group is to develop new image processing \nworkflows that are appropriate for brain-injured populations or specifically aimed at \ncharacterizing injury-related pathology. With individual lesion maps, we can optimize \nexisting image processing pipelines and directly examine associations between lesion \nlocation and functional disruption. We aim to work with others to develop pipelines for \nautomated detection of white matter hyperintensities. Additionally, there are a number of \napproaches that have been well studied in individual cohorts, including multimodal \napproaches like connectomics, which we plan to extend for use across multiple cohorts.\n\nOne of the key challenges of multi-site efforts is adequate data harmonization. Large sample \nsizes will not overcome uncharacterized heterogeneity between datasets, and there is a risk \nof a \xe2\x80\x9cgarbage in, garbage out\xe2\x80\x9d outcome if appropriate harmonization and quality control \nsteps are not taken. TBI manifests in different forms across severity, acuity, and age at injury, \nhighlighting the importance of defining the patient population. Harmonization crosses \nmultiple domains, including imaging, neuropsychological assessment, clinical outcomes, \nand blood biomarkers. Combining imaging data is first challenged by different naming \nconventions and data organization, which can be helped by using BIDS (Brain Imaging Data \nStructure) standards (Gorgolewski et al. 2016). As ENIGMA mainly works with data that \nhave already been collected, harmonization is completed post hoc as a data processing step. \nFor new data collection, we have the opportunity to harmonize aspects of different of \nprotocols. Of note, for structural imaging, T1-weighted MRI is more straightforward. While \nprotocols do differ across manufacturers, a voxel-size of 1 mm3 is standard, making volume \ncalculations less variable. For diffusion MRI (dMRI), there is considerable variability in \nangular resolution, diffusion weighting, and voxel size. Even two scanners from the same \nmanufacturer running the same protocols will yield slightly different average diffusivity \nmeasures, making it critical that dMRI analyses are meta-analyses, not mega-analyses, \nunless harmonization like ComBat or similar algorithms are applied (Cetin-Karayumak et al. \n2019; Cetin Karayumak et al. 2019; Johnson et al. 2007). One benefit of this variability, \nhowever, is that it increases the generalizability of results. We can have more confidence in \neffects that are detected at both 12 direction dMRI and 128 direction dMRI. Additionally, \nthere are current efforts in the ENIGMA consortium to develop harmonized methods for \nfunctional MRI and resting-state fMRI (Adhikari et al. 2018; Adhikari et al. 2018; Veer et al. \n2019). To address this challenge, the ENIGMA Brain Injury group will experiment with \nvarious harmonization approaches mentioned above, taking advantage of the multi-site \nprojects already included that have more formal harmonization procedures as part of the \nstudy protocol. This work will yield further insight into factors that impact the within \nsequence imaging heterogeneity.\n\nBrain Imaging Behav. Author manuscript; available in PMC 2022 April 01.\n\n\x0cA\nu\n\nt\n\nh\no\nr\n \n\nM\na\nn\nu\ns\nc\nr\ni\np\n\nt\n\nA\nu\n\nt\n\nh\no\nr\n \n\nM\na\nn\nu\ns\nc\nr\ni\np\n\nt\n\nA\nu\n\nt\n\nh\no\nr\n \n\nM\na\nn\nu\ns\nc\nr\ni\np\n\nt\n\nA\nu\nt\nh\no\nr\n \n\nM\na\nn\nu\ns\nc\nr\ni\np\n\nt\n\nWilde et al.\n\nPage 8\n\nAn additional challenge lies in harmonizing cognitive and clinical measures. Although the \nintroduction of International Common Data Elements for TBI has facilitated use of \nrecommended measures within a variety of outcome domains, considerable variability still \nexists as appropriate measures differ between populations of interest (e.g., athletes vs \nmilitary), age range (measures developed and normed for young children differ from those \nused in older children or adults), acuity (symptoms and outcome for acute vary from those in \nchronic phases of recovery) and severity (outcome domains and measures most relevant for \nconcussion differ from those used in more severe TBI). Although neuropsychological testing \nor other outcome assessment is common in many studies, the specific assessments used \nnecessarily vary widely. Several options exist for harmonizing these data or developing \ncommon comparable cognitive endpoints that could be used to further the research. The \nmost conservative approach involves identification of the most commonly used scales across \nstudies and focus analyses around domains and cohorts where common data was collected. \nAnother approach would be to convert scores within a given outcome domain to \nstandardized T-scores based on population means and standard deviations. This allows for \nmore variability in the specific measures that can be included, but care must be applied in \nensuring that the cognitive constructs are indeed consistent. A third approach involves \ncalculating a cognitive composition score created by assigning weighted scores based on the \ndegree to which an individual test score deviates from normative expectations (increased \ndeviation, increased weight). This method is expected to improve sensitivity by creating \nfiner gradations across patients and increase the ceiling for improved detection even in \nmTBI (Silverberg et al. 2017). Finally, in addition to working together to harmonize existing \noutcome data and to address novel imaging analytic pipelines, the development, testing, and \noptimization of innovative cognitive and neurobehavioral outcome measures that are specific \nto the assessment of mTBI, concussion, and repetitive head hits may be a goal of the \nworking groups, where appropriate.\n\nThough not discussed directly in this special issue, the recently established Cognitive \nEndpoints group has begun piloting additional statistical methods that are intended to \nquantify the disparity in cognitive measures administered between cohorts. Once the \ndisparity is known, the goal would be to apply a set of flexible statistical methods designed \nto minimize the disparity (i.e., item response theory, machine learning) in measures \nadministered to produce a set of co-calibrated and validated measures that can be used in \nanalyses with the harmonized imaging data. Following these steps (see Figure 3), seemingly \ndisparate cognitive data acquired independently across cohorts can move from a low value \nstate with regards to big data approaches toward a more useful high value set of variables \nthat can examined in an aggregated manner. Data curated and processed in this manner will \nbe especially critical when defining the relevant brain-behavior relationships or identifying \nany unique behavioral phenotypes that exist across or within TBI cohorts. In addition, this \npromising effort could be applied more broadly to other ENIGMA working data to allow for \nthe exploration of behavioral/functional relationships between imaging findings and these \ncommon cognitive endpoints.\n\nThe use of fluid biomarkers and genetics also brings challenges. As with imaging, important \nconsiderations surround differences in the collection, processing, and analysis of biofluid \nsamples that will necessarily vary by site. An optimal approach includes the use of \n\nBrain Imaging Behav. Author manuscript; available in PMC 2022 April 01.\n\n\x0cA\nu\n\nt\n\nh\no\nr\n \n\nM\na\nn\nu\ns\nc\nr\ni\np\n\nt\n\nA\nu\n\nt\n\nh\no\nr\n \n\nM\na\nn\nu\ns\nc\nr\ni\np\n\nt\n\nA\nu\n\nt\n\nh\no\nr\n \n\nM\na\nn\nu\ns\nc\nr\ni\np\n\nt\n\nA\nu\nt\nh\no\nr\n \n\nM\na\nn\nu\ns\nc\nr\ni\np\n\nt\n\nWilde et al.\n\nPage 9\n\nstandardized procedures on the actual samples using a limited number of \xe2\x80\x9cbatches\xe2\x80\x9d, but \nmeta-analyses are also possible with these data (Berger et al. 2012; Manley et al. 2010).\n\nWhile advances in technology have supported the growing movement towards data sharing \nand open science, there are important legal, ethical, and regulatory issues with global data \nsharing (Palk et al. 2019). All of these considerations are aimed at protecting participant \nprivacy and controlling data use. Anonymization is often required, although what is \nconsidered \xe2\x80\x9canonymized\xe2\x80\x9d differs (Sariyar et al. 2015), and this can include both meta-data in \na file header and physical features from MR images (Milchenko and Marcus 2013). While \nsome institutions and countries allow for data sharing to be considered under a \xe2\x80\x9cbroad \nconsent\xe2\x80\x9d, others require explicit statements regarding future potential uses of individual data. \nThe recently enacted General Data Protection Regulation (GDPR) imposed restrictions on \nsharing personal data within and outside the European Union and requires explicit consent \nfrom an individual to share data outside of a few lawful purposes (Voigt and Von dem \nBussche 2017). Material Transfer Agreements (MTAs) and Data Use Agreements (DUAs) \nare required by some institutions, and may be applied more stringently in certain populations \n(e.g., U.S. Veterans and Active Duty Service Members, children, etc.). A Uniform \nBiological Material Transfer Agreement (UBMTA) can expedite transfer between \nparticipating universities (Carr et al. 2017). Sharing genomic data brings additional \nconcerns, as release of this information could have broad legal, medical, and other \nconsequences for the individual. The NIH mandates that genomic data be submitted to \ndbGaP (database of Genotypes and Phenotypes) after identifying information is removed \n(names, dates, locations)(Paltoo et al. 2014). Controlled access to genomic data is then \ngranted to researchers for a specific project. Sharing summary level data within ENIGMA, \nas opposed to raw imaging data, addresses many of these concerns, although some \ninstitutions restrict this level of sharing as well. In these cases, scripts for site-level statistical \nanalysis can be shared with summary statistics returned to the primary site. COINSTAC \n(Collaborative Informatics and Neuroimaging Suite Toolkit for Anonymous Computation) is \nweb-based framework for executing harmonized processing and analysis across multiple \nsites that allows the data to stay local, negating the need for explicit sharing consent or \nMTA/DUAs (Plis et al. 2016). Encouragingly, other consortia have reported that \nharmonization was more of a challenge than permissions (Budin-Lj\xc3\xb8sne et al. 2014). The \nactivities of ENIGMA over the last decade have shown that while there are numerous \nhurdles to data-sharing and international collaboration, in most cases there are solutions that \nsatisfy the ethical and legal requirements while facilitating group science (Thompson et al. \n2020). ENIGMA is active in efforts to enhance understanding of regulations in several \nregions of the world around data sharing, and to formulate solutions to facilitate global \ncollaboration.\n\nProgress and Deliverables\n\nWe will continuously assess our progress to ensure that ENIGMA Brain Injury is moving the \nfield forward and supporting the researchers involved. Our immediate goals are to establish \nand expand our network of interested researchers along with potential new datasets to \ninclude, and identify funding mechanisms to support the effort. Intermediate goals include \ndeveloping and testing new pipelines for processing neuroimaging data that consider \n\nBrain Imaging Behav. Author manuscript; available in PMC 2022 April 01.\n\n\x0cA\nu\n\nt\n\nh\no\nr\n \n\nM\na\nn\nu\ns\nc\nr\ni\np\n\nt\n\nA\nu\n\nt\n\nh\no\nr\n \n\nM\na\nn\nu\ns\nc\nr\ni\np\n\nt\n\nA\nu\n\nt\n\nh\no\nr\n \n\nM\na\nn\nu\ns\nc\nr\ni\np\n\nt\n\nA\nu\nt\nh\no\nr\n \n\nM\na\nn\nu\ns\nc\nr\ni\np\n\nt\n\nWilde et al.\n\nPage 10\n\nstructural deformations, improve classification of neuropathology such as white matter \nhyperintensities, allow for longitudinal modeling of change after injury, and are stable and \nuseful across sites. Continuous goals include supporting the advancement of junior \ninvestigators through the opportunity to propose and lead new analyses and to provide a \nforum for researchers to discuss controversies and open questions in the field. In the long-\nterm, we hope that this effort will yield hypotheses that researchers can interrogate in greater \ndepth in their individual cohorts and lead to new collaborations among participants as they \nplan future studies with a goal of improving comparability in a mutually beneficial manner. \nThe deliverables for the ENIGMA Brain Injury group include publications, grants, new \npipeline development, establishment of best practices for combining data and for processing \nTBI neuroimaging data (particularly with regard to lesions), and datasets that have been \ncurated and harmonized. The ENIGMA Brain Injury group has quarterly conference calls \nalong with regular in-person meetings coinciding with relevant conferences to inform \ncollaborators of progress and make plans moving forward. Each subgroup has monthly or bi-\nmonthly conference calls to discuss specific analyses.\n\nThe ENIGMA Brain Injury group aims to bring together researchers from around the world \nwith the shared goal of furthering our understanding of the impact of brain injury and factors \nthat may influence outcome. Building off of the framework of the extremely productive \nbroader ENIGMA consortium, we are optimistic that this effort will yield new information \nand will help answer open questions in the field. We also expect our research to introduce \nnew questions and hypotheses that individual cohorts can investigate in greater detail and \nwill hopefully inspire new data collection. In contrast to other efforts, raw data are not \ncentralized, and contributing sites maintain ownership and control of their data, with all \nanalyses being opt-in. All members are welcome to submit secondary proposals, benefitting \nfrom an expanded collaborative network and larger sample size. Researchers interested in \njoining or learning more about the ENIGMA Brain Injury group are encouraged to read the \ncompanion articles in this special issue and contact the authors.\n\nConclusions\n\nAcknowledgements\n\nReferences\n\nWe acknowledge funding sources including K99 NS096116 to Dr. Dennis and PT12051 and I01 RX002174 \n(CENC) to Drs. Wilde, Tate, and Dennis. The authors wish to acknowledge the leadership of Dr. Paul Thompson \n(ENIGMA PI) as well as the leadership of Drs. Frank Hillary, Alexander Olsen, Inga Koerte, David Baron, \nAlexander Lin, Brenda Bartnik-Olson, Karen Caeyenberghs, Carrie Esopenko, and Neda Jahanshad, as well as \nENIGMA support personnel and all working group members and contributors. We wish to acknowledge the \ncontribution of Eamonn Kennedy in the conceptualization and creation of the figures. We also wish to thank all \nparticipants who have contributed to this research. Finally, we extend our gratitude to Drs. Erin D. Bigler and \nMartha E. Shenton for their thoughtful review of this manuscript and their enduring mentorship and support.\n\nAdhikari BM, Jahanshad N, Shukla D, Glahn DC, Blangero J, Reynolds RC, et al. (2018). Heritability \n\nestimates on resting state fMRI data using ENIGMA analysis pipeline. Pacific Symposium on \nBiocomputing. Pacific Symposium on Biocomputing, 23, 307\xe2\x80\x93318. [PubMed: 29218892] \n\nAdhikari BM, Jahanshad N, Shukla D, Turner J, Grotegerd D, Dannlowski U, et al. (2018). A resting \n\nstate fMRI analysis pipeline for pooling inference across diverse cohorts: an ENIGMA rs-fMRI \nprotocol. Brain imaging and behavior. doi:10.1007/s11682-018-9941-x\n\nBrain Imaging Behav. Author manuscript; available in PMC 2022 April 01.\n\n\x0cA\nu\n\nt\n\nh\no\nr\n \n\nM\na\nn\nu\ns\nc\nr\ni\np\n\nt\n\nA\nu\n\nt\n\nh\no\nr\n \n\nM\na\nn\nu\ns\nc\nr\ni\np\n\nt\n\nA\nu\n\nt\n\nh\no\nr\n \n\nM\na\nn\nu\ns\nc\nr\ni\np\n\nt\n\nA\nu\nt\nh\no\nr\n \n\nM\na\nn\nu\ns\nc\nr\ni\np\n\nt\n\nWilde et al.\n\nPage 11\n\nBartnik-Olson B, Alger J, Babikian T, Harris AD, Holshouser B, Kirov II, et al. (2020). The Clinical \n\nUtility of Magnetic Resonance Spectroscopy in Traumatic Brain Injury: Recommendations from the \nENIGMA MRS Working Group. Brain imaging and behavior, In Press (Special Issue).\n\nBerger RP, Beers SR, Papa L, Bell M, & Pediatric TBI CDE Biospecimens and Biomarkers \n\nWorkgroup. (2012). Common data elements for pediatric traumatic brain injury: recommendations \nfrom the biospecimens and biomarkers workgroup. Journal of neurotrauma, 29(4), 672\xe2\x80\x93677. \n[PubMed: 22106839] \n\nBroglio SP, McCrea M, McAllister T, Harezlak J, Katz B, Hack D, et al. (2017). A National study on \n\nthe effects of concussion in collegiate athletes and US military service academy members: the \nNCAA--DoD concussion assessment, research and education (CARE) consortium structure and \nmethods. Sports medicine, 47(7), 1437\xe2\x80\x931451. [PubMed: 28281095] \n\nBudin-Lj\xc3\xb8sne I, Isaeva J, Knoppers BM, Tass\xc3\xa9 AM, Shen H-Y, McCarthy MI, et al. (2014). Data \n\nsharing in large research consortia: experiences and recommendations from ENGAGE. European \njournal of human genetics: EJHG, 22(3), 317\xe2\x80\x93321. [PubMed: 23778872] \n\nCarr N, Shin I, & Maier S (2017, 3 1). Material Transfer and Data Use Agreements. Journal of Clinical \n\nResearch Best Practices. https://papers.ssrn.com/abstract=3195015. Accessed 11 September 2019\n\nCetin Karayumak S, Bouix S, Ning L, James A, Crow T, Shenton M, et al. (2019). Retrospective \nharmonization of multi-site diffusion MRI data acquired with different acquisition parameters. \nNeuroImage, 184, 180\xe2\x80\x93200. [PubMed: 30205206] \n\nCetin-Karayumak S, Di Biase MA, Chunga N, Reid B, Somes N, Lyall AE, et al. (2019). White matter \n\nabnormalities across the lifespan of schizophrenia: a harmonized multi-site diffusion MRI study. \nMolecular psychiatry. doi:10.1038/s41380-019-0509-y\n\nDennis EL, Caeyenberghs K, Asarnow RF, Babikian T, Bartnik-Olson B, Bigler ED, et al. (2020). \nBrain Imaging in Young Brain-Injured Patients: A Coordinated Effort Towards Individualized \nPredictors from the ENIGMA Pediatric msTBI Group. Brain imaging and behavior, In Press \n(Special Issue).\n\nDennis EL, Disner SG, Fani N, Salminen LE, Logue M, Clarke EK, et al. (2019, 6 20). Altered White \n\nMatter Microstructural Organization in Post-Traumatic Stress Disorder across 3,049 Adults: \nResults from the PGC-ENIGMA PTSD Consortium. bioRxiv. doi:10.1101/677153\n\nDuncan D, Barisano G, Cabeen R, Sepehrband F, Garner R, Braimah A, et al. (2018). Analytic Tools \nfor Post-traumatic Epileptogenesis Biomarker Search in Multimodal Dataset of an Animal Model \nand Human Patients. Frontiers in neuroinformatics, 12, 86. [PubMed: 30618695] \n\nEsopenko C, Meyer J, Wilde EA, Marshall A, Tate DF, Lin A, et al. (2020). Harmonization of \nMeasures to Assess IPV-Related Head Trauma: Recommendations from the ENIGMA IPV \nWorking Group. Brain imaging and behavior, In Press (Special Issue).\n\nFrodl T, Janowitz D, Schmaal L, Tozzi L, Dobrowolny H, Stein DJ, et al. (2017). Childhood adversity \nimpacts on brain subcortical structures relevant to depression. Journal of psychiatric research, 86, \n58\xe2\x80\x9365. [PubMed: 27918926] \n\nGorgolewski KJ, Auer T, Calhoun VD, Craddock RC, Das S, Duff EP, et al. (2016). The brain imaging \n\ndata structure, a format for organizing and describing outputs of neuroimaging experiments. \nScientific data, 3, 160044. [PubMed: 27326542] \n\nJohnson WE, Li C, & Rabinovic A (2007). Adjusting batch effects in microarray expression data using \n\nempirical Bayes methods. Biostatistics, 8(1), 118\xe2\x80\x93127. [PubMed: 16632515] \n\nKelly S, van Velzen L, Veltman D, Thompson P, Jahanshad N, Schmaal L, & Group, Group. (2017). \n\n941. White Matter Microstructural Differences in Major Depression: Meta-Analytic Findings from \nEnigma-MDD DTI. Biological psychiatry, 81(10), S381.\n\nKoerte IK, Dennis EL, Bazarian JJ, Bigler ED, Buckley T, Choe M, et al. (2020). Neuroimaging of \n\nSport-Related Brain Injury: Challenges and Recommendations from the ENIGMA Sports-Related \nBrain Injury group. Brain imaging and behavior, In Press (Special Issue).\n\nKremen WS, Franz CE, & Lyons MJ (2013). VETSA: the Vietnam Era Twin Study of Aging. Twin \nresearch and human genetics: the official journal of the International Society for Twin Studies, \n16(1), 399\xe2\x80\x93402. [PubMed: 23110957] \n\nLepage C, de Pierrefeu A, Koerte IK, Coleman MJ, Pasternak O, Grant G, et al. (2018). White matter \n\nabnormalities in mild traumatic brain injury with and without post-traumatic stress disorder: a \n\nBrain Imaging Behav. Author manuscript; available in PMC 2022 April 01.\n\n\x0cA\nu\n\nt\n\nh\no\nr\n \n\nM\na\nn\nu\ns\nc\nr\ni\np\n\nt\n\nA\nu\n\nt\n\nh\no\nr\n \n\nM\na\nn\nu\ns\nc\nr\ni\np\n\nt\n\nA\nu\n\nt\n\nh\no\nr\n \n\nM\na\nn\nu\ns\nc\nr\ni\np\n\nt\n\nA\nu\nt\nh\no\nr\n \n\nM\na\nn\nu\ns\nc\nr\ni\np\n\nt\n\nWilde et al.\n\nPage 12\n\nsubject-specific diffusion tensor imaging study. Brain imaging and behavior, 12(3), 870\xe2\x80\x93881. \n[PubMed: 28676987] \n\nLogue MW, van Rooij SJH, Dennis EL, Davis SL, Hayes JP, Stevens JS, et al. (2017). Smaller \n\nhippocampal volume in posttraumatic stress disorder: a multi-site ENIGMA-PGC study. \nBiological psychiatry.\n\nMaas AIR, Menon DK, Steyerberg EW, Citerio G, Lecky F, Manley GT, et al. (2015). Collaborative \n\nEuropean NeuroTrauma Effectiveness Research in Traumatic Brain Injury (CENTER-TBI): a \nprospective longitudinal observational study. Neurosurgery, 76(1), 67\xe2\x80\x9380. [PubMed: 25525693] \nMackey S, Allgaier N, Chaarani B, Spechler P, Orr C, Bunn J, et al. (2019). Mega-Analysis of Gray \n\nMatter Volume in Substance Dependence: General and Substance-Specific Regional Effects. The \nAmerican journal of psychiatry, 176(2), 119\xe2\x80\x93128. [PubMed: 30336705] \n\nManley GT, Diaz-Arrastia R, Brophy M, Engel D, Goodman C, Gwinn K, et al. (2010). Common data \n\nelements for traumatic brain injury: recommendations from the biospecimens and biomarkers \nworking group. Archives of physical medicine and rehabilitation, 91(11), 1667\xe2\x80\x931672. [PubMed: \n21044710] \n\nMilchenko M, & Marcus D (2013). Obscuring surface anatomy in volumetric imaging data. \n\nNeuroinformatics, 11(1), 65\xe2\x80\x9375. [PubMed: 22968671] \n\nOlsen A, Babikian T, Bigler E, Caeyenberghs K, Conde V, Dams-O\xe2\x80\x99Connor K, et al. (2020). Toward a \nGlobal and Open Science for Imaging Brain Trauma: the ENIGMA Adult msTBI Working Group. \nBrain imaging and behavior, In Press (Special Issue).\n\nPalk A, Illes J, Thompson PM, & Stein DJ (2019). Ethical Issues in Global Imaging Genetics \n\nCollaborations. NeuroImage, In Review.\n\nPaltoo DN, Rodriguez LL, Feolo M, Gillanders E, Ramos EM, Rutter JL, et al. (2014). Data use under \n\nthe NIH GWAS data sharing policy and future directions. Nature genetics, 46(9), 934\xe2\x80\x93938. \n[PubMed: 25162809] \n\nPlis SM, Sarwate AD, Wood D, Dieringer C, Landis D, Reed C, et al. (2016). COINSTAC: A Privacy \nEnabled Model and Prototype for Leveraging and Processing Decentralized Brain Imaging Data. \nFrontiers in Neuroscience. doi:10.3389/fnins.2016.00365\n\nPutukian M, D\xe2\x80\x99Alonzo BA, Campbell-McGovern CS, & Wiebe DJ (2019). The Ivy League-Big Ten \n\nEpidemiology of Concussion Study: A Report on Methods and First Findings. The American \njournal of sports medicine, 47(5), 1236\xe2\x80\x931247. [PubMed: 30943078] \n\nRenter\xc3\xada ME, Schmaal L, Hibar DP, Couvy-Duchesne B, Strike LT, Mills NT, et al. (2017). Subcortical \n\nbrain structure and suicidal behaviour in major depressive disorder: a meta-analysis from the \nENIGMA-MDD working group. Translational psychiatry, 7(5), e1116. [PubMed: 28463239] \n\nSariyar M, Schluender I, Smee C, & Suhr S (2015). Sharing and Reuse of Sensitive Data and Samples: \n\nSupporting Researchers in Identifying Ethical and Legal Requirements. Biopreservation and \nbiobanking, 13(4), 263\xe2\x80\x93270. [PubMed: 26186169] \n\nSchmaal L, Hibar DP, S\xc3\xa4mann PG, Hall GB, Baune BT, Jahanshad N, et al. (2017). Cortical \n\nabnormalities in adults and adolescents with major depression based on brain scans from 20 \ncohorts worldwide in the ENIGMA Major Depressive Disorder Working Group. Molecular \npsychiatry, 22(6), 900\xe2\x80\x93909. [PubMed: 27137745] \n\nSchmaal L, Veltman DJ, van Erp TGM, S\xc3\xa4mann PG, Frodl T, Jahanshad N, et al. (2016). Subcortical \n\nbrain alterations in major depressive disorder: findings from the ENIGMA Major Depressive \nDisorder working group. Molecular psychiatry, 21(6), 806\xe2\x80\x93812. [PubMed: 26122586] \n\nSilverberg ND, Crane PK, Dams-O\xe2\x80\x99Connor K, Holdnack J, Ivins BJ, Lange RT, et al. (2017). \n\nDeveloping a Cognition Endpoint for Traumatic Brain Injury Clinical Trials. Journal of \nneurotrauma, 34(2), 363\xe2\x80\x93371. [PubMed: 27188248] \n\nTate DF, Dennis EL, Adams JT, Adamson MM, Belanger HG, Bigler ED, et al. (2020). Coordinating \nGlobal Multi-Site Studies of Military TBI: Potential, Challenges, and Harmonization Guidelines \nfrom the ENIGMA Military Brain Injury Group. Brain imaging and behavior, In Press (Special \nIssue).\n\nThompson P, Jahanshad N, Ching CRK, Salminen L, Thomopoulos SI, Bright J, et al. (2020). \n\nENIGMA and Global Neuroscience: A Decade of Large-Scale Studies of the Brain in Health and \n\nBrain Imaging Behav. Author manuscript; available in PMC 2022 April 01.\n\n\x0cA\nu\n\nt\n\nh\no\nr\n \n\nM\na\nn\nu\ns\nc\nr\ni\np\n\nt\n\nA\nu\n\nt\n\nh\no\nr\n \n\nM\na\nn\nu\ns\nc\nr\ni\np\n\nt\n\nA\nu\n\nt\n\nh\no\nr\n \n\nM\na\nn\nu\ns\nc\nr\ni\np\n\nt\n\nA\nu\nt\nh\no\nr\n \n\nM\na\nn\nu\ns\nc\nr\ni\np\n\nt\n\nWilde et al.\n\nPage 13\n\nDisease across more than 40 Countries. Translational Psychiatry, 10(1), 1\xe2\x80\x9328 [PubMed: \n32066695] \n\nTozzi L, Garczarek L, Janowitz D, Stein DJ, Wittfeld K, Dobrowolny H, et al. (2019). Interactive \n\nimpact of childhood maltreatment, depression, and age on cortical brain structure: mega-analytic \nfindings from a large multi-site cohort. Psychological medicine, 1\xe2\x80\x9312.\n\nVeer I, Waller L, Lett T, Erk S, & Walter H (2019). ENIGMA task-based fMRI: A workgroup studying \n\nthe genetic basis of task-evoked brain activity. Presented at the Organization for Human Brain \nMapping.\n\nVoigt P, & Von dem Bussche A (2017). The eu general data protection regulation (gdpr). A Practical \n\nGuide, 1st Ed., Cham: Springer International Publishing. https://link.springer.com/content/pdf/\n10.1007/978-3-319-57959-7.pdf\n\nWalker WC, Carne W, Franke LM, Nolen T, Dikmen SD, Cifu DX, et al. (2016). The Chronic Effects \nof Neurotrauma Consortium (CENC) multi-centre observational study: Description of study and \ncharacteristics of early participants. Brain injury: [BI], 30(12), 1469\xe2\x80\x931480.\n\nWeiner MW, Veitch DP, Hayes J, Neylan T, Grafman J, Aisen PS, et al. (2014). Effects of traumatic \n\nbrain injury and posttraumatic stress disorder on Alzheimer\xe2\x80\x99s disease in veterans, using the \nAlzheimer's Disease Neuroimaging Initiative. Alzheimer\xe2\x80\x99s & dementia: the journal of the \nAlzheimer's Association, 10(3 Suppl), S226\xe2\x80\x9335.\n\nYue JK, Vassar MJ, Lingsma HF, Cooper SR, Okonkwo DO, Valadka AB, et al. (2013). Transforming \nresearch and clinical knowledge in traumatic brain injury pilot: multicenter implementation of the \ncommon data elements for traumatic brain injury. Journal of neurotrauma, 30(22), 1831\xe2\x80\x931844. \n[PubMed: 23815563] \n\nBrain Imaging Behav. Author manuscript; available in PMC 2022 April 01.\n\n\x0cA\nu\n\nt\n\nh\no\nr\n \n\nM\na\nn\nu\ns\nc\nr\ni\np\n\nt\n\nA\nu\n\nt\n\nh\no\nr\n \n\nM\na\nn\nu\ns\nc\nr\ni\np\n\nt\n\nA\nu\n\nt\n\nh\no\nr\n \n\nM\na\nn\nu\ns\nc\nr\ni\np\n\nt\n\nA\nu\nt\nh\no\nr\n \n\nM\na\nn\nu\ns\nc\nr\ni\np\n\nt\n\nWilde et al.\n\nPage 14\n\nFigure 1. \nOrganization of the ENIGMA consortium and the ENIGMA Brain Injury working group. \nMap in top right shows current sites across ENIGMA, map in bottom right shows current \nsites in the ENIGMA Brain Injury working group. TBI=traumatic brain injury, \ndMRI=diffusion magnetic resonance spectroscopy, EEG=electroencephalography, \nrsfMRI=resting state functional MRI, tbfMRI=task based fMRI, GWAS=genome-wide \nassociation study, CNV=copy number variation, MDD=major depressive disorder, \nAD=anxiety disorder, PTSD=post-traumatic stress disorder, FTD=frontotemporal dementia, \nHIV=human immunodeficiency virus, OCD=obsessive compulsive disorder, \nADHD=attention-deficit/hyperactivity disorder, 22q DS=22q11.2 deletion syndrome. \nAdapted from Thompson et al., 2020.\n\nBrain Imaging Behav. Author manuscript; available in PMC 2022 April 01.\n\n\x0cA\nu\n\nt\n\nh\no\nr\n \n\nM\na\nn\nu\ns\nc\nr\ni\np\n\nt\n\nA\nu\n\nt\n\nh\no\nr\n \n\nM\na\nn\nu\ns\nc\nr\ni\np\n\nt\n\nA\nu\n\nt\n\nh\no\nr\n \n\nM\na\nn\nu\ns\nc\nr\ni\np\n\nt\n\nA\nu\nt\nh\no\nr\n \n\nM\na\nn\nu\ns\nc\nr\ni\np\n\nt\n\nWilde et al.\n\nPage 15\n\nFigure 2. \nSchematic showing the approach and goals of the ENIGMA Brain Injury working group.\n\nBrain Imaging Behav. Author manuscript; available in PMC 2022 April 01.\n\n\x0cA\nu\n\nt\n\nh\no\nr\n \n\nM\na\nn\nu\ns\nc\nr\ni\np\n\nt\n\nA\nu\n\nt\n\nh\no\nr\n \n\nM\na\nn\nu\ns\nc\nr\ni\np\n\nt\n\nA\nu\n\nt\n\nh\no\nr\n \n\nM\na\nn\nu\ns\nc\nr\ni\np\n\nt\n\nA\nu\nt\nh\no\nr\n \n\nM\na\nn\nu\ns\nc\nr\ni\np\n\nt\n\nWilde et al.\n\nPage 16\n\nFigure 3. \nThis illustration shows several steps in producing common cognitive endpoints using data \nfrom different sources/cohorts. The first column shows a theoretical set of data from a \nvariety of sources with unknown disparity, for which the value of the data in aggregated \nform is unknown. When data is collated from these various sources, the disparity can be \nquantified and constructs emerge. Based on the degree of disparity, different statistical \nmethods for harmonizing the constructs can be applied to minimize disparity. Finally, these \nconstructs can be refactored into data that would allow investigators to perform comparisons \nacross these datasets, thereby improving the value and extending the usability of the data in \nbig data analyses.\n\nBrain Imaging Behav. Author manuscript; available in PMC 2022 April 01.\n\n\x0c""",nihms-1672220.pdf
"b'Neuron\n\nNeuroView\n\nBig Science, Team Science,\nand Open Science for Neuroscience\n\nChristof Koch1,* and Allan Jones1\n1Allen Institute for Brain Science, Seattle, WA 98109, USA\n*Correspondence: christofk@alleninstitute.org\nhttp://dx.doi.org/10.1016/j.neuron.2016.10.019\n\nThe Allen Institute for Brain Science is a non-pro\xef\xac\x81t private institution dedicated to basic brain science with an\ninternal organization more commonly found in large physics projects\xe2\x80\x94large teams generating complete,\naccurate and permanent resources for the mouse and human brain. It can also be viewed as an experiment\nin the sociology of neuroscience. We here describe some of the singular differences to more academic,\nPI-focused institutions.\n\nBut the greatest paradox of the\nsport has to do with the psycholog-\nical makeup of\nthe people who\npull the oars . Great crews may\nhave men or women of exceptional\ntalent and strength;\nthey may\nhave outstanding coxswains or\nstroke oars or bowmen; but they\nhave no stars. The team effort\xe2\x80\x94\nthe perfectly synchronized \xef\xac\x82ow of\nmuscle, oars, boat and water; the\nsingle, whole, uni\xef\xac\x81ed and beautiful\nsymphony that a crew in motion\nbecomes\xe2\x80\x94is all that matters. Not\nthe individual, not the self.\xe2\x80\x94Daniel\nJames Brown (The Boys in the\nBoat)\n\nThe \xef\xac\x81eld of biomedical science enjoys\nworldwide prestige, notable triumphs,\nand signi\xef\xac\x81cant funding. It has set itself\nthe goal of understanding biological\norganisms small and large, in health and\nin disease. But the \xef\xac\x81eld also faces two\nacute and severe challenges. The \xef\xac\x81rst\xe2\x80\x94\nthe vast complexity of organisms that it\nseeks to understand\xe2\x80\x94is a fundamental\nfeature of evolved organisms. The second\nchallenge is the growing recognition\nthe conclusions drawn\nthat many of\nfrom biomedical\nresearch are unreli-\nable, and cannot be reproduced. This\nreplication crisis is pernicious and could\nendanger\nthe long-term public sup-\nport, particularly given the prevailing\nanti-intellectual and anti-expert political\nenvironment.\n\nSuccessfully tackling both challenges is\nof the essence, in particular if the promise\nunderpinning the various international\nbrain projects to diagnose, ameliorate,\n\nand ultimately cure mental diseases and\ndisorders is to be realized in our lifetime.\nrequire complementing the\nThis will\ntraditional, small\nlaboratory culture that\nhas hitherto driven almost all biological\ndiscovery and that\nreward individual\ninvestigators and their fecundity in pub-\nlishing papers, with large, multi-disci-\nplinary teams working under highly repro-\nducible standards and making all of their\nmethods, data, and metadata publicly\navailable. This latter model\nis what the\nAllen Institute for Brain Science seeks to\nachieve with its focus on Big Science,\nTeam Science, and Open Science. We\nhere describe some of our experiences\nand lessons learned.\n\nBiomedical science seeks to answer\nan array of diverse questions, such as de-\nciphering the fates of\nindividuals from\ntheir genomes, linking the microbiome to\nlifestyle and disease, diagnosing and\ndeveloping rational therapies for autism\nspectrum disorders, and understanding\nhow the conscious mind emerges from\nthe \xef\xac\x82ickering activity of a dizzying number\nof heterogeneous nerve cells. Addressing\nthese questions requires the design,\nconstruction, and operation of an arma-\nmentarium of sophisticated tools and\nmethods from a variety of scienti\xef\xac\x81c and\ntechnological disciplines, in addition to\nsoftware engineers and computer scien-\ntists to grapple with the massive data\nstreams to extract and analyze key pa-\nrameters, and data scientists and theore-\nticians to \xef\xac\x81t these data to digital simulacra\nand theories. Biology has undergone a\ndramatic transformation since Gregor\nMendel, the founder of genetics, used\ngardening tools to discover the laws of\n\nthe Human Genome Project\n\nMendelian inheritance. Just 150 years\nlater,\nin-\nvolved a far-\xef\xac\x82ung coalition of technicians\nand scientists with their sophisticated\nmachinery. This singular project brought\nthe era of Big Science to biology.\n\nIt was the construction of large particle\naccelerators at the Radiation Laboratory\nin Berkeley under Ernest Lawrence in the\n1930s that gave birth to Big Science.\nGrowing up in the shadow of the Manhat-\ntan project, Big Science came of age\npost-Sputnik, during the Cold War, fueled\nby massive government programs.\nIts\nrationalization was the discovery of un-\nknown, exotic high energy particles\n(\xe2\x80\x98\xe2\x80\x98new\xe2\x80\x99\xe2\x80\x99 physics) and the testing of speci\xef\xac\x81c\ntheories, such as the Standard Model at\nthe LHC or General Relativity at LIGO. In\nastronomy, large telescopes and plane-\ntary probes opened up new windows\ninto the cosmos or investigated planets,\nmoons, or other denizens of the solar\nsystem. This motto of Big Science as ac-\ncessing new frontiers and surveying the\nlandscape of possibilities carried into the\nHuman Genome Project and its follow-\nup ENCODE project. This is also the spirit\nin which the Allen Institute views its contri-\nbution to neuroscience.\n\nBig Science is imposed by the com-\nplexity of the phenomena investigated\nand by the proli\xef\xac\x81cacy and intricacies of\nmodern instruments. Thus, developing\ntests to detect and therapies to stop and\nperhaps even reverse the ravages visited\nupon an adolescent brain by schizo-\nphrenia, one gifted professor working\nwith her graduate student and post-\ndoctoral fellow in isolation will not tame\nthe vast beast that is the genome and\n\n612 Neuron 92, November 2, 2016 \xc2\xaa 2016 Elsevier Inc.\n\n\x0cNeuron\n\nNeuroView\n\nbrain (Thompson et al., 2014), the adult\nmouse spinal cord, the adult human brain\n(Hawrylycz et al., 2012), the developing\nhuman brain (Miller et al., 2014), the adult\nmacaque brain (Bernard et al., 2012), and\nthe developing macaque brain (Bakken\net al., 2016). These massive online re-\nsources are complemented by a meso-\nscopic mouse connectivity atlas (Oh\net al., 2014) and a cellular-resolution\n(1 mm/pixel) annotated atlas of an adult\nhuman brain (Ding et al., 2016).\n\nThese large-scale and comprehensive\nprojects could not have thrived within\nthe academic ecosystem, with its need\nfor rapid results and speci\xef\xac\x81c hypotheses\nto sustain funding and multiple \xef\xac\x81rst-auth-\nored publications for participating grad-\nuate students to obtain their PhDs. The\nconstant re\xef\xac\x81nement and adjustment of\ntools, instruments, and analysis methods\nby multiple actors without documentation\nis also a hindrance for big science pro-\njects. Of course, technology development\npropels scienti\xef\xac\x81c discovery, but if the dy-\nnamics at which such improvements\noccur are faster than the project timeline,\nthe goal of obtaining a canonical and\nreproducible product that can serve as a\ngold standard becomes elusive.\n\nWe have learned several valuable les-\nsons over the years. Critical to success\nis testing competing technologies and in-\nstruments under realistic conditions to\nassess their\nrobustness and reliability\nand insisting on \xef\xac\x81rm deadlines. Bringing\na technology platform from proof-of-\nconcept to maturity requires planning for\nan extended time course. Finally,\nit is\nimportant\nthe siren song of\nconstantly switching to the latest \xe2\x80\x98\xe2\x80\x98hot\xe2\x80\x99\xe2\x80\x99\nmethod in favor of staying the course\nwith a well-proven, older technology until\nlarge-scale data production is completed\n(Figure 1).\n\nto resist\n\nTeam Science\nLarge science projects require a highly\nspecialized workforce that works together,\nday after day, hand-in-glove, across the\norganization. Consider\nthe team that\noperates our latest online offering, the\nAllen Brain Observatory.\nis to\nrecord the cellular level activity of thou-\nsands and ultimately millions of neurons\nin functionally identi\xef\xac\x81ed regions of the\nmouse brain as the animal is engaged in\na stereotyped behavior. Our \xef\xac\x81rst release\n\nIts goal\n\nNeuron 92, November 2, 2016 613\n\nFigure 1. Life Cycle of a Generic Technology in Basic Science\nAn important lesson we have learned many times is the need to pick state-of-the-art instrumentation\ntechnologies that are sensitive and reliable, with well-understood operational procedures that can operate\nstably over years for our large neuroscience data products. This may preclude adopting the latest cutting-\nedge technique that has just been published as a proof of principle. This is schematically illustrated with\nthe time line for some of the canonical events in the life cycle of some relevant technology. The ordinate\nplots penetration (usage and adoption by the science community) in red and quality and reproducibility of\nthe technology in blue.\n\nfor\n\ninstance,\n\nthe brain (see,\nthe long\nauthor list in Sekar et al. 2016). Under-\nstanding how brains work in health, and\nhow they break down in disease, de-\nmands Big Science to complement the\ntraditional discovery process. Big Sci-\nence, though, is a signi\xef\xac\x81cant departure\nfrom the investigator-focused culture of\nsmall and autonomous laboratories and\nrequires fresh thinking on how individual\nscientists are rewarded, teams are moti-\nvated, and research is planned, paid for,\nand executed.\n\nBig Science\nAt the beginning of the millennium, Paul\nAllen, co-founder of Microsoft, assem-\nbled a group of biologists to discuss the\nfuture of neuroscience and what could\nbe done to accelerate its course. From\nthese meetings emerged the idea of\ngenerating a complete atlas detailing\ngene expression throughout the brain of\nthe most popular mammalian model sys-\ntem, the laboratory mouse. Funded by\nan initial gift of $50 million by Mr. Allen, a\nteam of about 50 people accomplished\nthis feat in 2006, under budget and on\ntime, using in situ hybridization for map-\nping the expression of 20,000 genes in\nsectioned brain tissue. The annotated\ndata were, and continue to be,\nfreely\navailable to anybody with an internet\n\nconnection at http://brain-map.org. This\neffort was powered by a \xe2\x80\x98\xe2\x80\x98structured\nscience\xe2\x80\x99\xe2\x80\x99 approach adopted from the\nbiotechnology industry, including robotic\ninstrumentation, standardized operating\nprocedures to reduce variability, high\nquality control and quality assurance\nstandards and extensive program man-\nagement, supplemented by external sci-\nenti\xef\xac\x81c advisory boards.\n\nThe Allen Mouse Brain Atlas is now the\nde facto standard for visualizing gene\nexpression patterns in the mouse brain,\nwith more than 2,000 citations to the\nassociated publication (Lein et al. 2007)\nand more than 15,000 unique monthly vis-\nitors to the online resource.\nit\ncould be argued that the Allen Mouse\nBrain Atlas satis\xef\xac\x81es Sydney Brenner\xe2\x80\x99s\nfamous CAP criteria pertaining to large-\nscale surveys: complete coverage of all\ngenes throughout\nthe brain, accurate\nprobes (i.e., with high sensitivity and\nspeci\xef\xac\x81city), and maintained as a curated\nand permanent resource.\n\nIndeed,\n\nBuilding on the success of the Allen\nMouse Brain Atlas,\nthe Allen Institute\nre\xef\xac\x81ned and adapted this high-throughput,\nscalable, and robust platform to create\nadditional cartographies, comprehen-\nsively mapping transcriptional expression\npatterns within a three-dimensional coor-\ndinate system in the developing mouse\n\n\x0c(http://observatory.brain-map.org/) in the\nopen-source Neurodata Without Borders\ndata format (Teeters et al., 2015) features\nthe calcium activity of more than 30,000\nindividual neurons responding to a battery\nof visual stimuli,\nincluding static and\nmoving gratings, sparse noise, natural\nscenes, and the movie Touch of Evil. Activ-\nity is recorded via two-photon microscopy\nin primary visual cortex and nearby regions\nin different transgenic mice and mapped\nto a high-resolution neuroanatomical coor-\ndinate framework. Together with associ-\nated metadata, including eye movements,\nvideo of the running mice and so on, this\n\xef\xac\x81rst dataset exceeds 30TB.\n\nThe team that built and operates the\nAllen Brain Observatory numbers one\nhundred specialists and technicians (not\nall of whom work full-time on this\nproject)\xe2\x80\x94technicians to care for and train\nthe animals, neurobiologists to plan and\nexecute the behavioral experiment, elec-\ntrical and optical engineers to construct\nand maintain the microscopes to identify\nfunctional\nregions and image cellular-\nlevel activity, neurosurgeons to precisely\nplace transparent windows into murine\nskulls, mechanical engineers to build the\ngimbals, reticules, and other widgets to\nreproducibly return to the same set of\nneurons over multiple trials, anatomists\nand annotators to localize responses\nwithin the 3D brain, software engineers\nto harness and massage the massive\ndata stream from all\ninstruments into a\ncommon laboratory information manage-\nment system, data scientists to extract\nthe tiny fraction of relevant information,\nand modelers and theoreticians to carry\nout statistical analyses and build neural\nnetwork and other models to analyze\nand replicate the data. Four years in the\nmaking, the project has been fully funded\nby the generosity of one individual, Paul\nAllen.\n\njust\n\nThe dif\xef\xac\x81culties in operating such an\nthose asso-\nobservatory are not\nciated with setting up any advanced\ninstrumentation suite, but also include\nsigni\xef\xac\x81cant sociological and organizational\nchallenges.\n\nWe all treasure a sense of autonomy.\nYet inherent to any team, whether oper-\nating on the battle \xef\xac\x81eld, on the water rac-\ning a scull, in a start-up, or at an institute\nwith production deadlines, is the impera-\ntive to align on the agreed-upon goals\n\n614 Neuron 92, November 2, 2016\n\nimplications for\n\nand the attendant need to submerge the\nego, the self, to the group as a whole.\nThese con\xef\xac\x82icting demands have impor-\ntant\ninternal decision\nmaking and highlight the absolute need\nfor the development and maintenance of\ntrust across the entire team. Achieving\nthese goals requires a sophisticated\nmeeting culture (e.g., taking minutes, spe-\nci\xef\xac\x81c agenda,\nfollow-up, starting and\nending on time, no distracting smart-\nphone or laptop usage, and so on). Regu-\nlar team meetings are the most visible\ndifference to academic culture.\n\nThese challenges grow as the size of\nthe team grows. Our anecdotal evidence\nsuggests that above a hundred members,\ngroup cohesion appears to become\nweaker with the appearance of semi-\nautonomous cliques and sub-groups.\nThis may relate to the postulated limit on\nthe number of meaningful social interac-\ntions humans can sustain given the size\nof their brain (Dunbar, 1992).\n\nTeams are built on the strengths and\nabilities of their members. While alpha-\ntype personalities pursuing their own\nidea thrive in an academic setting, we\nare more dependent on team players\nwith a highly cooperative style of give\nand take in an environment that unleashes\ntheir energy and creativity.\n\nWithin this team-oriented context, how\nshould individual contributors, on whose\ncreativity, brilliance, dedication, disci-\npline, and hard work the entire enterprise\nrests, be rewarded? As a non-pro\xef\xac\x81t insti-\ntution, we do not offer stock options or\noutsized salaries to reward strong per-\nformers. Likewise, the promise of \xef\xac\x81rst\nor senior authorship only acts as a\nweak draw given the large number of\ncontributors.\n\nThis means that internal motivators are\nkey\xe2\x80\x94team members value the knowl-\nedge that they are participating in an his-\ntoric mission at the frontier of science\nthat will unearth new knowledge to the\nbene\xef\xac\x81t of all of humankind.\n\nSuch considerations require a judicious\nrecruitment process, ample opportunities\nfor growth, and promotion within the or-\nganization to reward performance, the\nnurturing of a sense of being part of some-\nthing larger than oneself, and distinct\ntracks for employees interested in careers\nin basic research (scientists I, II, senior\nscientist, and investigators), structured\n\nNeuron\n\nNeuroView\n\nscience (managers and directors of\ndistinct rankings), or science manage-\nment (project and program managers).\n\nFortunately,\n\nthe physics community\nhas shown the way by assembling colos-\nsal teams. The two independent collabo-\nrations at\nthe Large Hadron Collider\n(LHC) at CERN in Geneva that success-\nfully hunted for the Higgs Boson\xe2\x80\x94CMS\nand ATLAS\xe2\x80\x94each has about 3,000 par-\nticipants with elaborate, point-based\nrules for authorship and who can speak\nfor the group. The authors list of the\nLIGO consortium publication announcing\nthe \xef\xac\x81rst detection of gravity waves\nemitted by two merging black holes was\nstrictly alphabetical and included about\na thousand individuals from 133 institu-\ntions (Abbott et al., 2016). Note that these\nlarge organizations happily and produc-\ntively coexist with traditional academia.\nThus, while there can be upward of\n13,000 people on site at CERN in Geneva,\nonly about 2,200 of these are CERN em-\nployees; the rest are subcontractors, stu-\ndents, fellows, and visiting faculty.\n\nWe have learned from these and other\norganizations that to build a well-func-\ntioning team with a strong esprit de corps,\nit is critical to jointly and cooperatively\nalign on speci\xef\xac\x81c common goals, build\ntrust by open and transparent decision-\nmaking, a maximal sharing of both re-\nsponsibilities and credit, and nourish\nmorale by a dense web of formal and\ninformal means of communication. We\nhave also learned the importance of\ncompromise among con\xef\xac\x82icting demands\nand experts from different scienti\xef\xac\x81c\ntraditions. This is really nothing but the\nart of\nthe achievable under time and\nbudgetary constraints\xe2\x80\x94succinctly ex-\npressed by Otto von Bismarck\xe2\x80\x99s dictum,\n\xe2\x80\x98\xe2\x80\x98Politics is the art of the possible.\xe2\x80\x99\xe2\x80\x99\n\nOpen Science\nFrom the early days of the Institute, we\nhave made the fruits of our investigations\navailable to anybody with an internet\nconnection through our\nrepository at\nhttp://brain-map.org. These massive re-\nsources totaling more than 3,000 Tera-\nbytes of data are actively maintained\nand curated. No login or\nregistration\nstep is needed to browse, search, view,\nor download any of this wealth of informa-\ntion via our dedicated web tools. Data re-\nleases occur well ahead of the publication\n\n\x0cNeuron\n\nNeuroView\n\nBox 1. Open Science Recommendation\n\nTo accelerate the rate of discovery and to ameliorate the replication crisis, it is imperative that the neuroscience community moves\ntoward an open science ethic. After all, almost all basic science is funded, directly or indirectly, by citizens via their taxes. There-\nfore, the fruits of these labors ought to be publicly, freely, and widely shared. We offer some speci\xef\xac\x81c suggestions.\n\nd The detailed code for the complete analysis of data and statistical procedures should accompany every publication. The\nmost convenient form is a jupyter notebook (https://jupyter.org/), a Python-based web application for the creation and\nsharing of documents that contains live code, equations, \xef\xac\x81gures, and explanatory text. This allows researchers to easily\nreplicate\xe2\x80\x94and vary\xe2\x80\x94the conclusion of the study. This imposes a low burden on authors.\n\nd We do know from our own experience that making data freely available imposes a signi\xef\xac\x81cant burden (for a thoughtful dis-\ncussion of the costs and the bene\xef\xac\x81ts, see Choudhury et al. 2014). Data and relevant metadata need to be formatted into\na common data format and placed online. This repository needs to be curated. Increasingly, funding agencies are receptive\nto such initiatives and the attendant costs. With rare exceptions for singular \xef\xac\x81ndings, the era of illustrating discoveries via\nnothing but a \xef\xac\x82at PDF \xef\xac\x81le, with \xe2\x80\x98\xe2\x80\x98representative results\xe2\x80\x99\xe2\x80\x99 that are often the most expected or cleanest responses, ought to\nbe coming to a close.\n\nof associated platform papers. Our cur-\nrent 10-year plan calls for continual thrice\nyearly releases of data. We also share\nwhite papers describing in minute detail\nour operating procedures and methods\nand many of our tools, such as transgenic\nanimals and viruses (e.g., Madisen et al.,\n2010). Indeed, The Jackson Laboratory\nhas shipped more than 20,000 of our\ntransgenic mice to customers worldwide.\nWhy do we pursue such an \xe2\x80\x98\xe2\x80\x98information-\nwants-to-be-free\xe2\x80\x99\xe2\x80\x99 policy? The simplest\nanswer is that this is the vision of Paul\nAllen. His intent is that Institute resources\nshould accelerate neuroscience discov-\nery. He also believes that the Institute\nshould act as an example of what every\nlaboratory should be doing.\n\nFor millennials, communicating via texts\nand images is part of their social online\nexperience growing up. Thus, it comes\nnatural to young scientists to freely and\nopenly share data, computer code, and\nmanuscripts. Indeed, open science has\nbeen advocated for and practiced by a\nsmall number of neuroscientists for more\nthan a decade (e.g., Van Horn and Gazza-\nniga, 2013; Glasser et al., 2016). Regret-\ntably, however, the vast majority of data\nand metadata in the neurosciences con-\ntinues to remain inaccessible.\n\nIt is urgent that the \xef\xac\x81eld as a whole\nmove toward an open science policy, as\nit will alleviate some of the root causes\nof the replication crisis so pervasive in\nbiomedical research. A recent review on\nthis topic (Button et al., 2013) opens with\nan eye-catching, \xe2\x80\x98\xe2\x80\x98It has been claimed\nand demonstrated that many\n(and\npossibly most) of the conclusions drawn\n\nfrom biomedical research are probably\nfalse.\xe2\x80\x99\xe2\x80\x99 Estimates for the fraction of false\n\xef\xac\x81ndings range from a simple majority to\n80% and higher (Open Science Collabo-\nration, 2015). Given that replication is\none of the key steps in the scienti\xef\xac\x81c pro-\ncess, its systematic failure for so many\npublished \xef\xac\x81ndings constitute a striking\ndeparture from good scienti\xef\xac\x81c practice\nthat could come back to haunt the \xef\xac\x81eld.\nThis cannot be eluded by blithely ignoring\nit as most scientists are wont to do.\n\nThere are two broad categories of\n\ncauses for the replication crisis.\n\nFirst, even simple organisms have\nvastly more degrees of freedom than the\nnatural, non-evolved systems physicists\ntypically deal with, such as elementary\nparticles, gravity waves, or exo-planets.\nThus, while a Higgs Boson has the same\nsignature no matter where on Earth it is\ndetected, a C57BL/6J mouse in Seattle\nwill not be the same as a laboratory\nmouse in Boston (Crabbe et al., 1999).\nWhat is true for standard breeds of mice\nkept under controlled conditions is vastly\nmore so for \xe2\x80\x98\xe2\x80\x98neuro-typical\xe2\x80\x99\xe2\x80\x99 volunteers\nor patients. Mitigating this problem will\nnot be easy but must include enforcing,\nto the maximal extent possible, standard-\nization of model systems, procedures,\ntools, and instruments, as well as meticu-\nlously publicizing all logistical and meth-\nodological details. Unbiased surveys\neliminate some sources of these statisti-\ncal infelicities and have found widespread\nadoptions in some \xef\xac\x81elds. For example,\nwithin astronomy, generations of Sloan\nDigital Sky surveys have resulted in more\nthan 5,000 refereed publications and 200\n\nmillion SQL queries to the relevant data-\nbase and revolutionized the \xef\xac\x81eld in the\nprocess (Burns et al., 2014).\n\nA second set of causes relate to lack of\nstatistical \xe2\x80\x98\xe2\x80\x98hygiene,\xe2\x80\x99\xe2\x80\x99 including low statis-\ntical power, incorrect noise models, pub-\nlication bias (or \xe2\x80\x98\xe2\x80\x98the \xef\xac\x81le drawer problem\xe2\x80\x99\xe2\x80\x99),\nhypothesizing after the results are known\n(HARKing), and p-hacking. These phe-\nnomena have been much commented\nupon (Ioannidis 2005; Button et al. 2013).\nThe latest scandal follows the re-analysis\nof resting-state fMRI data from the public\ndatabase Functional Connectomes Proj-\nect (Eklund et al., 2016), which revealed\nthat widely used analysis software pack-\nages yield false-positive rates of up to\n70%. This may affect more than 3,000\npublished fMRI studies, funded at great\nexpense by the public purse.\n\nThere is no question that making all\ndata and metadata,\ntogether with all\ncomputational and statistical procedures\nof every publication freely and publicly\navailable would address some of these\nshortcomings (see Box 1). Such a policy\nallows researchers to easily test whether\nthe published conclusions are valid,\nparticularly when they con\xef\xac\x82ict with other\nstudies. Given the potential threat to the\nreputation of the authors, Open Science\nis likely to lead to statistically more valid\nresults than otherwise. Open access\nalso allows for analysis of published data\nusing different algorithms and assump-\ntions to gain additional or alternative in-\nsights, or to ask questions that were not\nconceived by the original authors. Finally,\nopen data sharing enables the aggrega-\ntion and contrasting of data across\n\nNeuron 92, November 2, 2016 615\n\n\x0cNeuron\n\nNeuroView\n\nmultiple studies for meta-analyses to\nextract broader insights. Quite simply,\nOpen Science is the right thing to do.\n\nThe Allen Institute for Brain Science has\nproven that Big Science, Team Science,\nand Open Science can be harnessed to\ncreate extraordinary neuroscienti\xef\xac\x81c re-\nsources that bene\xef\xac\x81t all. With bright eyes,\nwe look toward a future in which we,\ntogether with the world-wide community\nof\nlabora-\ntories at universities and independent\nresearch institutes, will decipher\nthe\nmost highly organized piece of excitable\nmatter in the known universe: the human\nbrain.\n\nresearchers from individual\n\nABOUT THE AUTHORS\n\nChristof Koch is the President and Chief Scienti\xef\xac\x81c\nOf\xef\xac\x81cer of the Allen Institute for Brain Science. After\nspending 27 years as a faculty member at the Cal-\nifornia Institute of Technology, he joined the Insti-\ntute in 2011. He seeks to understand the workings\nof the mammalian cerebral cortex and how it gen-\nerates conscious experience. Allan Jones is the\nPresident and Chief Executive Of\xef\xac\x81cer of the Allen\nInstitute. He joined the Institute from biotech/\npharma at the beginning of the Allen Brain Atlas\nproject in 2003 and is passionate about under-\nstanding complexity in biology.\n\nACKNOWLEDGMENTS\n\nWe thank Anton Arkhipov, Mike Hawrylycz, Han-\nnah Krakauer, Ed Lein, John Phillips, Erica Sessle,\nand Hongkui Zeng for thoughtful comments and\nfeedback. None of this work could have been\naccomplished without the vision and the enduring\n\nsupport of our founders, Paul G. Allen and Jody\nAllen. A.J. is the head of the overall Allen Institute,\nand C.K. is the head of its daughter, the Allen Insti-\ntute for Brain Science.\n\nREFERENCES\n\nAbbott, B.P., Abbott, R., Abbott, T.D., Abernathy,\nM.R., Acernese, F., Ackley, K., Adams, C., Adams,\nT., Addesso, P., Adhikari, R.X., et al.; LIGO Scien-\nti\xef\xac\x81c Collaboration and Virgo Collaboration (2016).\nPhys. Rev. Lett. 116, 061102.\n\nBakken, T.E., Miller, J.A., Ding, S.L., Sunkin, S.M.,\nSmith, K.A., Ng, L., Szafer, A., Dalley, R.A., Royall,\nJ.J., Lemon, T., et al. (2016). Nature 535, 367\xe2\x80\x93375.\n\nBernard, A., Lubbers, L.S., Tanis, K.Q., Luo, R.,\nPodtelezhnikov, A.A., Finney, E.M., McWhorter,\nM.M., Serikawa, K., Lemon, T., Morgan, R., et al.\n(2012). Neuron 73, 1083\xe2\x80\x931099.\n\nBurns, R., Vogelstein, J.T., and Szalay, A.S. (2014).\nNeuron 83, 1249\xe2\x80\x931252.\n\nButton, K.S., Ioannidis, J.P., Mokrysz, C., Nosek,\nB.A., Flint, J., Robinson, E.S., and Munafo` , M.R.\n(2013). Nat. Rev. Neurosci. 14, 365\xe2\x80\x93376.\n\nChoudhury, S., Fishman, J.R., McGowan, M.L.,\nand Juengst, E.T. (2014). Front. Hum. Neurosci.\n8, 239.\n\nCrabbe, J.C., Wahlsten, D., and Dudek, B.C.\n(1999). Science 284, 1670\xe2\x80\x931672.\n\nDing, S.-L., Royall, J.J., Sunkin, S.M., Ng, L.,\nFacer, B.A., Lesnar, P., Guillozet-Bongaarts, A.,\nMcMurray, B., Szafer, A., Dolbeare, T.A., et al.\n(2016). J. Comp. Neurol. 524, 3127\xe2\x80\x933481.\n\nT.S., Harms, M.P., Jenkinson, M., Moeller, S., et al.\n(2016). Nat. Neurosci. 19, 1175\xe2\x80\x931187.\n\nHawrylycz, M.J., Lein, E.S., Guillozet-Bongaarts,\nA.L., Shen, E.H., Ng, L., Miller, J.A., van de Lage-\nmaat, L.N., Smith, K.A., Ebbert, A., Riley, Z.L.,\net al. (2012). Nature 489, 391\xe2\x80\x93399.\n\nIoannidis, J.P.A. (2005). PLoS Med. 2, e124.\n\nLein, E.S., Hawrylycz, M.J., Ao, N., Ayres, M., Ben-\nsinger, A., Bernard, A., Boe, A.F., Boguski, M.S.,\nBrockway, K.S., Byrnes, E.J., et al. (2007). Nature\n445, 168\xe2\x80\x93176.\n\nMadisen, L., Zwingman, T.A., Sunkin, S.M., Oh,\nS.W., Zariwala, H.A., Gu, H., Ng, L.L., Palmiter,\nR.D., Hawrylycz, M.J., Jones, A.R., et al. (2010).\nNat. Neurosci. 13, 133\xe2\x80\x93140.\n\nMiller, J.A., Ding, S.L., Sunkin, S.M., Smith, K.A.,\nNg, L., Szafer, A., Ebbert, A., Riley, Z.L., Royall,\nJ.J., Aiona, K., et al. (2014). Nature 508, 199\xe2\x80\x93206.\n\nOh, S.W., Harris, J.A., Ng, L., Winslow, B., Cain, N.,\nMihalas, S., Wang, Q., Lau, C., Kuan, L., Henry,\nA.M., et al. (2014). Nature 508, 207\xe2\x80\x93214.\n\nOpen Science Collaboration (2015). Science 349,\nhttp://dx.doi.org/10.1126/science.aac4716.\n\nSekar, A., Bialas, A.R., de Rivera, H., Davis, A.,\nHammond, T.R., Kamitaki, N., Tooley, K., Presu-\nmey, J., Baum, M., Van Doren, V., et al.; Schizo-\nphrenia Working Group of the Psychiatric Geno-\nmics Consortium (2016). Nature 530, 177\xe2\x80\x93183.\n\nTeeters, J.L., Godfrey, K., Young, R., Dang, C.,\nFriedsam, C., Wark, B., Asari, H., Peron, S., Li,\nN., Peyrache, A., et al. (2015). Neuron 88, 629\xe2\x80\x93634.\n\nDunbar, R.I.M. (1992). J. Hum. Evol. 22, 469\xe2\x80\x93493.\n\nEklund, A., Nichols, T.E., and Knutsson, H. (2016).\nProc. Natl. Acad. Sci. USA 113, 7900\xe2\x80\x937905.\n\nThompson, C.L., Ng, L., Menon, V., Martinez, S.,\nLee, C.K., Glattfelder, K., Sunkin, S.M., Henry, A.,\nLau, C., Dang, C., et al.\n(2014). Neuron 83,\n309\xe2\x80\x93323.\n\nGlasser, M.F., Smith, S.M., Marcus, D.S., Ander-\nsson, J.L., Auerbach, E.J., Behrens, T.E., Coalson,\n\nVan Horn, J.D., and Gazzaniga, M.S. (2013). Neu-\nroimage 82, 677\xe2\x80\x93682.\n\n616 Neuron 92, November 2, 2016\n\n\x0c'",1-s2.0-S0896627316307206-main.pdf
"b'Jarke, H., Anand-Vembar, S., Alzahawi, S., Andersen, T. L., Bojani\xc4\x87, L., Carstensen, A.,\nFeldman, G., Garcia-Garzon, E., Kapoor, H., Lewis, S., Todsen, A. L., Ve\xc4\x87kalov, B.,\nZickfeld, J. H., & Geiger, S. J. (2022). A Roadmap to Large-Scale Multi-Country\nReplications in Psychology. Collabra: Psychology, 8(1).\nhttps://doi.org/10.1525/collabra.57538\n\n \n\n \n\n \n\n \n\n \n\n , Gilad Feldman 8  \n\n , Shilaan Alzahawi 4  \n\n , Eduardo Garcia-Garzon 2,9  \n\n a , Shaakya Anand-Vembar 2,3  \n\n , Anna Louise Todsen 2,14  , Bojana Ve\xc4\x87kalov 2,15  \n\nMethodology and Research Practice \nA Roadmap to Large-Scale Multi-Country Replications in \n \n \nPsychology  \nHannes Jarke 1,2  \nAlexandra Carstensen 7  \nSavannah Lewis 12,13  \n1 Centre for Business Research, Judge Business School, University of Cambridge, Cambridge, UK, 2 Junior Researcher Programme, Cambridge, UK, \n3 Department of Psychiatry, School of Medicine, Trinity College Dublin, Ireland, 4 Graduate School of Business, Stanford University, Stanford, CA, US, \n5 Child and Adolescent Mental Health Center, Copenhagen University Hospital, Mental Health Services Copenhagen, Copenhagen, Denmark, \n6 Department of Psychology and Mental Health, University of Manchester, Manchester, UK, 7 Psychology Department, University of California, San \nDiego, La Jolla, CA, US, 8 Psychology Department, University of Hong Kong, 9 School of Health, Camilo Jos\xc3\xa9 Cela University, Villafranca del Castillo, \nMadrid, Spain, 10 University of Connecticut, Storrs, CT, USA, 11 Department of Psychology, Monk Prayogshala, Mumbai, India, 12 Psychological Science \nAccelerator, 13 International Collaboration Research Center, Ashland University, Ashland, OH, US, 14 School of Psychology and Neuroscience, Faculty \nof Arts, University of St Andrews, St Andrews, UK, 15 Department of Psychology, University of Amsterdam, Amsterdam, The Netherlands, 16 Centre for \nIntegrative Business Psychology, Aarhus University, Aarhus, Denmark, 17 Department of Management, Aarhus University, Aarhus, Denmark, \n18 Environmental Psychology, Department of Cognition, Emotion, and Methods, Faculty of Psychology, University of Vienna, Vienna, Austria \nKeywords: Replication, Multi-country, Psychology, Big Team Science, Underrepresented Populations, Research Methodology, Reproducibility, \nInternational Research \nhttps://doi.org/10.1525/collabra.57538 \n\n , Thomas Lind Andersen 2,5  \n ,\n\n , Hansika Kapoor 10,11  \n\n , Janis H. Zickfeld 16,17  \n\n , Sandra J. Geiger 2,18  \n\n , Lana Bojani\xc4\x87 6  \n\n ,\n\nCollabra: Psychology \nVol. 8, Issue 1, 2022 \n\nClassic findings from psychology and the behavioural sciences are increasingly being \nrevisited. Methodological and technological advances provide opportunities to replicate \nstudies across a wide range of countries and settings to investigate whether these \nfindings are universally applicable, limited to specific countries, or vary in magnitude \ndepending on settings. Researchers from around the world connect to revisit such \nfindings collaboratively, adapt the original design to the Zeitgeist, integrate new \nknowledge to improve statistical analyses, and broaden the scope by testing effects \nglobally \xe2\x80\x93 or at least in as many countries, as budget and feasibility allow. We currently \nobserve multiple international consortia conducting large-scale multi-country \nreplications. How do such collaborations form and how do they approach these complex \ninvestigations? This paper brings together researchers from different initiatives that \nconduct replications on an international scale to outline approaches and summarises \nwhat we have learned in applying them: Junior Researcher Programme (JRP), \nPsychological Science Accelerator (PSA), ManyBabies, Collaborative Open-science \nREsearch (CORE), and International Study of Metanorms (ISMN). We describe different \nways for study selection, methodological approaches, statistical analyses, ethical issues, \nand most importantly, how the different collaborations formed and how team \ncommunication worked. We look in detail at challenges of including typically \nunderrepresented countries in psychological science, not only in terms of data collection \nbut also in making it possible for local researchers to contribute. This paper provides a \nstructured insight into how different collaborations work and issues to consider for \nanyone who seeks to conduct a multi-country replication in psychology, or looking for \nadditional perspectives to their existing plan. We close the article with a checklist built as \na helpful tool for colleagues putting together their study protocols for such efforts \xe2\x80\x93 and \ninvite them to collaboratively expand it in the future. \n\nIntroduction  \n\nThe replication and reproducibility crisis (Pashler & Wa-\ngenmakers,  2012;  Wiggins  &  Christopherson,  2019)  has \neroded  trust  in  classic  findings  from  psychology  and  be-\n\nhavioural sciences (Anvari & Lakens, 2018; Wingen et al., \n2020).  Revisiting  such  findings  and  investigating  whether \nwidely  cited  and  applied  concepts  can  be  replicated  and \ngeneralised across time, context, and countries is more and \nmore  visible  and  encouraged  by  researchers  and  journals. \n\na \n\nCorrespondence should be addressed to Hannes Jarke, hj318@cam.ac.uk \n\nl\n\nD\no\nw\nn\no\na\nd\ne\nd\n\n \nf\nr\no\nm\nh\n\n \n\nt\nt\n\np\n\n:\n/\n/\n\no\nn\n\nl\ni\n\n.\n\nn\ne\nu\nc\np\nr\ne\ns\ns\n.\ne\nd\nu\n/\nc\no\n\nl\nl\n\n/\n\na\nb\nr\na\na\nr\nt\ni\nc\ne\n-\np\nd\n\nl\n\nf\n/\n\n/\n\n/\n\n/\n\n8\n1\n5\n7\n5\n3\n8\n7\n6\n7\n4\n2\n7\n/\nc\no\n\nl\nl\n\na\nb\nr\na\n_\n2\n0\n2\n2\n_\n8\n_\n1\n_\n5\n7\n5\n3\n8\np\nd\n\n.\n\nf\n \n\nb\ny\n \ng\nu\ne\ns\nt\n \n\n \n\no\nn\n0\n4\n \nJ\na\nn\nu\na\nr\ny\n \n2\n0\n2\n3\n\n\x0cA Roadmap to Large-Scale Multi-Country Replications in Psychology\n\nMethodological and technological advances, as well as im-\nprovements  in  data  collection  and  management  practices \ncan now be utilised to update our knowledge about theory \nand  findings  and  expand  further  to  reach  populations \naround  the  globe.  These  advances  provide  new  opportu-\nnities for initiatives that are sometimes labelled Big Team \nScience: collaborations \xe2\x80\x9cinvolving a relatively large number \nof collaborators who may be dispersed across labs, institu-\ntions, disciplines, cultures, and continents\xe2\x80\x9d (Forscher et al., \n2020, p. 2). For example, to test their 1979 Nobel prize-win-\nning Prospect Theory, Daniel Kahneman and Amos Tversky \nrelied on a small random sample of students from the US, \nSweden, and Israel. Using collaborative online tools, Rug-\ngeri  and  colleagues  (2019)  recently  replicated  patterns  of \nprospect  theory  across  19  countries  and  in  13  languages \nwith  over  4,000  participants.  This  project  was  facilitated \nwith  the  help  of  the  Junior Researcher Programme  (JRP), \nan  initiative  that  provides  opportunities  for  early-career \nresearchers  in  psychology  (jrp.pscholars.org;  Jarke,  2021; \nRuggeri,  2020).  Multi-country  replication  initiatives  that \nstarted  earlier  include  the  Open  Science  Collaboration \n(2012),  ManyLabs  (e.g.,  Ebersole  et  al.,  2014;  Klein  et  al., \n2014), or the Collaborative Replications and Education Pro-\nject (CREP; Grahe et al., 2013). Another approach to such \nmulti-country replications is facilitated by the Psychological \nScience Accelerator  (PSA;  Beshears  et  al.,  2020;  Moshontz \net al., 2018), an organisation focused on producing psycho-\nlogical science that is generalizable and reliable by utilising \ntheir distributed network of labs. \n\nIn this paper, we aim to synthesise insights gained from \nseveral  multi-country  collaborations.  In  two  online  panel \ndiscussions (Anand-Vembar et al., 2021; Jarke et al., 2021), \nwe  brought  together  researchers  who  participated  in  and \norganised  multi-country  replication  efforts  to  discuss  key \naspects of such undertakings (see Table 1 for an overview). \nBased on these discussions, we summarise structured in-\nsights  and  experiences  with  different  approaches  on  key \ntopics, including study selection, replication criteria, trans-\nlation,  selection  of  potential  collaborators,  team  commu-\nnication, data collection methods and privacy issues, gain-\ning ethical approval for multiple countries, and statistical \napproaches.  We  also  address  the  inclusion  of  underrep-\nresented  countries  and  outline  hurdles  in  including  col-\nleagues from these areas. We conceptualised this paper as \na roadmap to support researchers planning a multi-country \nreplication  so  that  they  can  incorporate  what  has  been \nlearned  in  previous  attempts  and  tailor  successful  ap-\nproaches  to  their  lab\xe2\x80\x99s  situation.  Some  consortia  already \nhave  compiled  information  on  their  approaches  in  detail. \nThese are included in the accompanying reading list to this \npaper  (Appendix  1).  We  discuss  our  insights  chronologi-\ncally, starting at the beginning of a study until publication \nand beyond. In addition, we compiled a checklist of lessons \nlearned (Appendix 2). This checklist is not a comprehensive \n\n\xe2\x80\x9cmust-have\xe2\x80\x9d but rather a tool that summarises considera-\ntions for preparing a multi-country replication. Researchers \nalready involved in such efforts may also find suggestions \nand insights that can strengthen their existing approaches \nor  add  a  perspective  they  had  not  previously  considered. \nWhile this paper primarily focuses on replications including \nlarge  numbers  of  labs  and  researchers,  many  insights  are \nalso  applicable  to  replications  within  fewer  countries  or \nsettings. \n\nBuilding a Consortium: Identifying Collaborators \n \n\n \n\n \n\n \n\n \n\nand Team Communication \n \n\n \n\n \n\nPerhaps  the  easiest  way  to  build  a  consortium1  of  col-\nlaborators is to draw on existing networks. Uhlmann et al. \n(2019)  describe  multi-lab  collaborations  along  two  axes, \none  being  inclusiveness  versus  selectivity  (i.e.,  who  can \nparticipate and make decisions), the other being communi-\ncation (i.e., is there constant collaboration, or does the PI \n\xe2\x80\x9ccollect\xe2\x80\x9d and synthesise the work of otherwise mostly inde-\npendent units). We outline our own experiences below. \n\nThe JRP draws on its network of programme alumni and \ncurrent  team  members,  with  current  programme  interns \nusually  leading  a  country  team  in  collaboration  with  stu-\ndents participating in GLOBES - a programme at Columbia \nUniversity, which connects undergraduate students in be-\nhavioural  sciences  to  researchers.  The  majority  of  collab-\norators are early-career researchers (ECRs; here: students \nand postdocs who make up small project teams each year \nbut  often  go  on  to  participate  in  larger  collaborations), \nwhereas the project itself is led by an established academic. \nJRP  interns  and  alumni  can  be  integrated  into  projects \nwithout  further  screening,  as  a  working  relationship  and \ntrust are already established through them having partici-\npated in a summer school and a one-year research project \nsupported by the programme at that point. Participating re-\nsearchers are briefed usually five to six months before activ-\nities commence and learn about their responsibilities, the \nproject timeline, and how communication will be facilitated \n(Ruggeri, 2020). Shortly before and during data collection, \ncommunication is facilitated via daily briefing emails with \nupdates, key developments, and reminders. These are sup-\nported by group calls and a slack channel for discussion be-\ntween country teams as well as an FAQ page for providing \nimportant  information  across  country  teams.  While  the \nfirst  JRP-organised  multi-country  replication  (Ruggeri  et \nal., 2020) was conducted with most contributors on-site in \none place, the two following projects have been conducted \nonline  (Ruggeri  et  al.,  2021,  2022)  due  to  the  COVID-19 \npandemic. Although online work clearly allowed for more \ncontributors, it also constitutes a much higher workload for \nthe  project  leaders  and  administrators.  Further,  with  the \nJRP\xe2\x80\x99s  contributors  being  mostly  ECRs,  the  in-person  ap-\nproach also provided a unique and motivating experience \n\nl\n\nD\no\nw\nn\no\na\nd\ne\nd\n\n \nf\nr\no\nm\nh\n\n \n\nt\nt\n\np\n\n:\n/\n/\n\no\nn\n\nl\ni\n\n.\n\nn\ne\nu\nc\np\nr\ne\ns\ns\n.\ne\nd\nu\n/\nc\no\n\nl\nl\n\n/\n\na\nb\nr\na\na\nr\nt\ni\nc\ne\n-\np\nd\n\nl\n\nf\n/\n\n/\n\n/\n\n/\n\n8\n1\n5\n7\n5\n3\n8\n7\n6\n7\n4\n2\n7\n/\nc\no\n\nl\nl\n\na\nb\nr\na\n_\n2\n0\n2\n2\n_\n8\n_\n1\n_\n5\n7\n5\n3\n8\np\nd\n\n.\n\nf\n \n\nb\ny\n \ng\nu\ne\ns\nt\n \n\n \n\no\nn\n0\n4\n \nJ\na\nn\nu\na\nr\ny\n \n2\n0\n2\n3\n\n1 \n\nWe refer to a consortium in this context as any formal or informal collaboration between multiple labs \n\nCollabra: Psychology\n\n2\n\n\x0cA Roadmap to Large-Scale Multi-Country Replications in Psychology\n\nTable 1.  Contributors who Participated in the Panel on Multi-Country Replication Studies. \n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\nConsortium \n\nAuthors \n\nRelevant examples of work related to this topic \n\nShilaan Alzahawi, Savannah \nLewis, Janis Zickfeld \n\nMoshontz et al. (2018), Beshears et al. (2020) \n\nHannes Jarke, Lana Bojani\xc4\x87, \nEduardo Garcia-Garzon, Bojana \nVe\xc4\x87kalov, Sandra J. Geiger \n\nRuggeri et al. (2020), Ruggeri et al. (2021), Ruggeri et al. (2022) \n\nManyBabies \n\nAlexandra Carstensen \n\nGilad Feldman \n\nManyBabies Consortium (2020), Byers-Heinlein et al. (2020), Byers-\nHeinlein et al. (2021), Tsui et al. (2021); see also \nhttps://manybabies.github.io/ \n\nEfendi\xc4\x87 et al. (2022), Ziano, et al. (2021), Chandrashekar, et al. (2021), \nChen et al. (2021), Brick et al. (2021), Anvari et al. (2021), see also \nCollaborative Open-science REsearch (2022) \n\nHansika Kapoor \n\nEriksson et al. (2021) \n\nPsychological \nScience \nAccelerator \n(PSA) \n\nJunior \nResearcher \nProgramme \n(JRP) \n\nCollaborative \nOpen-science \nREsearch \n(CORE) \n\nInternational \nStudy of \nMetanorms \n(ISMN) \n\nand  ECRs  benefit  more  by  forming  stronger  relationships \nwith international colleagues. \n\nCORE works with a different model in which the Princi-\npal Investigator (PI) conducts all replications with students \nas part of the coursework in a one-semester course, publicly \nsharing each stage of replication, and inviting international \ninvolvement and collaborations throughout. Once projects \nare finalised at the end of the semester, ECRs are publicly \ninvited to take the lead over the completed projects, veri-\nfying and extending the available outputs, and then help-\ning bring these to publication in journals. The PI works with \neach ECR on their led projects, and over time, with gained \nexperience, the ECRs support each other on specific issues \nor initiatives, and join forces in tackling new projects. \n\nThe  PSA  represents  a  globally  distributed  network  of \nlabs located on all populated continents (see https://psysci-\nacc.org/map/  for  an  updated  map  of  all  labs)  organising \ndata  collection  of  democratically  chosen  projects \n(Moshontz et al., 2018). The network includes one director \nand four associate directors that are elected by all members. \nIn addition, the network is coordinated by several commit-\ntees focusing on different aspects of the study process such \nas study selection, ethics, translation and cultural diversity, \ncommunity  building,  project  monitoring,  data  and  meth-\nods,  training,  and  funding.  The  network  regularly  issues \ncalls for project submissions from both inside and outside \nthe network, which are first screened by the selection com-\nmittee, then sent out for review by at least three experts, \nand finally voted on by all PSA members. Once a project has \nbeen  successfully  selected,  PSA  members  are  able  to  sign \nup for contributing to the project by collecting data, trans-\nlating  questionnaires  or  coordinating  aspects  of  the  pro-\nject.  All  researchers  can  sign  up  to  become  a  PSA  mem-\nber. Therefore, the PSA focuses both on inclusiveness and a \nmode of constant collaboration. \n\nManyBabies is a network of developmental psychologists \ndistributed  across  a  range  of  large-scale  replication  pro-\n\njects. New members opt in to the network and join individ-\nual projects through the organization\xe2\x80\x99s website. The gov-\nerning board reviews new project proposals and advertises \napproved projects to the general listserv, where collabora-\ntors  can  volunteer  to  contribute  to  any  and  all  parts  of  a \nproject,  including  study  design,  data  collection,  analysis, \nand  writing.  The  organization  also  has  several  dedicated \npositions, including an executive director, office assistant, \nand  postdocs  who  support  the  range  of  ongoing  studies. \nManyBabies is committed to open science and uses a con-\nsensus-based approach for all project decisions to promote \ntransparency,  inclusivity,  diversity,  and  collective  gover-\nnance. \n\nAuthorship and credit \n \n\n \n\n \n\nNo matter how a consortium is established, it is advan-\ntageous if everyone\xe2\x80\x99s responsibilities and the consequences \nof inaction are clear from the beginning. These responsi-\nbilities  should  ideally  be  listed  in  the  final  output  (e.g., \nas  a  Contributor  Role  Taxonomy,  or  short  CRediT;  Allen \net  al.,  2019;  Holcombe  et  al.,  2020,  adopted  also  by  the \nPSA)  to  transparently  report  what  each  author  has  con-\ntributed. To set expectations, it is also advisable to decide \non the order of authors (or principles for determining au-\nthor  order)  as  early  as  possible  and  be  clear  on  who  will \nbe responsible for article processing fees. For instance, the \nPSA  has  specific  roles  for  each  project  (e.g.,  data  man-\nagement,  translation  coordinator,  ethics  coordinator)  and \nthese roles typically qualify for different authorship tiers; \nbut also adapts criteria on a case-by-case basis beforehand. \nManyBabies  has  a  guiding  example  for  determining  au-\nthorship (https://manybabies.github.io/authorship) but the \nleadership  team  for  each  new  project  is  responsible  for \ndefining  and  communicating  authorship  guidelines.  Simi-\nlarly, CORE employs a detailed guide for determining au-\nthorship (https://mgto.org/joinmassreplication). \n\nCollabra: Psychology\n\n3\n\nl\n\nD\no\nw\nn\no\na\nd\ne\nd\n\n \nf\nr\no\nm\nh\n\n \n\nt\nt\n\np\n\n:\n/\n/\n\no\nn\n\nl\ni\n\n.\n\nn\ne\nu\nc\np\nr\ne\ns\ns\n.\ne\nd\nu\n/\nc\no\n\nl\nl\n\n/\n\na\nb\nr\na\na\nr\nt\ni\nc\ne\n-\np\nd\n\nl\n\nf\n/\n\n/\n\n/\n\n/\n\n8\n1\n5\n7\n5\n3\n8\n7\n6\n7\n4\n2\n7\n/\nc\no\n\nl\nl\n\na\nb\nr\na\n_\n2\n0\n2\n2\n_\n8\n_\n1\n_\n5\n7\n5\n3\n8\np\nd\n\n.\n\nf\n \n\nb\ny\n \ng\nu\ne\ns\nt\n \n\n \n\no\nn\n0\n4\n \nJ\na\nn\nu\na\nr\ny\n \n2\n0\n2\n3\n\n\x0cA Roadmap to Large-Scale Multi-Country Replications in Psychology\n\nWe suggest that authorship be granted to all collabora-\ntors who contribute substantially to any crucial parts of the \nstudy.  A  collaboration  agreement  that  transparently  out-\nlines the criteria for co-authorship as well as how author-\nship  order  is  determined  can  ensure  expectations  on  this \ntopic  are  aligned.  Authorship  may  act  as  a  strong  indi-\nvidual, and career incentive to academic collaborators, in-\ncreases the public representation of researchers from mar-\nginalised  backgrounds,  and  shows  that  contributions  are \ncredited and valued. In terms of administration, it should \nalso be kept in mind that typing in all author names and \ndata for submission can take many hours of work. There-\nfore, we urge journals to implement systems that allow for \neasier  ways  of  providing  such  data,  such  as  the  option  of \nfilling in template spreadsheets that can be read and copied \nby the submission system. Big Team Science projects may \nrun into other unexpected issues regarding authorship: For \nexample, in a recent submission, the PSA Self-Determina-\ntion Theory Collaboration (2022) could not list all 500 au-\nthors on the byline because this would have exceeded the \njournal\xe2\x80\x99s  maximum  page  count.  Instead,  consortium  au-\nthorship  was  adapted,  which  especially  disadvantaged  re-\nsearchers from low-and middle-income countries and early \ncareer researchers: Institutional policies often do not rec-\nognize this type of authorship (for an extensive discussion \nabout disadvantages resulting from group authorship and a \ncall for change, see PSA\xe2\x80\x99s Open Letter2). \n\nStudy Selection \n \n\n \n\nThe process of deciding which study to replicate can vary \nsubstantially from lab to lab and consortium to consortium. \nApproaches  can  take  various  forms  from  where  the  PI  or \ntheir lab decide which study to replicate, how to do it, and \nwhom to include (top-down) to consortia forming through \nloose  ideas  and  developing  goals  and  structure  from  the \nbottom-up. \n\nWithin the PSA, several labs or PIs submit a study pro-\nposal, the study selection committee checks its feasibility, \nthe  network  votes  on  the  blinded  proposal,  then  the  lead \nteam of the accepted studies pre-registers the experiment, \nand publicly opens it up for potential collaborators. There \nare also more bottom-up approaches to study selection. For \ninstance,  the  ManyBabies  Consortium  accepts  study  pro-\nposals from any researcher, which are then evaluated by the \nManyBabies  Governing  Board.  Accepted  proposals  are  ad-\nvertised to ManyBabies Consortium members and through \nrelated organisations, enabling interested researchers and \nlabs  to  join  new  projects.  Similarly,  CREP  starts  projects \nthrough  collaborators  expressing  interest  first.  The  study \nDirector looks for top-cited studies in a range of sub-dis-\nciplines  of  psychology.  After  being  checked  for  feasibility \nby established researchers, feasible studies are sent to stu-\ndents and collaborators to rate on different aspects, such as \nfeasibility and interest. The CREP administrator then takes \n\nthose  ratings  and  identifies  the  next  studies  for  collabo-\nration.  After  a  study-specific  admin  team  is  put  together \nfor each study, labs are allowed to sign on to the project. \nOn the other hand, replications conducted by the JRP have \nstarted  with  a  small  team  around  the  PI  deciding  on  the \ntopic and detailed study plan first, before recruiting collab-\norators. \n\nFeasibility and durability: Is this topic suitable \n \n \nfor replication? \n \n\n \n\n \n\n \n\n \n\n \n\n \n\nThe most important criterion to decide on a topic is fea-\nsibility: whether a project is doable for the PI\xe2\x80\x99s team (plan-\nning and managing) and the partnered labs (execution). Key \nconsiderations include how easily the research can be ex-\necuted  across  environments  by  all  participating  labs  and \nwhether all equipment is available across sites and whether \nnecessary training has already been attained or can be pro-\nvided. All parties need to carefully consider whether there \nare sufficient resources to coordinate such a large-scale ef-\nfort,  which  can  require  substantial  amounts  of  time-con-\nsuming administrative work. For example, the ManyBabies \nconsortium  has  multiple  staff  members  dedicated  exclu-\nsively to non-research tasks. Basic considerations must in-\nclude budget (for ethics review, study materials, personnel, \netc.), whether research goals are attainable (e.g., study ma-\nterials are clear and available), and whether the expected \nimpact is worth the large-scale investment. \n\nReplication targets can also be selected based on feasi-\nbility in terms of the replicators\xe2\x80\x99 qualifications, and the in-\ntended  methods  and  target  sample.  In  CORE,  replication \nstudies are designed by undergraduate students as part of \na one-semester course with data collection designed using \nsimplified survey platforms such as Qualtrics to be collected \nonline  using  labour  markets  such  as  Amazon  Mechanical \nTurk  (with  CloudResearch)  and  Prolific,  mostly  with  US \nAmerican and British participants. Therefore, in CORE the \nselected  target  articles  are  typically  the  highest  impact \nclassics that can be easily adjusted to an online design and \nwith methods and statistics that are doable for undergrad-\nuate student level. \n\nIsager et al. (2020) propose another approach, based on \na  formalised  model  that  can  help  guide  the  decision  on \nchoosing  a  topic,  aiming  to  calculate  an  expected utility \ngain, which they label replication value. For this model, the \nauthors consider the costs of the study, uncertainty about \nthe  claim  before  replication,  the  value  of  the  scientific \nclaim, and the expected utility of the claim before the repli-\ncation as determinants for the expected utility gain. How-\never, they also state that these variables are not necessarily \nexclusive  and  further,  unexpected  variables  can  influence \nthe utility gain. \n\nl\n\nD\no\nw\nn\no\na\nd\ne\nd\n\n \nf\nr\no\nm\nh\n\n \n\nt\nt\n\np\n\n:\n/\n/\n\no\nn\n\nl\ni\n\n.\n\nn\ne\nu\nc\np\nr\ne\ns\ns\n.\ne\nd\nu\n/\nc\no\n\nl\nl\n\n/\n\na\nb\nr\na\na\nr\nt\ni\nc\ne\n-\np\nd\n\nl\n\nf\n/\n\n/\n\n/\n\n/\n\n8\n1\n5\n7\n5\n3\n8\n7\n6\n7\n4\n2\n7\n/\nc\no\n\nl\nl\n\na\nb\nr\na\n_\n2\n0\n2\n2\n_\n8\n_\n1\n_\n5\n7\n5\n3\n8\np\nd\n\n.\n\nf\n \n\nb\ny\n \ng\nu\ne\ns\nt\n \n\n \n\no\nn\n0\n4\n \nJ\na\nn\nu\na\nr\ny\n \n2\n0\n2\n3\n\n2 \n\nhttps://docs.google.com/document/d/1OLb1VSkHLBo4z8XTEsqPUOK5PEYltbjAkpP2LyxNDSM/ \n\nCollabra: Psychology\n\n4\n\n\x0cA Roadmap to Large-Scale Multi-Country Replications in Psychology\n\nDifferent types of replications \n \n\n \n\n \n\n \n\nDirect replications3 are usually defined as the repetition \nof  a  procedure,  whereas  conceptual replications  test  hy-\npotheses  or  results  of  existing  research  using  different \nmethods or participant populations (Open Science Collab-\noration, 2015; Schmidt, 2009). In principle, both forms of \nreplications can add valuable knowledge to the literature. A \nthird possibility combining the two types is to run a direct \nreplication  with  a  conceptual  replication  extension  added \nin  a  way  that  would  have  minimal  impact  on  the  direct \nreplication yet allow for a comparison of the two types of \nreplication. For example, Korbmacher et al. (2022) ran a di-\nrect replication of Kruger\xe2\x80\x99s (1999) above and below average \neffects which manipulated difficulty in a within-subject de-\nsign,  with  an  added  extension  on  top  of  that  introducing \na manipulation in a between-subject design, resulting in a \nmixed design demonstrating the strength, robustness, and \ngeneralizability of the phenomenon. Chen et al. (2021) re-\nport a combination of two direct replications (Studies 1 and \n2) and one additional conceptual replication (Study 3 build-\ning on the same design as in Study 1) within the same arti-\ncle. \n\nA replication and extension design is especially helpful \nin disentangling the cause for failure in finding support for \na  novel  contribution,  whether  the  failure  has  to  do  with \nthe  novelty  added  or  the  replicability  of  the  underlying \nphenomenon. Extensions can also add nuances to what is \nknown, address potential weaknesses in the original design, \nor  explore  new  directions  that  would  advance  the  litera-\nture beyond the original\xe2\x80\x99s. For example, when Jakob et al. \n(2019)  replicated  an  investigation  of  the  relationship  be-\ntween membership in the fictional, fraternity-like Hogwarts \nhouses  from  the  Harry  Potter  franchise  and  psychometric \nmeasures of personality, the authors identified the concept \nof basic human values to be more in line with the descrip-\ntion  of  these  houses  found  in  the  books,  and  added  this \nmeasurement to their questionnaire. \n\nIn addition to planning these technical aspects, it is also \nhelpful  to  take  a  step  back  and  consider  the  implications \nof  the  study  design  and  its  potential  outcomes.  Let\xe2\x80\x99s  as-\nsume  an  effect  replicates  in  the  country  where  the  origi-\nnal study was conducted, in a neighbouring country as well, \nbut not in a third country, or across a specific set of states. \nThe  study  would  then  look  into  not  just  whether  an  ef-\nfect replicates, but also its generalisability. The two compet-\ning hypotheses here would be consistency in the effect vs. \nvariation. This variability in specific countries can add nu-\nance and allow for comparison of populations, even with-\nout adding extensions. For example, Hornsey et al. (2018) \nprovide country-specific insights regarding political ideol-\nogy and scepticism about climate change, highlighting how \nthese relationships were stronger in the USA compared to \nthe rest of the world. In a similar fashion, other studies can \n\nreveal  how  country  or  cultural  differences  might  mediate \nthe effect of interventions, as shown for social discounting \n(Tiokhin et al., 2019). As such, multi-country replications \ncan  be  both  replications  and  investigations  into  general-\nisability, showing either variation or robustness of a phe-\nnomenon  across  countries,  or  even  means  of  data  collec-\ntion (e.g. near-identical results across platforms Mturk and \nProlific, see Brick et al., 2021; Chandrashekar et al., 2022; \nEfendi\xc4\x87 et al., 2022). \n\nSecurity and safety of collaborators and \n \n \nparticipants  \n\n \n\n \n\n \n\n \n\nSometimes, a research topic may be incredibly valuable \nand relevant to both researchers and participants, but also \ntoo taboo in a country to study safely. This could involve \nresearch  eliciting  people\xe2\x80\x99s  opinions  on  sensitive  political, \nreligious, health, social/moral issues, or sexuality, and can \nreach from these topics being frowned upon to being out-\nright illegal to discuss in public. Even in situations where \ngoverning bodies or the laws of a country are not of con-\ncern,  the  community  of  a  participant  or  collaborator  may \nexact  punishment  if  sensitive  information  is  somehow \nmade accessible. Consulting local collaborators in an infor-\nmal manner (if possible) before deciding to go ahead with \nthe replication is of utmost importance here, and if safety \nconcerns are found, the best approach is to not conduct the \nstudy  in  that  particular  area.  The  security  of  participants \nand local researchers always comes before anything else. \n\nIncluding Underrepresented Populations \n \n\n \n\n \n\nPsychological research often lacks the inclusion of his-\ntorically  underrepresented  populations  (Thalmayer  et  al., \n2021).  However,  their  inclusion  is  key  when  trying  to  in-\nvestigate phenomena globally. While researchers appear to \nbe generally motivated to recruit collaborators and partic-\nipants  from  outside  more  traditionally  represented  popu-\nlations (e.g. those often summarised as western, educated, \nindustrialised,  rich  and  democratic  [WEIRD],  see  also  Ar-\nnett, 2008), they face a number of obstacles that limit the \nfeasibility  and  perceived  benefits  of  doing  so.  Below  are \nsome prevalent examples of such obstacles, along with sug-\ngestions for addressing them. \n\nFunding  \n\nReaching  out  to  potential  collaborators  from  different \ngeographic areas often incurs costs that researchers are un-\nprepared  to  bear.  These  may  include  costs  for  hiring  and \ntraining local research assistants, high-quality translation \nof  materials,  travel  associated  with  training  and  meeting \ncollaborators, and incentivising participants. When multi-\ncountry efforts are not funded with an explicitly cross-cul-\n\n3 \n\nNote that even a \xe2\x80\x9cdirect\xe2\x80\x9d replication is not exactly identical to the original investigation. They are usually as close as possible, with ide-\nally only inevitable differences present (e.g. a new sample). Brandt et al. (2014) argue that \xe2\x80\x9cclose\xe2\x80\x9d replications would be a more fitting \nterm. \n\nCollabra: Psychology\n\n5\n\nl\n\nD\no\nw\nn\no\na\nd\ne\nd\n\n \nf\nr\no\nm\nh\n\n \n\nt\nt\n\np\n\n:\n/\n/\n\no\nn\n\nl\ni\n\n.\n\nn\ne\nu\nc\np\nr\ne\ns\ns\n.\ne\nd\nu\n/\nc\no\n\nl\nl\n\n/\n\na\nb\nr\na\na\nr\nt\ni\nc\ne\n-\np\nd\n\nl\n\nf\n/\n\n/\n\n/\n\n/\n\n8\n1\n5\n7\n5\n3\n8\n7\n6\n7\n4\n2\n7\n/\nc\no\n\nl\nl\n\na\nb\nr\na\n_\n2\n0\n2\n2\n_\n8\n_\n1\n_\n5\n7\n5\n3\n8\np\nd\n\n.\n\nf\n \n\nb\ny\n \ng\nu\ne\ns\nt\n \n\n \n\no\nn\n0\n4\n \nJ\na\nn\nu\na\nr\ny\n \n2\n0\n2\n3\n\n\x0cA Roadmap to Large-Scale Multi-Country Replications in Psychology\n\ntural  approach  in  mind,  it  can  be  difficult  for  researchers \nto  globalise  their  samples  at  later  stages.  In  some  cases, \neven well-funded studies designed to document cross-cul-\ntural phenomena can be difficult to implement because of \nrestrictions from funding agencies requiring that purchases \nbe made from companies based in the home country of the \nfunder.  Based  on  discussions  in  our  panels  and  some  lit-\nerature (Azouaghe et al., 2020; Silan et al., 2021), funding \nappears to be the largest obstacle to achieving more glob-\nally  representative  samples  in  multi-country  replications. \nWhere the collaborations included here have made efforts \nto  address  these,  they  are  summarised  in  Table  2  (When \nconsulting this table, please note that the PI for JRP pro-\njects  is  not  in  the  current  authorship.  Out  of  respect  for \nthe personal and professional sacrifices that the individual \nmade toward the work, and that there will be an additional \npublication  with  extensive  detail  from  the  PI,  we  provide \nonly brief information here). Consider also that the use of \nexpensive software may provide a barrier to participating in \na collaborative project for institutions that do not have nec-\nessary licenses. It is worth exploring if institutions are able \nto  share  licenses  or  provide  project-based  access,  or  con-\nsidering open-source alternatives with bespoke edits made \nthrough expert collaborators. \n\n \n\n \n\nData vs. collaborators from underrepresented \n \ngroups  \n\n \n\n \n\nWhere  funding  is  not  a  limiting  factor,  it  is  beneficial \nto  approach  cross-country  replications  with  the  mindset \nof  not  simply  obtaining  underrepresented  or  non-WEIRD \ndata, but rather involving collaborators (researchers, clini-\ncians, trainees, etc.) from underrepresented groups and re-\ngions  in  fundamental  stages  of  the  research  process,  ide-\nally  starting  with  the  planning  phase.  This  approach  can \nhelp to broaden perspectives and mitigate potential power \nimbalances that may otherwise exist when lead researchers \nfrom high-resource institutions seek out collaborators from \nlower-resourced ones (Silan et al., 2021). \n\nPart of making a study valuable to any group of people is \napproaching the topic of replication from all angles, includ-\ning the local perspectives of researchers from all collaborat-\ning regions. Trying to conduct a replication in a vastly dif-\nferent culture than that of the original study may result in \nweaker replication findings, not due to issues in the meth-\nods,  false  positive  findings,  or  a  challenge  to  the  central \ntheory/phenomenon,  but  rather  because  that  theory/phe-\nnomenon works differently in a different context in which \nthe  replication  is  conducted.  For  example,  while  conspir-\nacy  beliefs  have  been  found  to  mediate  the  link  between \npolitical ideology and risk perception in Americans, the ef-\nfects seem to be much weaker in India (Puthillam & Kapoor, \n2021). Conversely, a successful replication in a novel popu-\nlation may or may not mean that the studied phenomenon \noccurs in the same way across countries\xe2\x80\x92it might also be the \nresult of non-representative sampling. \n\nInvolving local collaborators from the outset of the pro-\nject  not  only  demonstrates  consideration  and  respect  but \nalso benefits the research by ensuring that design decisions \nare cross-culturally informed. One option is to establish an \n\n\xe2\x80\x9coutreach committee\xe2\x80\x9d within the research group or consor-\ntium,  that  specialises  in  finding  and  contacting  potential \ncollaborators  for  specific  tasks  and  studies.  The  PSA  uses \ntheir community building and network expansion commit-\ntee to find ways to engage with all members of the network \n(i.e.  social  events,  conferences/seminars,  slack  threads, \netc.) which in turn allows the committee to connect on a \nnatural basis. For conceptual replications, inspiration could \nalso be taken from the STRATEQ-2 project (Dujols & IJz-\nerman, 2021), where researchers started developing an in-\nternational  questionnaire  by  first  asking  researchers  from \ndifferent countries around the globe to generate items rel-\nevant to the phenomenon at hand to gather what fits the \nrespective cultural context. \n\nConsidering study value for participants and \n \n \ncollaborators  \n\n \n\n \n\n \n\n \n\nThe ability to recruit participants and collaborators will \nlikely depend on how valuable a study\xe2\x80\x99s topic is to the local \npopulation it aims to sample (Silan et al., 2021). When con-\nsidering a study replication, researchers are advised to con-\nsider  (and  ideally  consult  local  collaborators  about)  how \nrelevant the topic might be in different populations, as this \nmight heavily influence participation rates or interest from \npotential  collaborators.  For  instance,  replicating  a  study \nabout  financial  choices  relating  to  health  insurance  costs \nmight be of interest to US citizens but relatively inconse-\nquential to someone in a country with universal healthcare \nprovided by the state. Researchers can then either adapt the \nreplication to better suit their target populations or, with \nthe help of local collaborators, find ways to convey or aug-\nment the value and benefits (direct or indirect) of the re-\nsearch to potential participants. In the case of large stud-\nies requiring community investment or designed to impact \ncommunities through intervention, it may be most appro-\npriate  to  involve  community  leaders  and  local  stakehold-\ners in ongoing conversations about both costs and benefits \nof the study. In determining adequate compensation, it is \nadvisable  to  consult  researchers  and  personnel  with  local \nknowledge, especially if a population may be vulnerable for \neconomic or social reasons. \n\nStrengths-based vs. deficit-based framing of \n \ncross-population comparisons \n \n\n \n\n \n\n \n\n \n\n \n\nAn important aspect of taking local perspectives into ac-\ncount is to frame research questions in a way that will ad-\nequately contextualise the study\xe2\x80\x99s results and allow room \nfor the exploration of phenomena that may vary from those \nseen in the original study. One way of accomplishing this \nwould be to use a strengths-based (as opposed to a deficit-\nbased) model when comparing the original population with \nthose from multi-country replications. Instead of viewing \ndivergences  in  results  as  an  indication  of  one  population \nperforming better at a task or better understanding a theory \nthan the other population, it would be more meaningful to \ninvestigate  how  differing  skill  sets,  values,  goals,  and  in-\ncentives play into how different populations interact with \nthe  concepts  being  studied.  As  discussed  in  the  previous \n\nCollabra: Psychology\n\n6\n\nl\n\nD\no\nw\nn\no\na\nd\ne\nd\n\n \nf\nr\no\nm\nh\n\n \n\nt\nt\n\np\n\n:\n/\n/\n\no\nn\n\nl\ni\n\n.\n\nn\ne\nu\nc\np\nr\ne\ns\ns\n.\ne\nd\nu\n/\nc\no\n\nl\nl\n\n/\n\na\nb\nr\na\na\nr\nt\ni\nc\ne\n-\np\nd\n\nl\n\nf\n/\n\n/\n\n/\n\n/\n\n8\n1\n5\n7\n5\n3\n8\n7\n6\n7\n4\n2\n7\n/\nc\no\n\nl\nl\n\na\nb\nr\na\n_\n2\n0\n2\n2\n_\n8\n_\n1\n_\n5\n7\n5\n3\n8\np\nd\n\n.\n\nf\n \n\nb\ny\n \ng\nu\ne\ns\nt\n \n\n \n\no\nn\n0\n4\n \nJ\na\nn\nu\na\nr\ny\n \n2\n0\n2\n3\n\n\x0cA Roadmap to Large-Scale Multi-Country Replications in Psychology\n\nsection, certain concepts or relationships may not be sta-\ntistically significant in some populations; take for instance \nconspiratorial  blame  and  political  ideology  as  laid  out  by \nPuthillam  and  Kapoor  (2021)  \xe2\x80\x93  a  researcher  could  either \nlook at this phenomenon through a deficit-based lens (e.g. \nthe comparison population has less of a grasp on conspir-\nacy theories than the original sample, or they\xe2\x80\x99re less \xe2\x80\x98clued \ninto\xe2\x80\x99/exposed to international media), or they could use a \nstrengths-based  approach  to  explore  why  the  comparison \npopulation is not incentivized or motivated to link conspir-\natorial blame to their political ideologies. What aspects of \ntheir context (daily life challenges, governmental structure, \netc.) might counterbalance their (potential) inclination to \nengage with certain conspiracy theories? \n\nMethodological Approaches \n \n\n \n\nEven for conceptual replications, the general methodol-\nogy is often at least partially based on the methodology of \nthe original study. However, there are several methodologi-\ncal aspects that require extra attention and adaptation due \nto the cross-cultural nature of multi-country replications, \nand  these  come  with  some  challenges  to  consider  before \nembarking on such a study. Also, note that multi-country \ncollaborations focussing on existing research and materials \ncan take various forms, including the validation of metrics \nin different countries and cultures, or a revalidation to see \nif an instrument measures the same construct in different \npopulations (see also Ruggeri et al., 2019). \n\nPre-registering your project \n \n\n \n\n \n\nPre-registration is always advisable in confirmatory re-\nsearch  (Munaf\xc3\xb2  et  al.,  2017)  but  especially  important  in \nmulti-country  replication  studies,  where  adaptations  are \nlikely needed for different countries. Where possible, this \nmay  take  the  form  of  a  registered  report:  a  publishing \nmodel in which a journal first approves a study protocol and \nlater  the  full  report  with  results  and  conclusions,  includ-\ning potential deviations from the original plan. While reg-\nistered reports are no deus ex machina to solve issues with \nreproducibility, initial evidence supports that they promote \nreproducibility,  transparency,  and  self-correction  across \ndisciplines as intended (Chambers & Tzavella, 2021; Scheel \net al., 2021) and that they are perceived as having greater \nresearch quality and rigour compared to the standard pub-\nlishing  model  (Soderberg  et  al.,  2021).  However,  consider \nalso  that  publishing  a  registered  report  can  impact  the \ntimeline of your project, as review at this early stage can \ntake a long time and may delay the start of your study. If \nthis a concern\xe2\x80\x94or where materials need to remain private \nbefore  the  conclusion  of  data  collection\xe2\x80\x94a  time-stamped \npre-registration  (private  or  public)  on  platforms  such  as \nthe OSF can ensure that any deviations from the planned \nmethodology are traceable upon publication. \n\nEthics  \n\nApplying  for  ethical  approval  can  be  challenging  when \noperating in multiple countries. In general, there are two \n\noptions: a centralised approval from the PI\xe2\x80\x99s Institutional \nReview  Board  (IRB)  or  approval  from  the  PI\xe2\x80\x99s  institution \nand at least one IRB per country. It is also possible that in-\nstitutions  or  journals  require  approval  from  each  institu-\ntion\xe2\x80\x99s IRB. Having only one approval is obviously the eas-\nier  approach  but  may  not  always  be  possible.  IRBs  may \nrefuse such a wide-ranging approval because their members \nare not able to review materials in all languages involved \nor cannot assess data privacy matters for all countries. In \nour  experience,  contributors  should  at  least  sign  a  letter \nconfirming their commitment to upholding procedures ex-\nactly as outlined in the procedural plans submitted to the \nIRB. Where approvals from multiple IRBs are needed, time-\nlines should account for the potential of considerable de-\nlays so that data collection can commence in all countries \naround the same time, as global events may otherwise in-\nterfere with the validity of data collected at different time \npoints. In the experience of PSA, approvals can take differ-\nent forms and may take any time between just one week and \nnine months to be finalised, which might even lead to aban-\ndoning plans for a country. However, having obtained ap-\nproval from one institution can often speed up the process \nas it may provide another IRB with extra confidence where \nother colleagues have already approved it. This should also \nbe considered when making the choice of a private pre-reg-\nistration versus registered report, as a journal may require \nall ethical approvals before consideration, which can cause \ndelays in the project timeline. Further, you can find your-\nself in a situation where one IRB requires a change in ap-\nproach (e.g., dropping a culturally sensitive question). This \nimpact will then have to be reconsidered from a method-\nological perspective. \n\nIn  addition  to  these  administrative  aspects,  there  are \nspecific ethical considerations to keep in mind when plan-\nning research in multiple countries. Primarily, this means \ndiscussing  cultural  aspects  with  the  local  collaborators  in \ndetail  and  also  involving  local  stakeholders  for  their  ap-\nproval. Especially when the research requires intense time \ncommitment, the consortium should consider what to pro-\nvide in return. For example, linguists commonly create dic-\ntionaries based on their work and share them with partic-\nipants.  Further,  the  age  of  consent  for  participation  may \ndiffer between countries and IRBs may require a different \nage minimum, an aspect which makes sense to be checked \nwith local collaborators early. \n\nTranslation and adaptation of materials \n \n\n \n\n \n\n \n\n \n\nAnother time-consuming aspect of multi-country stud-\nies is the translation of study materials. It may take mul-\ntiple  iterations  and  checks  to  confirm  that  materials  are \nnot only translated, but also that the translated text is per-\nceived  as  intended  in  the  base  version  of  the  materials. \nThe forward- and back-translation protocol (Brislin, 1970) \nseems  to  be  the  most  commonly  utilised  method  in  psy-\nchological research and requires at least one bilingual, as \nwell  as  additional  native  speakers  of  the  target  language. \nIdeally, the forward- and back-translation should be com-\nplemented  by  methods  that  can  test  the  conceptual  (i.e., \nconstruct  has  a  comparable  meaning  in  all  countries/cul-\n\nCollabra: Psychology\n\n7\n\nl\n\nD\no\nw\nn\no\na\nd\ne\nd\n\n \nf\nr\no\nm\nh\n\n \n\nt\nt\n\np\n\n:\n/\n/\n\no\nn\n\nl\ni\n\n.\n\nn\ne\nu\nc\np\nr\ne\ns\ns\n.\ne\nd\nu\n/\nc\no\n\nl\nl\n\n/\n\na\nb\nr\na\na\nr\nt\ni\nc\ne\n-\np\nd\n\nl\n\nf\n/\n\n/\n\n/\n\n/\n\n8\n1\n5\n7\n5\n3\n8\n7\n6\n7\n4\n2\n7\n/\nc\no\n\nl\nl\n\na\nb\nr\na\n_\n2\n0\n2\n2\n_\n8\n_\n1\n_\n5\n7\n5\n3\n8\np\nd\n\n.\n\nf\n \n\nb\ny\n \ng\nu\ne\ns\nt\n \n\n \n\no\nn\n0\n4\n \nJ\na\nn\nu\na\nr\ny\n \n2\n0\n2\n3\n\n\x0cTable 2.  Actions regarding funding and including under-represented populations by different consortia so far. \n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\nA Roadmap to Large-Scale Multi-Country Replications in Psychology\n\nJRP \n\nPSA \n\nManyBabies \n\nCORE \n\nISM \n\nFunding \n\nStudies are conducted online; PI \nprovides access to survey; \ncentralised budget is provided to \npay data collection services if \nnot all data can be obtained by \ncollaborators; collaborators \nwork on their own time \n\nFunding has been limited but \ndoes come up with a few of \nour projects for populations \nthat are underrepresented \nor for translators. Surveys \nare typically run with an \nonline link and funding for \nparticipants are most likely \nfunded by the lab\xe2\x80\x99s funds or \nresources. \n\nFunding has been very limited \nso far. Single PI used his own \nseed and internal funding to \nconduct all data collections \nonline using online labour \nmarkets. \n\nSurvey-based online \nresearch; PI \nprovides access to \nsurvey; funds are \navailable to offset \ndata collection \ncosts and \nparticipant \nincentives \n\nCollaborators \nfrom \nunderrepresented \ngroups \n\nValue for \nparticipants and \ncollaborators \n\nExisting network is explored for \ntrusted colleagues from as many \ncountries as possible; the more \nthe network grows, the more \ngrows the outreach and \ncolleagues from more countries \ncan be included (First study: 32 \ncollaborators and data from 19 \ncountries; latest: 169 authors \nfrom 61 countries included) \n\nParticipants: Surveys are short \n(5 minutes or less), minimal time \ninvestment \xe2\x80\x93 results are often \nshared via the same channels \nparticipants were recruited; \nCollaborators: Access to \nnetwork of collaborators (option \nof further connection), \nopportunity to re-use data for \nadditional research (e.g. country \nspecific sub-analyses, or \nconnecting data to new research \nquestion); for ECR \xe2\x80\x93 training \n(adaptation of materials, work \nwith Qualtrics, data collection) \n\nThe PSA is a Distributed \nLaboratory Network which \nmeans we have and recruit \ncollaborators from any and \nall Geopolitical regions. Our \nnetwork continues to grow \nand at the moment we have \nover 1300 researchers \nacross 83 geopolitical \nregions.  \n\nParticipants: Value for \nparticipants varies for each \nproject. Value can be \nmonetary, knowledge based, \nand credit. Participants are \nalso shared results through \nways in which are \nunderstandable and public. \nCollaborators: The PSA \nallows for any career level to \njoin and get involved in an \naspect of the study to gain \nauthorship, experience, and \nnew skill sets. Collaborators \nalso gain connections and \n\nProject funding varies across the MB \ninitiatives, but several have received \nfunding through scientific agencies or \nthrough internal grants from a PI\xe2\x80\x99s home \ninstitution. Funds have been awarded to \npromote large collaborative projects and \nsupport capacity-building by supplying \nnecessary training and equipment (e.g., \nlaptops, speakers, software, ethical \nreview, and a training workshop in the \ncase of MB1-A, for labs that had not \npreviously run infant looking-time \nstudies). In many cases, however, \ncollaborating labs opt in to projects that \nare consistent with existing grants and \nmethods in the lab, and do not require \nadditional funding. \n\nAny researcher or lab can join an MB \nproject; across the 16 MB studies to date, \nthere are 490 collaborators from 45 \ncountries on 6 continents, including 191 \nin the Americas, 230 in Europe, 21 in \nAfrica, 34 in Asia, and 15 in Oceania. \n\nThe student body has been \nbased at Hong Kong, \nextending to collaborations \nwith early career researchers \nfrom around the world. \n\nThe families of infant participants are \noften compensated financially or given \nsmall gifts (e.g., books, toys) for \nparticipation. \nCollaborators benefit from training in \nexperimental and analytic methods, open \naccess to materials and data, authorship \non publications, funding to attend \nprofessional conferences, and developing \nrelationships with a large international \nnetwork of colleagues. \n\nFor replication and extension \nprojects early career \nresearchers are invited to \njoin as lead authors over \nprojects started with \nstudents in course and thesis \nwork. \nFor the developed \ncollaborative resources, any \nmeaningful contribution is \nacknowledged and likely to \nlead to coauthorship. \n\nl\n\nD\no\nw\nn\no\na\nd\ne\nd\n\n \nf\nr\no\nm\nh\n\n \n\nt\nt\n\np\n\n:\n/\n/\n\no\nn\n\nl\ni\n\n.\n\nn\ne\nu\nc\np\nr\ne\ns\ns\n.\ne\nd\nu\n/\nc\no\n\nl\nl\n\n/\n\na\nb\nr\na\na\nr\nt\ni\nc\ne\n-\np\nd\n\nl\n\nf\n/\n\n/\n\n/\n\n/\n\n8\n1\n5\n7\n5\n3\n8\n7\n6\n7\n4\n2\n7\n/\nc\no\n\nl\nl\n\na\nb\nr\na\n_\n2\n0\n2\n2\n_\n8\n_\n1\n_\n5\n7\n5\n3\n8\np\nd\n\n.\n\nf\n \n\nb\ny\n \ng\nu\ne\ns\nt\n \n\n \n\no\nn\n0\n4\n \nJ\na\nn\nu\na\nr\ny\n \n2\n0\n2\n3\n\nResearchers and \nparticipants from \n57 countries, \nincluding 7 African \ncountries, 10 \nAmerican countries, \n18 Asian countries, \n21 European \ncountries, and \nAustralia. \n\nParticipants: Value \nfor participants \ncould be monetary \nor knowledge-\nbased. \nCollaborators: \nAccess to network \nof collaborators in \nthe same area of \nresearch, \nopportunity to re-\nuse data for \nadditional research \n(e.g. country specific \nsub-analyses, or \n\nCollabra: Psychology\n\n8\n\n\x0cA Roadmap to Large-Scale Multi-Country Replications in Psychology\n\nand support from country team \nmembers in conducting the \nstudy \n\naccess to researchers across \nthe world. \nOther people: The PSA uses \nopen material practices and \ntherefore allows everyone \nto have access to our \nresearch methods and data. \nValue to the field is shown \nby contributing large and \ngeneralizable data sets for \nstudies. \n\nN/A \n\nStrengths-based \napproach \n\nJRP encourages country-specific \nanalyses, examples can be found \nin the appendix of Ruggeri et al. \n(2020) \n\nN/A \n\nBy involving scientists from a diverse \nrange of research sites in all project \nstages, MB provides a platform for \nperspectives that are often \nunderrepresented within scientific \ndiscourse. MB projects have generally \ninterpreted differential performance \nacross participants and populations as \nevidence for the many paths that typical \ncognitive development may take. \n\nconnecting data to \nnew research \nquestion), co-\nauthorship on all \nsubsequent papers \nemerging from the \nmain dataset. \n\nISM encourages \ncountry-level \nanalyses and \ncontextualisation of \nmeta-norms in \ncontemporary times \n(e.g., a follow-up \nstudy on COVID-19 \nrelated meta-\nnorms) \n\nl\n\nD\no\nw\nn\no\na\nd\ne\nd\n\n \nf\nr\no\nm\nh\n\n \n\nt\nt\n\np\n\n:\n/\n/\n\no\nn\n\nl\ni\n\n.\n\nn\ne\nu\nc\np\nr\ne\ns\ns\n.\ne\nd\nu\n/\nc\no\n\nl\nl\n\n/\n\na\nb\nr\na\na\nr\nt\ni\nc\ne\n-\np\nd\n\nl\n\nf\n/\n\n/\n\n/\n\n/\n\n8\n1\n5\n7\n5\n3\n8\n7\n6\n7\n4\n2\n7\n/\nc\no\n\nl\nl\n\na\nb\nr\na\n_\n2\n0\n2\n2\n_\n8\n_\n1\n_\n5\n7\n5\n3\n8\np\nd\n\n.\n\nf\n \n\nb\ny\n \ng\nu\ne\ns\nt\n \n\n \n\no\nn\n0\n4\n \nJ\na\nn\nu\na\nr\ny\n \n2\n0\n2\n3\n\nCollabra: Psychology\n\n9\n\n\x0cA Roadmap to Large-Scale Multi-Country Replications in Psychology\n\ntures),  instrument  (i.e.,  construct  is  operationalized  in  a \nsimilar way across countries/cultures), item (i.e., construct \ncan  be  measured  with  the  same  instrument  across  coun-\ntries/cultures), and scalar (i.e., construct can be assessed on \nthe same metric) equivalence of an instrument to meaning-\nfully compare different cultures/countries (Hui & Triandis, \n1985). \n\nDespite the prevalence of the forward- and back-trans-\nlation  method,  evidence  that  this  method  yields  the  best \nresults  is  lacking  (Epstein  et  al.,  2015).  While  some  re-\nsearchers  consider  back-translation  indispensable,  others \nargue  that  it  does  not  necessarily  guarantee  equivalence \nand  linguistic  appropriateness  of  an  instrument  (Behr, \n2017) and therefore advise against it (Epstein et al., 2015). \nIn addition, there is no standardised method of translating \nresearch material in psychology (Cha et al., 2007; Epstein \net  al.,  2015).  This  poses  an  issue  as  the  back-translation \nprocess requires several people with different backgrounds/\nexpertise levels, making it difficult to achieve a truly valid \ntranslation where the target language is very rare or in re-\ngions  where  psychological  research  may  not  have  a  large \npresence. In these cases, it may be worth considering utilis-\ning a combination of translation techniques, depending on \nthe resources and personnel available. \n\nThe  PSA  uses  a  variation  of  back-translation  as  its  of-\nficial method and also appoints language coordinators for \neach study. \n\nThe PSA method uses two translators, and goes as fol-\n\nlows: \n\n1.  First bilingual translator translates from original lan-\n\nguage into target language. \n\n2.  Second  bilingual  translator  back-translates  the  first \n\ntranslator\xe2\x80\x99s work into the original language. \n\n3.  The two versions are discussed, and a third version is \n\nmade with changes. \n\n4.  External  non-academic  readers  fluent  in  target  lan-\n\nguage read the third version and give feedback. \n\n5.  Final  version  is  created  based  on  cultural  feedback \n\nfrom external readers. \n\nWhen  following  the  back-translation  method,  focusing \non the feedback of non-academic speakers of the target lan-\nguage  is  crucial,  as  they  are  ensuring  that  materials  are \njargon-free,  make  practical/social  (not  just  grammatical) \nsense in their cultural contexts, and are easy to follow for \neveryone. To put it bluntly: When your colleague tells you \nyour  questionnaire  is  fine,  but  your  grandma  tells  you  it \ndoes  not  make  sense,  your  grandma  is  right.4  Sometimes \nsuch  feedback  may  require  changes  to  very  fundamental \nparts of the materials, such as items in a scale that do not \napply or make sense in a particular culture. Another issue \nto  consider  is  adapting  materials  to  local  dialects  or  ver-\nsions rather than leaving them in the \xe2\x80\x98standard\xe2\x80\x99 version of \nthe language (such as Austrian German vs. Standard High \nGerman)  -  an  extra  measure  to  ensure  cultural  validity. \n\nWhether such an adaptation is adequate should be evalu-\nated by a local contributor. Additionally, in countries with \nmultiple  major  languages  where  people  may  be  fluent  in \nmore than one, discussions to decide which languages the \nmaterials will be translated to are helpful. \n\nGiven the culturally dependent and, at times, subjective \nnature of translation, transparency in the process is para-\nmount. One of the easiest ways to maintain transparency \nis to keep track of changes made to materials through the \ntranslation process, and comment on the rationale behind \nthose changes. This can be done in an informal way at first \n(such  as  commenting  on  a  Word  document  or  using  the \nTrack Changes feature), and later refined into an easily di-\ngestible format (such as a table with every major change to \neach material; see Supplementary materials in Ruggeri et \nal., 2020, 2021). \n\nEspecially  when  replicating  older  studies,  adaptations \nto the base materials are sometimes needed. For example, \nstudies including financial decisions need to be adapted to \nthe local currency, but also to inflation, and anchored to in-\ncome levels of the country (for an example, see the appen-\ndix of Ruggeri et al., 2020). Similarly, the Zeitgeist might \nmake adaptation of materials necessary, as encountered in \nWagenmakers  and  colleagues\xe2\x80\x99  (2016)  attempt  at  replicat-\ning  experiments  underlying  the  facial  feedback  hypothe-\nsis, where the pictures used in the original study to evoke \nlaughs were deemed not funny anymore. \n\nData Collection \n \n\n \n\nThe process of collecting and storing data from multiple \ncountries differs across consortia, with some collecting data \ncentrally  (i.e.,  the  PI  saves  data  from  multiple  countries \ninto their own database) and others collecting data through \nindividual researchers or labs (i.e., collaborators store the \ndata they collected in their countries, and then pass it on \nto the PIs). Both, national data protection laws and ethics \ncommittees,  determine  some  aspects  of  how  data  is  sup-\nposed to be collected and stored. While these determinants \nprovide guidance on establishing a data collection protocol, \nthe  multitude  of  regulations  also  limits  how  data  can  be \ncollected  and  shared  and  may  require  differences  in  how \ndata is collected and stored from country to country, which \ncan cause methodological inconsistencies. \n\nData Privacy \n \n\n \n\nData privacy should be considered in parallel to ethical \nissues before applying to an IRB. IRB applications can be \ncomplicated  when  multiple  countries  with  different  data \nprotection laws are involved. It is advisable to think about \ndata protection from a general ethical perspective first and \ntry to adhere to this set of standards throughout the pro-\nject, even if the law in some countries may not require it. In \nmost aspects, the EU\xe2\x80\x99s General Data Protection Regulation \n\nl\n\nD\no\nw\nn\no\na\nd\ne\nd\n\n \nf\nr\no\nm\nh\n\n \n\nt\nt\n\np\n\n:\n/\n/\n\no\nn\n\nl\ni\n\n.\n\nn\ne\nu\nc\np\nr\ne\ns\ns\n.\ne\nd\nu\n/\nc\no\n\nl\nl\n\n/\n\na\nb\nr\na\na\nr\nt\ni\nc\ne\n-\np\nd\n\nl\n\nf\n/\n\n/\n\n/\n\n/\n\n8\n1\n5\n7\n5\n3\n8\n7\n6\n7\n4\n2\n7\n/\nc\no\n\nl\nl\n\na\nb\nr\na\n_\n2\n0\n2\n2\n_\n8\n_\n1\n_\n5\n7\n5\n3\n8\np\nd\n\n.\n\nf\n \n\nb\ny\n \ng\nu\ne\ns\nt\n \n\n \n\no\nn\n0\n4\n \nJ\na\nn\nu\na\nr\ny\n \n2\n0\n2\n3\n\n4 \n\nThe first author\xe2\x80\x99s grandmother agrees with this assessment. \n\nCollabra: Psychology\n\n10\n\n\x0cA Roadmap to Large-Scale Multi-Country Replications in Psychology\n\n(GDPR)  appears  to  be  the  strictest  law  in  that  sense.  For \nthose coming in contact with the GDPR for the first time, \nit is important to know that it only applies if personal data \nare collected, but is not applicable if individuals are not di-\nrectly identifiable from the answers in a research endeavour \n(i.e. the data is pseudonymised, see Recital 26). When col-\nlecting  data  online,  researchers  should  pay  special  atten-\ntion not to collect IP addresses or geolocation of respon-\ndents (as these may make participants identifiable). GDPR \nalso  usually  does  not  apply  to  data  collection  in  research \ncontexts (Article 89), so the applicability should be checked \nin  advance  \xe2\x80\x93  where  possible  with  the  help  of  a  legal  ad-\nvisor or data privacy expert, such as the institution\xe2\x80\x99s Data \nProtection Officer. As GDPR is sometimes poorly understood \nand interpreted differently by IRBs, both outside and inside \nEurope,  it  is  advisable  to  plan  additional  time  to  clarify \nthese issues. Independent of which data laws apply, collect-\ning non-personal and anonymised data is always the least \nlikely to cause problems and avoids having to add compli-\ncated and long notices that can scare people off. If personal \nidentifiable  data  are  involved,  researchers  may  also  need \nto set up Data Sharing Agreements between the consortium \npartners (these regulate how the collected data are shared \nbetween partners and processed), or even Data Privacy Im-\npact Assessments (a process to identify and minimise data \nprotection risks which is necessary when processing sensi-\ntive data on a large scale). As of 2022, this also largely still \napplies to the UK, as the Data Protection Act 2018 is the UK\xe2\x80\x99s \nadaptation of GDPR. Note that, depending on the category \nof  data  concerned,  countries  will  often  have  specific  laws \npertaining solely to those categories (e.g., HIPAA or FERPA \nin the US). Based on GDPR categories and the authors\xe2\x80\x99 ex-\nperiences, the following categories of data always warrant \nspecial attention to details and protection and will there-\nfore be more complicated to handle: \n\n\xe2\x80\xa2  Racial or ethnic origin \n\xe2\x80\xa2  Health or genetic data \n\xe2\x80\xa2  Financial data \n\xe2\x80\xa2  Philosophical, political, or religious beliefs \n\xe2\x80\xa2  Sexual orientation \n\xe2\x80\xa2  Trade union membership \n\xe2\x80\xa2  Educational data / IQ scores \n\xe2\x80\xa2  Data from underaged individuals, especially children \n\xe2\x80\xa2  Data from closed networks, where the combination of \n\nanswers may reveal identities \n\nStatistical Approach \n \n\n \n\nIf you ask multiple data analysts what analysis to choose \nfor  your  research  project,  chances  are  high  you  will  find \nyourself with many different replies (Menkveld et al., 2021; \nSilberzahn et al., 2018). However, there are multiple aspects \nthat  should  always  be  considered  when  choosing  the  sta-\ntistical approach to a multi-country replication. For direct \nreplications, studies should be powered to detect an effect \nat least equal to the one found in the original study, ideally \neven  smaller  \xe2\x80\x93  if  there  is  no  existing  cross-cultural  com-\nparison, it is possible that the effect you are investigating \nmay be smaller in some of the target countries. More gen-\n\nerally, due to the combination of publication bias and small \nsample sizes, reported effect sizes in the literature tend to \nbe inflated (Gelman & Carlin, 2014; Lane & Dunlap, 1978). \nDue to this \xe2\x80\x9cwinner\xe2\x80\x99s curse\xe2\x80\x9d, it is advisable to power repli-\ncations  to  a  smaller  effect  size  than  the  one  reported  in \nthe literature (e.g., using the small telescopes approach; Si-\nmonsohn, 2015). Where resources are constrained, sequen-\ntial analyses can also be considered: This approach, com-\nmonly used in medical trials (where stakes for participants \nare  high),  uses  interim  analyses  to  observe  if  a  sufficient \nsample has been reached while controlling for Type-1 error \nrate (Lakens, 2014). \n\nIn addition, researchers should take into consideration \nwhether it is advantageous and feasible to power the study \nfor individual regions or groups. One option is to replicate \nthe  effect  in  one  country  only  before  opting  for  a  multi-\ncountry  replication.  That  is  specifically  useful  when  the \noriginal evidence may be old and conducted on small sam-\nples. It is also an option to repeat the original analysis (of-\nten frequentist) and additionally analyse the data with an \nequivalent Bayesian analysis. In any case, the statistical ap-\nproach(es) should be part of the pre-registration, and any \nadditional, exploratory analyses should be labelled as such. \nTo estimate the necessary sample size per country it should \nbe considered whether the approach allows for a multi-level \nstructure. If so, the number of countries and the variation \nof the effect at that level should be taken into considera-\ntion. \n\nFor  conceptual  replications,  assessing  sample  size  is \nmore  complicated.  Assumptions  on  effect  sizes  should  be \nas conservative as possible, yet might yield unrealistically \nlarge  minimum  sample  sizes.  Lakens  (2021)  provides  de-\ntailed  discussions  on  different  approaches  and  highlights \nthat,  depending  on  the  justification  for  a  sample  size,  it \nshould be considered \xe2\x80\x9c1) what the smallest effect size of in-\nterest  is,  2)  which  minimal  effect  size  will  be  statistically \nsignificant, 3) which effect sizes they expect (and what they \nbase these expectations on), 4) which effect sizes would be \nrejected  based  on  a  confidence  interval  around  the  effect \nsize, 5) which ranges of effects a study has sufficient power \nto  detect  based  on  a  sensitivity  power  analysis,  and  6) \nwhich effect sizes are plausible in a specific research area\xe2\x80\x9d \n(p. 1). When powering your study, you should also consider \nthe practical implications of the effect size you are aiming \nfor: while an extremely small but significant effect may pro-\nvide insight, its practical relevance may be nil (G\xc3\xb6tz et al., \n2021; see also reply by Primbs et al., 2021). While we would \nclearly not advise against research for the sole purpose of \ngaining knowledge in general, practical relevance is surely a \nkey consideration for such large-scale endeavours where re-\nsources could be used to investigate potentially more ben-\neficial matters. Anvari and colleagues (2021) provide an in-\nsightful discussion on this matter. \n\nReporting and Disseminating Findings \n \n\n \n\n \n\n \n\nOnce the study is completed, it is time for the exciting \npart of sharing the results with the world. The unique ad-\nvantage in communicating results of a multi-country repli-\ncation  lies  in  the  opportunity  to  1)  share  results  in  every \n\nCollabra: Psychology\n\n11\n\nl\n\nD\no\nw\nn\no\na\nd\ne\nd\n\n \nf\nr\no\nm\nh\n\n \n\nt\nt\n\np\n\n:\n/\n/\n\no\nn\n\nl\ni\n\n.\n\nn\ne\nu\nc\np\nr\ne\ns\ns\n.\ne\nd\nu\n/\nc\no\n\nl\nl\n\n/\n\na\nb\nr\na\na\nr\nt\ni\nc\ne\n-\np\nd\n\nl\n\nf\n/\n\n/\n\n/\n\n/\n\n8\n1\n5\n7\n5\n3\n8\n7\n6\n7\n4\n2\n7\n/\nc\no\n\nl\nl\n\na\nb\nr\na\n_\n2\n0\n2\n2\n_\n8\n_\n1\n_\n5\n7\n5\n3\n8\np\nd\n\n.\n\nf\n \n\nb\ny\n \ng\nu\ne\ns\nt\n \n\n \n\no\nn\n0\n4\n \nJ\na\nn\nu\na\nr\ny\n \n2\n0\n2\n3\n\n\x0cA Roadmap to Large-Scale Multi-Country Replications in Psychology\n\nauthor\xe2\x80\x99s native language and 2) highlight country-specific \nfindings that might fall short of attention in the shadow of \nthe  overall  findings  of  the  paper.  It  can  be  advantageous \nto put a dissemination plan in writing early, to avoid scat-\ntered communication around results. All collaborators may \ninform their universities and institutions or faculty, who of-\nten provide the opportunity to write a blog post or news ar-\nticle. Sharing results before peer review has become fairly \ncommon  but  also  raises  concerns  about  research  being \nshared uncritically, as most people may not be able to dif-\nferentiate  between  peer-reviewed  research  and  preprints. \nThis concern was confirmed by Wingen et al. (2022), who \nexperimentally  showed  that  a  brief  explanation  can  help \nclarify this issue. As such, if you opt for a preprint publica-\ntion, we recommend both clearly marking your preprint as \nsuch, as well as adding the explanation developed by Win-\ngen and colleagues. In addition, it is good practice to ensure \nthat all materials are accessible. Collecting information and \nconducting studies is costly \xe2\x80\x93 it is important to make this \ndata available to achieve the maximum benefit of your work \nand  resources,  so  that  others  may  answer  additional  re-\nsearch questions. The PSA has even incentivised secondary \nanalysis,  by  challenging  researchers  to  work  with  one  of \ntheir large datasets, offering monetary rewards (Forscher et \nal., 2019). \n\nConcluding Remarks \n \n\n \n\nLarge-scale multi-country replications are not the most \nstraightforward  or  easiest  research  endeavours.  Yet  they \ncome with large benefits such as comparable data from dif-\nferent countries, and datasets which\xe2\x80\x94if curated well\xe2\x80\x94can \nbe the source of future insights. While there is clearly no \none-size-fits-all  approach,  we  hope  that  the  lessons  we \nlearned and summarised in this paper will be helpful at all \nsteps of planning future multi-country replications. How-\never,  when  approaching  your  multi-country  replication, \nkeep  in  mind  to  plan  the  protocol  and  responsibilities  of \ncollaborators well in advance and listen closely to your col-\nlaborators\xe2\x80\x99 insights of their own countries, identify poten-\ntial pitfalls, and make sure everyone\xe2\x80\x99s safety is guaranteed. \nLikewise, share your knowledge about your own region or \ncountry. Your study also does not need to solve every ques-\ntion there is \xe2\x80\x93 a simple effect or theory is much more real-\nistic to test at scale and makes it easier to provide tangible \ninsights from the observations. Likewise, extensions should \nbe equally straightforward. \n\nWorking  on  a  large  scale  with  colleagues  from  many \ncountries  can  be  a  challenging  but  enriching  experience \nand provides collaborators with research expertise and in-\nsights  into  how  scientists  operate  in  other  parts  of  the \nworld.  These  differences  allow  for  additional  perspectives \nand a more holistic view of phenomena, but they also re-\nquire clear guidelines on communication channels and re-\nsponsibilities  (ideally  provided  in  a  way  that  is  easily  ac-\ncessible \nincludes  using \ncommunication  tools  that  are  accessible  to  every  partner \nin the consortium. Simple and prevalent communication is \nkey to the success of every project employing Big Team Sci-\nence. However, it should also be considered that commu-\n\nto  all  collaborators.)  This \n\nnication tools may not always take into account their us-\nability for researchers with disabilities. In order to ensure \nthat colleagues with disabilities can contribute without ad-\nditional hurdles, such factors should be explored before the \nstart of a project and not be made the responsibility of po-\ntential contributors. \n\nLastly, it is our impression that despite the differences \nin approaches, all large-scale multi-country endeavours we \nhave been part of have one thing in common: the motiva-\ntion to conduct such studies mostly stems from a drive to \nproduce solid research that can help improve people\xe2\x80\x99s lives. \nMulti-lab approaches may facilitate a mindset-shift in how \nresearch  is  conducted,  where  instead  of  operating  in  si-\nlos and potentially competing for publication spots in jour-\nnals,  a  collaborative  approach  allowing  for  different  per-\nspectives  within  one  research  project  allows  for  not  only \nmore productive discussions where everyone has the same \ngoal,  but  also  leads  to  more  nuanced  insights.  Including \nstudents in such initiatives can further help to support de-\nvelopments  towards  more  collaborations  of  this  kind  and \nprovide valuable early research experience and network op-\nportunities. On the other hand, journals will also have to \nconsider how to best provide impartial reviews as experts \nfor  such  collaborations  will  become  more  and  more  con-\nnected to each other and it will become increasingly diffi-\ncult to find editors who are knowledgeable in multi-coun-\ntry  replications  but  without  ties  to  author  consortia  that \noften include more than a hundred researchers. While we \nhope  that  our  insights  and  checklist  are  helpful  in  con-\nducting your multi-country replication, we also encourage \nresearchers  to  build  meaningful  and  lasting  relationships \nwith their project partners, moving towards a methodolog-\nically  sound,  more  collaborative,  and  inclusive  field.  We \nwould also like to encourage researchers who have created, \nor know of, additional materials for approaching this topic \nto get in touch and add resources or links to the accompa-\nnying online repository (https://osf.io/xrv5p/). \n\nAcknowledgements  \n\nThe  authors  representing  JRP  would  like  to  thank  Kai \nRuggeri for his leadership and extensive work in the pro-\njects described and for providing this opportunity to early \ncareer researchers. \n\nContributions  \n\nHJ, SAV, and SJG have conceptualised and organised the \npanel discussions that form the basis of this paper. SA, LB, \nAC, GF, EGG, HK, SL, BV, and JZ were speakers at these pan-\nels  (moderated  by  HJ  and  SAV)  and  contributed  their  in-\nsights. ALT and TLA compiled notes at these sessions that \ninformed the writing of this manuscript. HJ structured the \ninsights gained and led the writing of the first draft, sup-\nported by SAV and SJG. HJ supervised the administrative as-\npects of this project. All authors contributed to the writing \nand revision of this manuscript. SAV is overseeing the up-\ndating of the accompanying OSF repository. \n\nCollabra: Psychology\n\n12\n\nl\n\nD\no\nw\nn\no\na\nd\ne\nd\n\n \nf\nr\no\nm\nh\n\n \n\nt\nt\n\np\n\n:\n/\n/\n\no\nn\n\nl\ni\n\n.\n\nn\ne\nu\nc\np\nr\ne\ns\ns\n.\ne\nd\nu\n/\nc\no\n\nl\nl\n\n/\n\na\nb\nr\na\na\nr\nt\ni\nc\ne\n-\np\nd\n\nl\n\nf\n/\n\n/\n\n/\n\n/\n\n8\n1\n5\n7\n5\n3\n8\n7\n6\n7\n4\n2\n7\n/\nc\no\n\nl\nl\n\na\nb\nr\na\n_\n2\n0\n2\n2\n_\n8\n_\n1\n_\n5\n7\n5\n3\n8\np\nd\n\n.\n\nf\n \n\nb\ny\n \ng\nu\ne\ns\nt\n \n\n \n\no\nn\n0\n4\n \nJ\na\nn\nu\na\nr\ny\n \n2\n0\n2\n3\n\n\x0cA Roadmap to Large-Scale Multi-Country Replications in Psychology\n\nSubmitted: August 26, 2022 PST, Accepted: November 28, 2022 \nPST \n\nFunding Information \n \n\n \n\nNo specific funding has been received for this work. \n\nCompeting Interests \n \n\n \n\nThe authors declare no competing interests. \n\nl\n\nD\no\nw\nn\no\na\nd\ne\nd\n\n \nf\nr\no\nm\nh\n\n \n\nt\nt\n\np\n\n:\n/\n/\n\no\nn\n\nl\ni\n\n.\n\nn\ne\nu\nc\np\nr\ne\ns\ns\n.\ne\nd\nu\n/\nc\no\n\nl\nl\n\n/\n\na\nb\nr\na\na\nr\nt\ni\nc\ne\n-\np\nd\n\nl\n\nf\n/\n\n/\n\n/\n\n/\n\n8\n1\n5\n7\n5\n3\n8\n7\n6\n7\n4\n2\n7\n/\nc\no\n\nl\nl\n\na\nb\nr\na\n_\n2\n0\n2\n2\n_\n8\n_\n1\n_\n5\n7\n5\n3\n8\np\nd\n\n.\n\nf\n \n\nb\ny\n \ng\nu\ne\ns\nt\n \n\n \n\no\nn\n0\n4\n \nJ\na\nn\nu\na\nr\ny\n \n2\n0\n2\n3\n\nThis is an open-access article distributed under the terms of the Creative Commons Attribution 4.0 International License \n(CCBY-4.0). View this license\xe2\x80\x99s legal deed at http://creativecommons.org/licenses/by/4.0 and legal code at http://creativecom-\nmons.org/licenses/by/4.0/legalcode for more information. \n\nCollabra: Psychology\n\n13\n\n\x0cA Roadmap to Large-Scale Multi-Country Replications in Psychology\n\nReferences  \n\nAllen, L., O\xe2\x80\x99Connell, A., & Kiermer, V. (2019). How can \n\nBrick, C., Fillon, A., Yeung, S., Wang, M., Lyu, H., Ho, J., \n\nwe ensure visibility and diversity in research \ncontributions? How the Contributor Role Taxonomy \n(CRediT) is helping the shift from authorship to \ncontributorship. Learned Publishing, 32(1), 71\xe2\x80\x9374. htt\nps://doi.org/10.1002/leap.1210 \n\nAnand-Vembar, S. (chair), Alzahawi, S., Carstensen, A., \nGarcia-Garzon, E., Kapoor, H., Lewis, S., Geiger, S. J., \n& Jarke, H. (2021, August 22). Approaches to Multi-\ncountry Replications in Psychology and Behavioural \nSciences \xe2\x80\x93 Session Two (of Two) [Panel discussion]. \n10th Junior Researcher Programme Conference, \nOnline, UK. https://youtu.be/lGFi2Dp_Hp4 \n\nAnvari, F., Kievit, R., Lakens, D., Pennington, C. R., \n\nPrzybylski, A. K., Tiokhin, L., Wiernik, B. M., & \nOrben, A. (2021). Evaluating the practical relevance \nand significance of observed effect sizes in psychological \nresearch. https://doi.org/10.31234/osf.io/g3vtr \n\nAnvari, F., & Lakens, D. (2018). The replicability crisis \n\nand public trust in psychological science. \nComprehensive Results in Social Psychology, 3(3), \n266\xe2\x80\x93286. https://doi.org/10.1080/23743603.2019.168\n4822 \n\nAnvari, F., Olsen, J., Hung, W., & Feldman, G. (2021). \nMisprediction of affective outcomes due to different \nevaluation modes: Replication and extension of two \ndistinction bias experiments by Hsee and Zhang \n(2004). Journal of Experimental Social Psychology, 92, \n104052. https://doi.org/10.1016/j.jesp.2020.104052 \nArnett, J. J. (2008). The neglected 95%: Why American \n\npsychology needs to become less American. American \nPsychologist, 63(7), 602\xe2\x80\x93614. https://doi.org/10.1037/\n0003-066x.63.7.602 \n\nAzouaghe, S., Adetula, A., Forscher, P. S., Basnight-\n\nBrown, D., Ouherrou, N., Charyate, A., & IJzerman, \nH. (2020). Psychology and open science in Africa: Why \nis it needed and how can we implement it? https://doi.o\nrg/10.31730/osf.io/ke7ub \n\nBehr, D. (2017). Assessing the use of back translation: \n\nThe shortcomings of back translation as a quality \ntesting method. International Journal of Social \nResearch Methodology, 20(6), 573\xe2\x80\x93584. https://doi.or\ng/10.1080/13645579.2016.1252188 \n\nBeshears, J., Gjoneska, B., Schmidt, K., Pfuhl, G., Saari, \n\nT., McAuliffe, W. H. B., Steltenpohl, C. N., Onie, S., \nChartier, C. R., & Moshontz, H. (2020). Psychological \nScience Accelerator: A promising resource for clinical \npsychological science. https://doi.org/10.31234/osf.io/q\nwy4k \n\nBrandt, M. J., IJzerman, H., Dijksterhuis, A., Farach, F. \n\nJ., Geller, J., Giner-Sorolla, R., Grange, J. A., Perugini, \nM., Spies, J. R., & van \xe2\x80\x99t Veer, A. (2014). The \nReplication Recipe: What makes for a convincing \nreplication? Journal of Experimental Social Psychology, \n50(1), 217\xe2\x80\x93224. https://doi.org/10.1016/j.jesp.2013.1\n0.005 \n\nWong, S., & Feldman, G. (2021). Self-interest is \noverestimated: Two successful pre-registered \nreplications of Miller and Ratner (1998). Collabra: \nPsychology, 7(1), 23443. https://doi.org/10.1525/collab\nra.23443 \n\nBrislin, R. W. (1970). Back-translation for cross-cultural \n\nresearch. Journal of Cross-Cultural Psychology, 1(3), \n185\xe2\x80\x93216. https://doi.org/10.1177/1359104570001003\n01 \n\nByers-Heinlein, K., Bergmann, C., Davies, C., Frank, M. \nC., Hamlin, J. K., Kline, M., Kominsky, J. F., Kosie, J. \nE., Lew-Williams, C., Liu, L., Mastroberardino, M., \nSingh, L., Waddell, C. P. G., Zettersten, M., & \nSoderstrom, M. (2020). Building a collaborative \npsychological science: Lessons learned from \nManyBabies 1. Canadian Psychology / Psychologie \ncanadienne, 61(4), 349\xe2\x80\x93363. https://doi.org/10.1037/c\nap0000216 \n\nByers-Heinlein, K., Tsui, A. S. M., Bergmann, C., Black, \nA. K., Brown, A., Carbajal, M. J., Durrant, S., Fennell, \nC. T., Fi\xc3\xa9vet, A.-C., Frank, M. C., Gampe, A., Gervain, \nJ., Gonzalez-Gomez, N., Hamlin, J. K., Havron, N., \nHernik, M., Kerr, S., Killam, H., Klassen, K., \xe2\x80\xa6 \nWermelinger, S. (2021). A multi-lab study of bilingual \ninfants: Exploring the preference for infant-directed \nspeech. Advances in Methods and Practices in \nPsychological Science, 4(1), 1\xe2\x80\x9330. https://doi.org/10.11\n77/2515245920974622 \n\nCha, E. S., Kim, K. H., & Erlen, J. A. (2007). Translation \n\nof scales in cross-cultural research: Issues and \ntechniques. Journal of Advanced Nursing, 58(4), \n386\xe2\x80\x93395. https://doi.org/10.1111/j.1365-2648.2007.04\n242.x \n\nChambers, C. D., & Tzavella, L. (2021). The past, \n\npresent and future of Registered Reports. Nature \nHuman Behaviour, 6(1), 29\xe2\x80\x9342. https://doi.org/10.103\n8/s41562-021-01193-7 \n\nChandrashekar, S., Adelina, N., Zeng, S., Chiu, Y., \nLeung, Y., Henne, P., Cheng, B., & Feldman, G. \n(2022). Defaults versus framing: Revisiting Default \nEffect and Framing Effect with replications and \nextensions of Johnson and Goldstein (2003) and \nJohnson, Bellman, and Lohse (2002). Pre-print. http\ns://osf.io/krsf2/ \n\nChandrashekar, S., Yeung, S., Yau, K., Cheung, C., \n\nAgarwal, T. K., Wong, C., Pillai, T., Thirlwell, T. N., \nLeung, W., Li, Y., Tse, C., Cheng, B., Chan, H., & \nFeldman, G. (2021). Agency and self-other \nasymmetries in perceived bias and shortcomings: \nReplications of the Bias Blind Spot and extensions \nlinking to free will beliefs. Judgment and Decision \nMaking, 16(6), 1392\xe2\x80\x931413. \n\nl\n\nD\no\nw\nn\no\na\nd\ne\nd\n\n \nf\nr\no\nm\nh\n\n \n\nt\nt\n\np\n\n:\n/\n/\n\no\nn\n\nl\ni\n\n.\n\nn\ne\nu\nc\np\nr\ne\ns\ns\n.\ne\nd\nu\n/\nc\no\n\nl\nl\n\n/\n\na\nb\nr\na\na\nr\nt\ni\nc\ne\n-\np\nd\n\nl\n\nf\n/\n\n/\n\n/\n\n/\n\n8\n1\n5\n7\n5\n3\n8\n7\n6\n7\n4\n2\n7\n/\nc\no\n\nl\nl\n\na\nb\nr\na\n_\n2\n0\n2\n2\n_\n8\n_\n1\n_\n5\n7\n5\n3\n8\np\nd\n\n.\n\nf\n \n\nb\ny\n \ng\nu\ne\ns\nt\n \n\n \n\no\nn\n0\n4\n \nJ\na\nn\nu\na\nr\ny\n \n2\n0\n2\n3\n\nCollabra: Psychology\n\n14\n\n\x0cA Roadmap to Large-Scale Multi-Country Replications in Psychology\n\nEfendi\xc4\x87, E., Chandrashekar, S., Cheong, S., Yeung, L., \n\nHui, C. H., & Triandis, H. C. (1985). Measurement in \n\nChen, J., Kwan, L. C., Ma, L. Y., Choi, H. Y., Lo, Y. C., Au, \n\nS. Y., Tsang, C. H., Cheng, B. L., & Feldman, G. \n(2021). Retrospective and prospective hindsight bias: \nReplications and extensions of Fischhoff (1975) and \nSlovic and Fischhoff (1977). Journal of Experimental \nSocial Psychology, 96, 104154. https://doi.org/10.1016/\nj.jesp.2021.104154 \n\nCollaborative Open-science REsearch. (2022). \n\nReplications and extensions of classic findings in Social \nPsychology and Judgment and Decision Making. htt\np://osf.io/5z4a8 \n\nDujols, O., & IJzerman, H. (2021, January 8). Creating a \npsychological tool applicable to individuals around the \nworld (STRAEQ-2 item generation and selection phase \n[Blog post]. https://corelab.blog/creating-a-psycholog\nical-tool-applicable-to-individuals-around-the-worl\nd-straeq-2-items-generation-and-selection-phas\ne/%26sa=D%26source=docs%26ust=164495447044437\n1%26usg=AOvVaw09PqgKtM_lPV84MLT2NHl3 \n\nEbersole, C. R., Klein, R. A., & Atherton, O. E. (2014). \n\nThe Many Lab. Online Project Repository. https://osf.i\no/89vqh/ \n\nKim, M., Lee, C., \xe2\x80\xa6 Feldman, G. (2022). Risky \ntherefore not beneficial: Replication and extension of \nFinucane et al. (2000)\xe2\x80\x99s Affect Heuristic experiment. \nSocial Psychological and Personality Science. https://do\ni.org/10.1177/19485506211056761 \n\nEpstein, J., Osborne, R. H., Elsworth, G. R., Beaton, D. \nE., & Guillemin, F. (2015). Cross-cultural adaptation \nof the Health Education Impact Questionnaire: \nExperimental study showed expert committee, not \nback-translation, added value. Journal of Clinical \nEpidemiology, 68(4), 360\xe2\x80\x93369. https://doi.org/10.101\n6/j.jclinepi.2013.07.013 \n\nEriksson, K., Strimling, P., Gelfand, M., Wu, J., \n\nAbernathy, J., Akotia, C. S., Aldashev, A., Andersson, \nP. A., Andrighetto, G., Anum, A., Arikan, G., Aycan, \nZ., Bagherian, F., Barrera, D., Basnight-Brown, D., \nBatkeyev, B., Belaus, A., Berezina, E., Bj\xc3\xb6rnstjerna, \nM., \xe2\x80\xa6 Van Lange, P. A. M. (2021). Perceptions of the \nappropriate response to norm violation in 57 \nsocieties. Nature Communications, 12(1), 1481. http\ns://doi.org/10.1038/s41467-021-21602-9 \n\nForscher, P. S., DeBruine, L. M., Jones, B. C., Flake, J. K., \n\nColes, N. A., & Chartier, C. R. (2019, September 1). \nIntroducing the PSA001 Secondary Analysis Challenge. \nhttps://psysciacc.org/2019/09/01/introducing-the-psa\n001-secondary-analysis-challenge/ \n\nForscher, P. S., Wagenmakers, E.-J., Coles, N. A., Silan, \n\nM. A. A., Dutra, N. B., Basnight-Brown, D., & \nIJzerman, H. (2020). The Benefits, Barriers, and Risks \nof Big Team Science. Preprint. https://doi.org/10.3123\n4/osf.io/2mdxh \n\nGelman, A., & Carlin, J. (2014). Beyond power \n\ncalculations: Assessing type S (sign) and type M \n(magnitude) errors. Perspectives on Psychological \nScience, 9(6), 641\xe2\x80\x93651. https://doi.org/10.1177/17456\n91614551642 \n\nG\xc3\xb6tz, F. M., Gosling, S. D., & Rentfrow, P. J. (2021). \nSmall effects: The indispensable foundation for a \ncumulative psychological science. Perspectives on \nPsychological Science, 17(1), 205\xe2\x80\x93215. https://doi.org/\n10.1177/1745691620984483 \n\nGrahe, J., Brandt, M., Wagge, J., Legate, N., Wiggins, B., \nChristopherson, C., Weisberg, Y., Corker, K., Chartier, \nC., Fallon, M., Hildebrandt, L., Hurst, M., Lazarevic, \nL., Levitan, C., McFall, J., McLaughlin, H., Pazda, A., \nIJzerman, H., Nosek, B., \xe2\x80\xa6 Adetula, A. (2013). \nCollaborative Replications and Education Project \n(CREP). Open Science Framework. https://doi.org/10.1\n7605/OSF.IO/WFC6U \n\nHolcombe, A. O., Kovacs, M., Aust, F., & Aczel, B. \n(2020). Documenting contributions to scholarly \narticles using CRediT and tenzing. PLoS ONE, 15(12), \n1\xe2\x80\x9311. https://doi.org/10.1371/journal.pone.0244611 \n\nHornsey, M. J., Harris, E. A., & Fielding, K. S. (2018). \n\nRelationships among conspiratorial beliefs, \nconservatism and climate scepticism across nations. \nNature Climate Change, 8(7), 614\xe2\x80\x93620. https://doi.or\ng/10.1038/s41558-018-0157-2 \n\ncross-cultural psychology: A review and comparison \nof strategies. Journal of Cross-Cultural Psychology, \n16(2), 131\xe2\x80\x93152. https://doi.org/10.1177/00220021850\n16002001 \n\nIsager, P. M., van Aert, R. C. M., Bahn\xc3\xadk, \xc5\xa0., Brandt, M. \n\nJ., DeSoto, K. A., Giner-Sorolla, R., Krueger, J., \nPerugini, M., Ropovik, I., van \xe2\x80\x99t Veer, A. E., Vranka, \nM. A., & Lakens, D. (2020). Deciding what to replicate: \nA decision model for replication study selection under \nresource and knowledge constraints. Preprint. https://d\noi.org/10.31222/osf.io/2gurz \n\nJakob, L., Garcia-Garzon, E., Jarke, H., & Dablander, F. \n(2019). The Science Behind the Magic? The Relation \nof the Harry Potter \xe2\x80\x9cSorting Hat Quiz\xe2\x80\x9d to Personality \nand Human Values. Collabra: Psychology, 5(1), 31. htt\nps://doi.org/10.1525/collabra.240D \n\nJarke, H. (2021, June 24). The Junior Researcher \n\nProgramme - An initiative providing opportunities to \nearly career researchers in psychology [Lightning \nTalk]. Annual Conference of the Society for the \nImprovement of Psychological Science (SIPS) 2021, \nOnline. https://osf.io/s82qp/ \n\nJarke, H. (chair), Bojani\xc4\x87, L., Feldman, G., Ve\xc4\x87kalov, B., \nZickfeld, J., Geiger, S. J., & Anand-Vembar, S. (2021, \nAugust 22). Approaches to Multi-country Replications \nin Psychology and Behavioural Sciences \xe2\x80\x93 Session One \n(of Two) [Panel discussion]. 10th Junior Researcher \nProgramme Conference, Online, UK. https://youtu.b\ne/CHlgMCFTj9c \n\nKahneman, D., & Tversky, A. (1979). Prospect Theory: \nAn Analysis of Decision under Risk. Econometrica: \nJournal of the Econometric Society, 47(3), 263\xe2\x80\x93291. htt\nps://doi.org/10.1111/j.1536-7150.2011.00774.x \n\nl\n\nD\no\nw\nn\no\na\nd\ne\nd\n\n \nf\nr\no\nm\nh\n\n \n\nt\nt\n\np\n\n:\n/\n/\n\no\nn\n\nl\ni\n\n.\n\nn\ne\nu\nc\np\nr\ne\ns\ns\n.\ne\nd\nu\n/\nc\no\n\nl\nl\n\n/\n\na\nb\nr\na\na\nr\nt\ni\nc\ne\n-\np\nd\n\nl\n\nf\n/\n\n/\n\n/\n\n/\n\n8\n1\n5\n7\n5\n3\n8\n7\n6\n7\n4\n2\n7\n/\nc\no\n\nl\nl\n\na\nb\nr\na\n_\n2\n0\n2\n2\n_\n8\n_\n1\n_\n5\n7\n5\n3\n8\np\nd\n\n.\n\nf\n \n\nb\ny\n \ng\nu\ne\ns\nt\n \n\n \n\no\nn\n0\n4\n \nJ\na\nn\nu\na\nr\ny\n \n2\n0\n2\n3\n\nCollabra: Psychology\n\n15\n\n\x0cA Roadmap to Large-Scale Multi-Country Replications in Psychology\n\nKlein, R. A., Ratliff, K. A., Vianello, M., Adams, R. B., \n\nPashler, H., & Wagenmakers, E. (2012). Editors\xe2\x80\x99 \n\nJr., Bahn\xc3\xadk, \xc5\xa0., Bernstein, M. J., Bocian, K., Brandt, M. \nJ., Brooks, B., Brumbaugh, C. C., Cemalcilar, Z., \nChandler, J., Cheong, W., Davis, W. E., Devos, T., \nEisner, M., Frankowska, N., Furrow, D., Galliani, E. \nM., \xe2\x80\xa6 Nosek, B. A. (2014). Investigating variation in \nreplicability: A \xe2\x80\x9cmany labs\xe2\x80\x9d replication project. Social \nPsychology, 45(3), 142\xe2\x80\x93152. https://doi.org/10.1027/1\n864-9335/a000178 \n\nKorbmacher, M., Kwan, C., & Feldman, G. (2022). Both \nbetter and worse than others depending on difficulty: \nReplication and extensions of Kruger\xe2\x80\x99s (1999) above and \nbelow average effects. https://osf.io/7yfkc/ \n\nKruger, J. (1999). Lake Wobegon be gone! The \xe2\x80\x9cbelow-\n\naverage effect\xe2\x80\x9d and the egocentric nature of \ncomparative ability judgments. Journal of Personality \nand Social Psychology, 77(2), 221\xe2\x80\x93232. https://doi.org/\n10.1037/0022-3514.77.2.221 \n\nLakens, D. (2014). Refinements and New Developments: \n\nPerforming High-Powered Studies Efficiently With \nSequential Analyses. European Journal of Social \nPsychology, 44(7), 701\xe2\x80\x93710. https://doi.org/10.1002/ej\nsp.2023 \n\nLakens, D. (2021). Sample Size Justification. https://doi.o\n\nrg/10.31234/osf.io/9d3yf \n\nLane, D. M., & Dunlap, W. P. (1978). Estimating effect \nsize: Bias resulting from the significance criterion in \neditorial decisions. British Journal of Mathematical \nand Statistical Psychology, 31(2), 107\xe2\x80\x93112. https://do\ni.org/10.1111/j.2044-8317.1978.tb00578.x \n\nManyBabies Consortium. (2020). Quantifying sources of \n\nvariability in infancy research using the infant-\ndirected speech preference. Advances in Methods and \nPractices in Psychological Science, 3(1), 24\xe2\x80\x9352. http\ns://doi.org/10.1177/2515245919900809 \n\nMenkveld, A. J., Dreber, A., Holzmeister, F., Huber, J., \n\nJohanneson, M., Kirchler, M., Razen, M., Weitzel, U., \nAbad, D., Abudy, M. (Meni), Adrian, T., Ait-Sahalia, \nY., Akmansoy, O., Alcock, J., Alexeev, V., Aloosh, A., \nAmato, L., Amaya, D., Angel, J. J., \xe2\x80\xa6 Bao, L. (2021). \nNon-Standard Errors. SSRN Electronic Journal. http\ns://doi.org/10.2139/ssrn.3961574 \n\nMoshontz, H., Campbell, L., Ebersole, C. R., Ijzerman, \nH., Urry, H. L., Forscher, P. S., Grahe, J. E., McCarthy, \nR. J., Musser, E. D., Antfolk, J., Castille, C. M., Evans, \nT. R., Fiedler, S., Flake, J. K., Forero, D. A., Janssen, S. \nM. J., Keene, J. R., Protzko, J., Aczel, B., \xe2\x80\xa6 Chartier, C. \nR. (2018). The psychological science accelerator: \nAdvancing psychology through a distributed \ncollaborative network. Advances in Methods and \nPractices in Psychological Science, 1(4), 501\xe2\x80\x93515. http\ns://doi.org/10.1177/2515245918797607 \n\nMunaf\xc3\xb2, M. R., Nosek, B. A., Bishop, D. V. M., Button, K. \n\nS., Chambers, C. D., Percie Du Sert, N., Simonsohn, \nU., Wagenmakers, E.-J., Ware, J. J., & Ioannidis, J. P. \nA. (2017). A manifesto for reproducible science. \nNature Human Behaviour, 1(1), 1\xe2\x80\x939. https://doi.org/1\n0.1038/s41562-016-0021 \n\nOpen Science Collaboration. (2015). Estimating the \nreproducibility of psychological science. Science, \n349(6251). https://doi.org/10.1126/science.aac4716 \n\nintroduction to the special section on replicability in \npsychological science: A crisis of confidence? \nPerspectives on Psychological Science, 7(6), 528\xe2\x80\x93530. ht\ntps://doi.org/10.1177/1745691612465253 \n\nPrimbs, M., Pennington, C. R., Lakens, D., Silan, M. A. \n\nA., Lieck, D. S. N., Forscher, P. S., Buchanan, E. M., & \nWestwood, S. J. (2021). Are Small Effects the \nIndispensable Foundation for a Cumulative \nPsychological Science? A Reply to G\xc3\xb6tz et al. (2022). htt\nps://doi.org/10.31234/osf.io/6s8bj \n\nPsychological Science Accelerator Self-Determination \n\nTheory Collaboration, Legate, N., Nguyen, T., \nWeinstein, N., Moller, A., Legault, L., Vally, Z., \nTajchman, Z., Zsido, A. N., Zrimsek, M., Chen, Z., \nZiano, I., Gialitaki, Z., Ceary, C. D., Jang, Y., Lin, Y., \nKunisato, Y., Yamada, Y., Xiao, Q., \xe2\x80\xa6 Primbs, M. A. \n(2022). A global experiment on motivating social \ndistancing during the COVID-19 pandemic. \nProceedings of the National Academy of Sciences, \n119(22), 2111091119. https://doi.org/10.1073/pnas.21\n11091119 \n\nPuthillam, A., & Kapoor, H. (2021). Does conspiratorial \n\nblame mediate the relationship between political \nideology and risk perception? Evidence from India and \nthe US. https://doi.org/10.31234/osf.io/frgzq \n\nRuggeri, K. (2020, May 19). The collaboration behind the \nreplication [Blog post]. https://socialsciences.nature.c\nom/posts/the-collaboration-behind-the-replication \nRuggeri, K., Al\xc3\xad, S., Berge, M. L., Bertoldo, G., Bj\xc3\xb8rndal, \nL. D., Cortijos-Bernabeu, A., Davison, C., Demi\xc4\x87, E., \nEsteban-Serna, C., Friedemann, M., Gibson, S. P., \nJarke, H., Karakasheva, R., Khorrami, P. R., Kveder, J., \nAndersen, T. L., Lofthus, I. S., McGill, L., Nieto, A. E., \n\xe2\x80\xa6 Folke, T. (2020). Replicating patterns of Prospect \nTheory for decision under risk. Nature Human \nBehaviour, 4(6), 622\xe2\x80\x93633. https://doi.org/10.1038/s41\n562-020-0886-x \n\nRuggeri, K., Bojani\xc4\x87, L., van Bokhorst, L., Jarke, H., \n\nMareva, S., Ojinaga-Alfageme, O., Mellor, D. T., & \nNorton, S. (2019). Editorial: Advancing methods for \npsychological assessment across borders. Frontiers in \nPsychology, 10, 503. https://doi.org/10.3389/fpsyg.201\n9.00503 \n\nRuggeri, K., Panin, A., Vdovic, M., Ve\xc4\x87kalov, B., Abdul-\n\nSalaam, N., Achterberg, J., Akil, C., Amatya, J., \nAmatya, K., Andersen, T. L., Aquino, S. D., \nArunasalam, A., Ashcroft-Jones, S., Askelund, A. D., \nAyacaxli, N., Sheshdeh, A. B., Bailey, A., Barea \nArroyo, P., Mej\xc3\xada, G. B., & Garc\xc3\xada-Garzon, E. (2022). \nThe globalizability of temporal discounting. Nature \nHuman Behaviour, 6. https://doi.org/10.1038/s4156\n2-022-01392-w \n\nRuggeri, K., Ve\xc4\x87kalov, B., Bojani\xc4\x87, L., Andersen, T. L., \nAshcroft-Jones, S., Ayacaxli, N., Barea-Arroyo, P., \nBerge, M. L., Bj\xc3\xb8rndal, L. D., Bursal\xc4\xb1o\xc4\x9flu, A., B\xc3\xbchler, \nV., \xc4\x8cadek, M., \xc3\x87etin\xc3\xa7elik, M., Clay, G., Cortijos-\nBernabeu, A., Damnjanovi\xc4\x87, K., Dugue, T. M., Esberg, \nM., Esteban-Serna, C., \xe2\x80\xa6 Folke, T. (2021). The \ngeneral fault in our fault lines. Nature Human \nBehaviour, 5(10), 1369\xe2\x80\x931380. https://doi.org/10.1038/\ns41562-021-01092-x \n\nCollabra: Psychology\n\n16\n\nl\n\nD\no\nw\nn\no\na\nd\ne\nd\n\n \nf\nr\no\nm\nh\n\n \n\nt\nt\n\np\n\n:\n/\n/\n\no\nn\n\nl\ni\n\n.\n\nn\ne\nu\nc\np\nr\ne\ns\ns\n.\ne\nd\nu\n/\nc\no\n\nl\nl\n\n/\n\na\nb\nr\na\na\nr\nt\ni\nc\ne\n-\np\nd\n\nl\n\nf\n/\n\n/\n\n/\n\n/\n\n8\n1\n5\n7\n5\n3\n8\n7\n6\n7\n4\n2\n7\n/\nc\no\n\nl\nl\n\na\nb\nr\na\n_\n2\n0\n2\n2\n_\n8\n_\n1\n_\n5\n7\n5\n3\n8\np\nd\n\n.\n\nf\n \n\nb\ny\n \ng\nu\ne\ns\nt\n \n\n \n\no\nn\n0\n4\n \nJ\na\nn\nu\na\nr\ny\n \n2\n0\n2\n3\n\n\x0cA Roadmap to Large-Scale Multi-Country Replications in Psychology\n\nScheel, A. M., Schijen, M. R. M. J., & Lakens, D. (2021). \nAn excess of positive results: Comparing the standard \nPsychology literature with Registered Reports. \nAdvances in Methods and Practices in Psychological \nScience, 4(2), 251524592110074. https://doi.org/10.11\n77/25152459211007467 \n\nSchmidt, S. (2009). Shall We Really Do It Again? The \n\nPowerful Concept of Replication Is Neglected in the \nSocial Sciences. Review of General Psychology, 13(2), \n90\xe2\x80\x93100. https://doi.org/10.1037/a0015108 \n\nSilan, M., Adetula, A., Basnight-Brown, D. M., Forscher, \n\nP. S., Dutra, N., & IJzerman, H. (2021, October 26). \nPsychological Science Needs the Entire Globe, Part 2 \nLet\xe2\x80\x99s Talk About the \xe2\x80\x9cC\xe2\x80\x9d Word: Colonialism and the \nChallenges of Psychological Science in the \nDeveloping World [Blog Post]. Aps Observer. https://w\nww.psychologicalscience.org/observer/psychological-\nscience-needs-the-entire-globe-part-2 \n\nSilberzahn, R., Uhlmann, E. L., Martin, D. P., Anselmi, \nP., Aust, F., Awtrey, E., Bahn\xc3\xadk, \xc5\xa0., Bai, F., Bannard, \nC., Bonnier, E., Carlsson, R., Cheung, F., Christensen, \nG., Clay, R., Craig, M. A., Rosa, A. D., Dam, L., Evans, \nM. H., Cervantes, I. F., \xe2\x80\xa6 Nosek, B. A. (2018). Many \nanalysts, one data set: Making transparent how \nvariations in analytic choices affect results. Advances \nin Methods and Practices in Psychological Science, 1(3), \n337\xe2\x80\x93356. https://doi.org/10.1177/2515245917747646 \n\nSimonsohn, U. (2015). Small telescopes: Detectability \n\nand the evaluation of replication results. \nPsychological Science, 26(5), 559\xe2\x80\x93569. https://doi.org/\n10.1177/0956797614567341 \n\nSoderberg, C. K., Errington, T. M., Schiavone, S. R., \n\nBottesini, J., Thorn, F. S., Vazire, S., Esterling, K. M., \n& Nosek, B. A. (2021). Initial evidence of research \nquality of registered reports compared with the \nstandard publishing model. Nature Human Behaviour, \n5(8), 990\xe2\x80\x93997. https://doi.org/10.1038/s41562-021-01\n142-4 \n\nThalmayer, A. G., Toscanelli, C., & Arnett, J. J. (2021). \n\nThe neglected 95% revisited: Is American psychology \nbecoming less American? American Psychologist, \n76(1), 116\xe2\x80\x93129. https://doi.org/10.1037/amp0000622 \n\nTiokhin, L., Hackman, J., Munira, S., Jesmin, K., & \n\nHruschka, D. (2019). Generalizability is not optional: \nInsights from a cross-cultural study of social \ndiscounting. Royal Society Open Science, 6(2), 181386. \nhttps://doi.org/10.1098/rsos.181386 \n\nTsui, A. S. M., Carstensen, A., Kachergis, G., Abubakar, \nA., Asnake, M., Barry, O., Basnight-Brown, D., Bentu, \nD., Bergmann, C., Binan Dami, E., Boll-Avetisyan, N., \nde Jongh, M., Diop, Y., Herrmann, E., Jang, C., Kizito, \nS., Lamba, T., Maliwichi-Senganimalunje, L., \nMarangu, J., \xe2\x80\xa6 Frank, M. C. (2021). Exploring \nvariation in infants\xe2\x80\x99 preference for infant-directed \nspeech: Evidence from a multi-site study in Africa. \nStage 1 RR accepted at Developmental Science. \n\nUhlmann, E. L., Ebersole, C. R., Chartier, C. R., \n\nErrington, T. M., Kidwell, M. C., Lai, C. K., McCarthy, \nR. J., Riegelman, A., Silberzahn, R., & Nosek, B. A. \n(2019). Scientific utopia III: Crowdsourcing science. \nPerspectives on Psychological Science, 14(5), 711\xe2\x80\x93733. \nhttps://doi.org/10.1177/1745691619850561 \n\nWagenmakers, E.-J., Beek, T., Dijkhoff, L., Gronau, Q. F., \nAcosta, A., Adams, R. B., Jr., Albohn, D. N., Allard, E. \nS., Benning, S. D., Blouin-Hudon, E.-M., Bulnes, L. C., \nCaldwell, T. L., Calin-Jageman, R. J., Capaldi, C. A., \nCarfagno, N. S., Chasten, K. T., Cleeremans, A., \nConnell, L., DeCicco, J. M., \xe2\x80\xa6 Zwaan, R. A. (2016). \nRegistered Replication Report. Perspectives on \nPsychological Science, 11(6), 917\xe2\x80\x93928. https://doi.org/\n10.1177/1745691616674458 \n\nWiggins, B. J., & Christopherson, C. D. (2019). The \nreplication crisis in psychology: An overview for \ntheoretical and philosophical psychology. Journal of \nTheoretical and Philosophical Psychology, 39(4), \n202\xe2\x80\x93217. https://doi.org/10.1037/teo0000137 \n\nWingen, T., Berkessel, J. B., & Dohle, S. (2022). Caution, \n\npreprint! Brief explanations allow non-scientists to \ndifferentiate between preprints and peer-reviewed \njournal articles. Advances in Methods and Practices in \nPsychological Science, 5(1), 251524592110705. http\ns://doi.org/10.1177/25152459211070559 \n\nWingen, T., Berkessel, J. B., & Englich, B. (2020). No \n\nreplication, no trust? How low replicability influences \ntrust in psychology. Social Psychological and \nPersonality Science, 11(4), 454\xe2\x80\x93463. https://doi.org/1\n0.1177/1948550619877412 \n\nZiano, I., Xiao, Q., Yeung, S. K., Wong, C. Y., Cheung, M. \n\nY., Lo, C. Y. J., Yan, H. C., Narendra, G. I., Kwan, L. \nW., Chow, C. S., Man, C. Y., & Feldman, G. (2021). \nNumbing or Sensitization? Replications and \nExtensions of Fetherstonhaugh et al. (1997)\xe2\x80\x99s \n\xe2\x80\x9cInsensitivity to the Value of Human Life.\xe2\x80\x9d Journal of \nExperimental Social Psychology, 97, 104222. https://do\ni.org/10.1016/j.jesp.2021.104222 \n\nl\n\nD\no\nw\nn\no\na\nd\ne\nd\n\n \nf\nr\no\nm\nh\n\n \n\nt\nt\n\np\n\n:\n/\n/\n\no\nn\n\nl\ni\n\n.\n\nn\ne\nu\nc\np\nr\ne\ns\ns\n.\ne\nd\nu\n/\nc\no\n\nl\nl\n\n/\n\na\nb\nr\na\na\nr\nt\ni\nc\ne\n-\np\nd\n\nl\n\nf\n/\n\n/\n\n/\n\n/\n\n8\n1\n5\n7\n5\n3\n8\n7\n6\n7\n4\n2\n7\n/\nc\no\n\nl\nl\n\na\nb\nr\na\n_\n2\n0\n2\n2\n_\n8\n_\n1\n_\n5\n7\n5\n3\n8\np\nd\n\n.\n\nf\n \n\nb\ny\n \ng\nu\ne\ns\nt\n \n\n \n\no\nn\n0\n4\n \nJ\na\nn\nu\na\nr\ny\n \n2\n0\n2\n3\n\nCollabra: Psychology\n\n17\n\n\x0cA Roadmap to Large-Scale Multi-Country Replications in Psychology\n\nSupplementary Materials \n \n\n \n\n \n\n \n\nPeer Review History \n \nDownload: https://collabra.scholasticahq.com/article/57538-a-roadmap-to-large-scale-multi-country-replications-in-\npsychology/attachment/123028.docx?auth_token=KvOiqA6SMDFrubXeqHxa \n\n \n\nAppendix 1 \n \nDownload: https://collabra.scholasticahq.com/article/57538-a-roadmap-to-large-scale-multi-country-replications-in-\npsychology/attachment/123030.docx?auth_token=KvOiqA6SMDFrubXeqHxa \n\n \n\nAppendix 2 \n \nDownload: https://collabra.scholasticahq.com/article/57538-a-roadmap-to-large-scale-multi-country-replications-in-\npsychology/attachment/123193.docx?auth_token=KvOiqA6SMDFrubXeqHxa \n\nl\n\nD\no\nw\nn\no\na\nd\ne\nd\n\n \nf\nr\no\nm\nh\n\n \n\nt\nt\n\np\n\n:\n/\n/\n\no\nn\n\nl\ni\n\n.\n\nn\ne\nu\nc\np\nr\ne\ns\ns\n.\ne\nd\nu\n/\nc\no\n\nl\nl\n\n/\n\na\nb\nr\na\na\nr\nt\ni\nc\ne\n-\np\nd\n\nl\n\nf\n/\n\n/\n\n/\n\n/\n\n8\n1\n5\n7\n5\n3\n8\n7\n6\n7\n4\n2\n7\n/\nc\no\n\nl\nl\n\na\nb\nr\na\n_\n2\n0\n2\n2\n_\n8\n_\n1\n_\n5\n7\n5\n3\n8\np\nd\n\n.\n\nf\n \n\nb\ny\n \ng\nu\ne\ns\nt\n \n\n \n\no\nn\n0\n4\n \nJ\na\nn\nu\na\nr\ny\n \n2\n0\n2\n3\n\nCollabra: Psychology\n\n18\n\n\x0c'",A_Roadmap_to_Large-Scale_Multi-Country_Replication 3.pdf
