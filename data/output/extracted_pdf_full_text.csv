Sentence,Source
"INTRODUCTION
Crowdsourcing Methods in Addiction Science:
Emerging Research and Best Practices
Justin C. Strickland1, Michael Amlung2, 3, and Derek D. Reed2, 3
1Department of Psychiatry and Behavioral Sciences, Johns Hopkins University School of Medicine
2Department of Applied Behavioral Science, University of Kansas
3Cofrin Logan Center for Addiction Research and Treatment, Lawrence, Kansas, United States
Crowdsourcing platforms such as Amazon Mechanical Turk, Proli ﬁc, and Qualtrics Panels have become a
dominant form of sampling in recent years. Crowdsourcing enables researchers to effectively and
efﬁciently sample research participants with greater geographic variability, access to hard-to-reach
populations, and reduced costs. These methods have been increasingly used across varied areas of
psychological science and essential for research during the COVID-19 pandemic due to their facilitation
of remote research. Recent work documents methods for improving data quality, emerging crowdsourcing
platforms, and how crowdsourcing data ﬁt within broader research programs. Addiction scientists will bene ﬁt
from the adoption of best practice guidelines in crowdsourcing as well as developing novel approaches,
venues, and applications to advance the ﬁeld.
Public Health Signi ﬁcance
The following set of articles in this special issue describes best practice methods and novel applications
of crowdsourcing in addiction and psychological science. These articles advance the ﬁeld and present
practical guidelines and open-source resources for researchers using crowdsourcing in future work.
Keywords: crowdsourcing, methods, mTurk, Proli ﬁc, validity
With the changing landscape of work amidst the global COVID-19
pandemic and increasing costs associated with collecting data from
large participant samples, researchers are turning to alternative meth-
ods for recruiting participants and collecting data. Accordingly,
crowdsourcing platforms such as Amazon Mechanical Turk, Proli ﬁc,
and Qualtrics Panels have become a dominant form of sampling
in recent years ( Strickland & Stoops, 2019 ). These crowdsourcing
platforms enable researchers to continue to collect data from large
samples of human participants when face-to-face visits are challeng-
ing, cost-prohibitive, or infeasible due to other barriers (e.g., research
sites at logistically prohibitive distances; social distancing require-
ments during COVID-19). Alongside optimism about the practical
beneﬁts that crowdsourcing may provide are uncertainties about the
validity of these approaches and how they can (and cannot) be used.
This special issue includes a collection of articles on best practices and
emerging research using crowdsourcing for addictions research.Articles included in this special issue emphasize that data quality
and control methods are critical in crowdsourcing platforms to
ensure data are reliable and valid. Jones et al. (2022) summarize
the importance of this work using a meta-analysis of careless
responding in crowdsourced alcohol use research and ﬁnd that
approximately 12% of participants are classi ﬁed as careless respon-
ders. They also provide practical recommendations to address these
issues to include the use of both overt and covert ﬁdelity measures.
Belliveau and Yakovenko (2022) complement these ﬁndings by
providing a practical implementation guide with step-by-step in-
structions for screening for speeding, straight-lining (i.e., tendency
to make the same response in a group of questions), inconsistent
responding, nonsensical responding, and missing data. Open-source
code for conducting these procedures is provided for those looking
to adopt these methods in their own work. Conceptually related
behavioral economic research shows the practical implications ofThis document is copyrighted by the American Psychological Association or one of its allied publishers.
Content may be shared at no cost, but any requests to reuse this content in part or whole must go through the American Psychological Association.
Justin C. Strickland
 https://orcid.org/0000-0003-1077-0394
Justin C. Strickland ’s contribution was partially supported by National
Institute on Drug Abuse (NIDA; Grant R03DA054098). Michael Amlung ’s
contribution was partially supported by National Institute on Alcohol Abuse
and Alcoholism (NIAAA; Grant R01AA027255) and the Cofrin Logan Center
for Addiction Research and Treatment at the University of Kansas. The authors
have no ﬁnancial con ﬂicts of interest in regard to this editorial introduction.Justin C. Strickland played lead role in writing of original draft and equal
role in conceptualization. Michael Amlung played equal role in conceptuali-
zation and writing of review and editing. Derek D. Reed played equal role in
conceptualization and writing of review and editing.
Correspondence concerning this article should be addressed to Justin C.
Strickland, Department of Psychiatry and Behavioral Sciences, Johns
Hopkins University School of Medicine, 5510 Nathan Shock Drive,
Baltimore, MD 21224, United States. Email: jstric14@jhmi.eduExperimental and Clinical Psychopharmacology
© 2022 American Psychological Association 2022, Vol. 30, No. 4, 379 –380
ISSN: 1064-1297 https://doi.org/10.1037/pha0000582
379",ContentServer.asp.pdf
"these quality checks. Craft et al. (2022) ﬁnd that delay-discounting
data from participants failing systematicity checks did not differ from
randomly generated data highlighting the importance of screening
and removing data with a priori validity checks. Freitas-Lemos et al.
(2022) describe how a novel check based on instructional under-
standing differentiated participants on consistency of cigarette use
reporting, responding on a cigarette demand task, and the relationship
between use behavior and demand data.
As the methods to screen for quality data have improved, so have
the venues in which crowdsourcing has been applied. Historically,
Amazon Mechanical Turk was the primary crowdsourcing outlet for
psychological and addiction science research. Belliveau et al. (2022)
describe the use of the more recently developed Qualtrics Panels
resource to study behavioral addictions (video gaming and gaming
disorder). They ﬁnd that Qualtrics data offered a participant pool
similar in demographics to a community-recruited population sup-
porting feasibility and potential usefulness of the resource. Stanton
et al. (2022) show how another novel platform, Proli ﬁc, can facilitate
repeated measures data in two protocols: a 5-day daily diary protocol
and a test –retest protocol. They explain across these two independent
studies how Proli ﬁc-recruited participants provided valid data
consistent with theoretical expectations and afforded ef ﬁcient
collection of longitudinal outcomes. Beyond online platforms like
Amazon Mechanical Turk, Qualtrics Panels, and Proli ﬁc,Pennington
et al. (2022) describe how big team science may ef ﬁciently crowd-
source researchers with the goal of producing reproducible projects
conducted across varied institutions. A review of existing work using
big team, crowdsourced approaches (e.g., ManyLabs, Psychological
Science Accelerator) as well as a novel approach used by the study
team are described.
How crowdsourcing ﬁts within a broader research program is
ultimately varied and may include pilot projects, methods develop-
ment, intervention deployment, and more. Rzeszutek et al. (2022)
provide one example of how crowdsourcing may be used to evaluate
novel behavioral outcomes by studying cross-drug withdrawal
effects for cigarette and opioids. They use a behavioral economic
framework to show that opioid withdrawal may increase cigarette
valuation, thereby providing a pathway for future treatment devel-
opment work to build upon. Borodovsky (2022) integrates the above
work to describe differences between generalizability and represen-
tativeness. A clear and concise review of these concepts is provided
along with how such differences may inform the boundary condi-
tions under which internet-based research may (or may not) advance
the literature.
Given the rapid emergence and evolution of crowdsourcing
research platforms, it is safe to assume this is a method that is here to
stay. This special issue highlights best practices for using crowdsour-
cing in addiction science while also drawing attention to important
methodological and conceptual limitations. We hope that addictionscientists continue to adhere to these guidelines while pushing the
boundaries of possible work within crowdsourcing science.
References
Belliveau, J., Soucy, K. I., & Yakovenko, I. (2022). The validity of qualtrics
panel data for research on video gaming and gaming disorder. Experi-
mental and Clinical Psychopharmacology ,30(4), 424 –431. https://
doi.org/10.1037/pha0000575
Belliveau, J., & Yakovenko, I. (2022). Evaluating and improving the quality
of survey data from panel and crowd-sourced samples: A practical guide
for psychological research. Experimental and Clinical Psychopharmacol-
ogy,30(4), 400 –408. https://doi.org/10.1037/pha0000564
Borodovsky, J. T. (2022). Generalizability and representativeness: Consid-
erations for internet-based research on substance use behaviors. Experi-
mental and Clinical Psychopharmacology ,30(4), 466 –477. https://
doi.org/10.1037/pha0000581
Craft, W. H., Tegge, A. N., Freitas-Lemos, R., Tomlinson, D. C., & Bickel,
W. K. (2022). Are poor quality data just random responses?: A crowd-
sourced study of delay discounting in alcohol use disorder. Experimental
and Clinical Psychopharmacology ,30(4), 409 –414. https://doi.org/10
.1037/pha0000549
F r e i t a s - L e m o s ,R . ,T e g g e ,A .N . ,C r a f t ,W .H . ,T o m l i n s o n ,D .C . ,S t e i n ,J .S . ,&
Bickel, W. K. (2022). Understanding data quality: Instructional comprehen-
sion as a practical metric in crowdsourced investigations of behavioral
economic cigarette demand. Experimental and Clinical Psychopharmacol-
ogy,30(4), 415 –423. https://doi.org/10.1037/pha0000579
Jones, A., Earnest, J., Adam, M., Clarke, R., Yates, J., & Pennington, C. R.
(2022). Careless responding in crowdsourced alcohol research: A system-
atic review and meta-analysis of practices and prevalence. Experimental
and Clinical Psychopharmacology ,30(4), 381 –399. https://doi.org/10
.1037/pha0000546
Pennington, C. R., Jones, A. J., Tzavella, L., Chambers, C. D., & Button,
K. S. (2022). Beyond online participant crowdsourcing: The bene ﬁts
and opportunities of big team addiction science. Experimental and Clini-
cal Psychopharmacology ,30(4), 444 –451. https://doi.org/10.10
37/pha0000541
Rzeszutek, M. J., Gipson-Reichardt, C. D., Kaplan, B. A., & Koffarnus,
M. N. (2022). Using crowdsourcing to study the differential effects of
cross-drug withdrawal for cigarettes and opioids in a behavioral economic
demand framework. Experimental and Clinical Psychopharmacology ,
30(4), 452 –465. https://doi.org/10.1037/pha0000558
Stanton, K., Carpenter, R. W., Nance, M., Sturgeon, T., & Villalongo
Andino, M. (2022). A multisample demonstration of using the proli ﬁc
platform for repeated assessment and psychometric substance use research.
Experimental and Clinical Psychopharmacology ,30(4), 432 –443. https://
doi.org/10.1037/pha0000545
Strickland, J. C., & Stoops, W. W. (2019). The use of crowdsourcing in
addiction science research: Amazon Mechanical Turk. Experimental and
Clinical Psychopharmacology ,27(1), 1 –18.https://doi.org/10.1037/pha
0000235
Received May 10, 2022
Accepted May 10, 2022 ▪This document is copyrighted by the American Psychological Association or one of its allied publishers.
Content may be shared at no cost, but any requests to reuse this content in part or whole must go through the American Psychological Association.380 STRICKLAND, AMLUNG, AND REED",ContentServer.asp.pdf
"1 
 The Benefits, Barriers, and Risks of Big Team Science 
Authors: Patrick S. Forscher1,2, Eric-Jan Wagenmakers3, Nicholas A. Coles4,  
Miguel Alejandro Silan5,6, Natália Dutra7, Dana Basnight-Brown8, & Hans IJzerman2,9 
 
Affiliations : 
1Busara Center for Behavioral Economics 
2Université Grenoble Alpes 
3University of Amsterdam 
4Stanford University 
5Université Lumière Lyon 2 
6Annecy Behavioral Science Lab 
7Universidade Federal do Pará 
8United States International University-Africa 
9Institut Universitaire de France 
 
Notes: PSF and HIJ wrote the first draft; all authors provided critical comments and revisions. 
Conflict of Interest Statement : PSF, NAC, MAS, ND, DB, and HIJ are members of the 
Psychological Science Accelerator, a network of psychology research labs that conducts big 
team science projects.  
 
Abstract: Progress in psychology has been frustrated by challenges concerning replicability, 
generalizability, strategy selection, inferential reproducibility, and computational reproducibility. 
Although often discussed separately, these five challenges may share a common cause: 
insufficient investment of intellectual and non-intellectual resources into the typical psychology 
study. We suggest that the emerging emphasis on big team science  can help address these 
challenges by allowing researchers to pool their resources together to increase the amount 
available for a single study. However, the current incentives, infrastructure, and institutions in 
academic science have all developed under the assumption that science is conducted by solo 
Principal Investigators and their dependent trainees, an assumption that creates barriers to 
sustainable big team science. We also anticipate that big team science carries unique risks, such 
as the potential for big team science organizations to be co-opted by unaccountable leaders, 
become overly conservative, and make mistakes at a grand scale. Big team science organizations 
must also acquire personnel who are properly compensated and have clear roles, raising risks 
related to mismanagement and a lack of financial sustainability. If researchers can manage its 
unique barriers and risks, big team science has the potential to spur great progress in psychology 
and beyond. 
  ","Forscher et al. - The Benefits, Barriers, and Risks of Big Team Scie.pdf"
"2 
 Despite decades of investment, progress in psychology has been slower than many 
scholars would like (Fanelli, 2010; Meehl, 1978; Newell, 1973). The lack of progress is 
empirically supported by the results of replication studies. Whether with the same population, 
setting, and materials (the replicability challenge ; Klein et al., 2014; Open Science 
Collaboration, 2015) or after a change to one or more of these features (the generalizability 
challenge ; Henrich et al., 2010; Tiokhin et al., 2019; Yarkoni, 2019), replicated results often 
differ meaningfully from original results. Meaningful differences also occur in other forms of 
replication, such as: when separate teams develop research strategies to address the same 
research question (the strategy selection challenge ; Landy et al., 2020) when separate teams 
develop analysis plans for the same dataset (the inferential reproducibility challenge ; Botvinik-
Nezer et al., 2019; Silberzahn et al., 2018), and even when separate teams write code to execute 
the same analysis (the computational reproducibility challenge ; Donoho et al., 2008; Hardwicke 
et al., 2018; Obels et al., 2019).  
These five challenges have complex proximal causes. Yet they may share a common 
ultimate cause: insufficient resource investment in the typical psychology study. Insofar as this 
premise is true, a particular method of collaboration, big team science , may help address this 
ultimate cause by efficiently scaling up the resources that can be invested in any given study. We 
define big team science  as a method involving a relatively large number of collaborators who 
may be dispersed across labs, institutions, disciplines, cultures, and continents. We contrast this 
system of science with small team science , which is usually organized around a single Principal 
Investigator and their dependent trainees. If the unique risks and challenges of big team science 
are properly understood and managed, this method of collaboration may have great potential to 
improve the efficiency and information value of psychological science. 
The challenges in psychology share a common cause 
Discussion of the slow progress in psychology has a long history. Scholars have argued 
persuasively that each of the five challenges facing psychology has a pernicious and potentially 
destructive influence on scientific progress (Donoho et al., 2008; Landy et al., 2020; Meehl, 
1978; Sears, 1986; Silberzahn et al., 2018). These scholars have also presented a dizzying array 
of remedies for these challenges, ranging from increasing study sample sizes (Button et al., 
2013), which ought to improve replicability, to implementing version control (Vuorre & Curley, 
2018), which ought to improve computational reproducibility. Although varied, these remedies 
share a common feature: they ask researchers to incur additional costs to improve a particular 
aspect of study rigor (Finkel et al., 2017; LeBel et al., 2017; see Table 1 for a list of proposed 
remedies and their attendant costs).  
Because the individual researcher incurs costs for each remedy, discussions of problems 
in psychology tend to assume that implementations of these remedies are zero-sum in the 
following sense. The scientific resources that are available to a given researcher, such as time, 
labor, and money, are limited. This means that, as long as a solution to improve, say, replicability 
does not also improve all the other aspects of study rigor, resources devoted to implementing that 
solution to replicability must necessarily take away resources that could be spent on solutions to 
other aspects of rigor. Replicability, generalizability, inferential reproducibility, strategy 
selection, and computational reproducibility thus become qualities that must be traded off each ","Forscher et al. - The Benefits, Barriers, and Risks of Big Team Scie.pdf"
"3 
 other because devoting resources to improve one of these five qualities means sacrificing 
resources that could be invested in improving the others. 
Challenge  Suggested remedy  Costs of implementation  Who bears costs?  
Replicability  Increase sample size  Labor and money for extra recruitment  Researchers; 
participants  
Preregister analysis plans  Labor creating preregistration and making it 
accessible on a platform  Researchers;  
platform maintainers  
Improve documentation of 
materials Labor creating and archiving documentation 
materials Researchers;  
platform maintainers  
Generalizability  Use special platforms to recruit 
culturally different participants  Labor developing, maintaining, and learning to 
use new platforms; money using and 
maintaining the platforms  Researchers; 
platform maintainers  
Collaborate with colleagues from 
different settings & backgrounds  Labor developing relationships with new 
colleagues  Researchers  
Improve generality of materials  Labor and money developing and validating 
improved stimuli and measures  Researchers; 
participants  
Strategy selection  Examine many construct 
operationalizations  Labor and money examining different 
operationalizations  Researchers; 
participants  
Consensus design  Labor developing relationships with relevant 
experts; labor implementing consensus process  Researchers  
Inferential 
reproducibility  Machine-readable hypothesis tests  Labor learning and implementing machine-
readable systems; money developing and 
maintaining platforms  Researchers;  
platform maintainers  
Many analyst design  Labor making connections with relevant 
analysts Researchers  
Examine a multiverse of tests  Labor developing relationships with relevant 
experts; labor implementing multiverse analysis  Researchers  
Computational 
reproducibility  Share code and data  Labor to put code and data into a shareable 
form; money developing and maintaining 
sharing platforms  Researchers;  
platform maintainers  
Implement code checking  Labor and money implementing code checking  Researches; journals  
Improve documentation of 
codebases Labor making code readable and hygienic  Researchers  
Use capsules, version control, and 
markdown  Labor learning and implementing new 
workflows; money maintaining capsule and 
version control platforms  Researchers;  
platform maintainers  
 
Table 1. Five challenges in psychology and some proposed remedies for these challenges. The challenges all ask 
researchers to bear extra costs to improve some aspect of study rigor.   
 ","Forscher et al. - The Benefits, Barriers, and Risks of Big Team Scie.pdf"
"4 
 We suggest that the views that prioritize one aspect of rigor over the others all share an 
important, mistaken assumption: the pool of resources available for investment in a given study 
is fixed. If the resources devoted to a single study are not fixed, these resources may be raised to 
a level that is high enough that no tradeoffs are necessary among the various types of research 
rigor. Instead, a researcher need merely select as many of the rigor-enhancing strategies from 
Table 1 as they like and invest the money, human resources, and skills required to deploy those 
strategies. 
Thus, although they are often discussed separately, all five of psychology’s major 
problems may share a common cause: under-investment of resources, whether those resources 
are money, person-hours, or specialized expertise, in psychology studies (see also Cuccolo et al., 
2021; Uhlmann et al., 2019). Instead of asking which aspects of study rigor ought to be 
prioritized, reframing psychology’s problems from this perspective allows us to ask a different 
question: how can we increase the resources invested in psychology studies and ascertain that the 
extra investment is used efficiently? 
Increasing resource investment through big team science organizations 
Increasing the per-study resource investment across an entire research ecosystem faces a 
collective action problem: as long as scientists are rewarded for publishing more studies, any 
single scientist who decides to invest more resources in fewer studies will be outcompeted by 
scientists who invest less resources in more studies (Smaldino & McElreath, 2016). Any attempt 
to increase the per-study resource investment of the entire ecosystem must adopt one of two 
solutions: (1) directly change the institutional incentives that prioritize quantity of publication 
and/or (2) devise new institutions, which we call big team science organizations , that allow blocs 
of scientists to increase the resource investments in concert. 
Direct change in the institutions of science is difficult because science itself has a 
decentralized structure – changing its reward structures requires buy-in from many independent 
actors. This limits the effectiveness of initiatives to directly change the incentives that prioritize 
quantity of publication. A similar problem afflicts the establishment of new organizations that 
coordinate the efforts of scientists to jointly invest in single projects. Fortunately, new 
information and communication technologies, such as the Google suite of collaboration tools, the 
Open Science Framework, Slack, and Zoom, have made the establishment of such organizations 
more feasible because they permit rapid, low-cost communication across far-flung countries and 
circumstances (Teasley & Wolinsky, 2001). Such communication lays the groundwork for new 
institutions that can change how scientific actors spend resources on their science (Spellman, 
2015). 
The function of an organization is to coordinate the activities of many actors 
simultaneously. An organization solves the collective action problem faced by individual actors 
by allowing multiple actors to pre-commit to a coordinated, simultaneous course of action. In the 
context of science, a big team science organization  allows many scientists to pre-commit to 
investing their limited scientific resources into one big project in exchange for an individual 
reward – usually, authorship on a publication, though other rewards are possible, such as money 
or networking opportunities. During the project itself, the organization coordinates the actions of ","Forscher et al. - The Benefits, Barriers, and Risks of Big Team Scie.pdf"
"5 
 the scientists so that they do not work at cross-purposes. These functions allow the per-study 
investments from big team science projects to eclipse what is achievable in small team science. 
We believe that the primary and most definitional function of a big team science 
organization is to allow larger investments of material and intellectual resources in a given 
project. However, once established, big team science organizations have the potential to provide 
benefits to both individual scientists and the broader scientific ecosystem that are external to this 
primary function. As an example of one such positive externality , big team science organizations 
centralize many aspects of project administration in one organization rather than forcing 
individual labs to complete these functions on their own. This not only produces an efficiency for 
the scientific ecosystem, but also allows researchers at organizations without strong research 
infrastructure, such as instructors at teaching-focused universities (Wagge, Baciu, et al., 2019; 
Wagge, Brandt, et al., 2019) to do more research than would otherwise be possible. Big teams 
also allow individual researchers to specialize into roles that match their skills rather than 
occupying all roles simultaneously, producing an efficiency for the scientific ecosystem and 
giving the individual researcher the opportunity to develop a specialized skill and thereby 
command a higher salary. Researchers who join a big team science organization also gain access 
to an expansive community. For the individual researcher, this reduces intellectual isolation and 
opens the door to professional opportunities; for the scientific ecosystem, this can generate new 
collaborations that would not have been possible absent the expansive community. Finally, if big 
team science organizations become robust enough, they can become political actors of their own 
right within the broader scientific ecosystem. Thus, they can nudge the incentives and values of 
the entire scientific ecosystem by, for example, prioritizing the qualities of scientists to improve 
team productivity rather than individual productivity (Tiokhin et al., 2021). 
Thus, big team science organizations can invest more resources into individual projects 
than is possible in small teams. Big team science organizations also have the potential to provide 
a variety of benefits to individual researchers and the broader scientific ecosystem that are 
external to this primary function. Whether these benefits materialize depend on establishing the 
organizations in the first place and ensuring the organizations do not fall prey to some of the 
unique risks and challenges of this style of science. 
Big team science outside of psychology  
Some disciplines have already adopted a big team science model, so they can provide 
useful examples of how big team science organizations can arise. These examples can also 
illustrate whether big team science can address the limitation of under-resourced studies. In the 
1990s, behavioral genetics generally featured small studies examining the relationship between 
variation in a single candidate gene and a complex behavior or trait. In one prominent example, 
only 52 patients provided genetic material for an analysis of the relationship between the 5-
HTTLPR polymorphism and major depression (Heils et al., 1996), a finding that spurred 
enormous interest in the biological mechanisms through which these genes might cause 
depression (Dannlowski et al., 2008; Gotlib et al., 2008). Unfortunately, and similar to the 
current situation in psychology, these early results were contradicted by failed replication studies 
(Gillespie et al., 2005). As more and more replication studies contradicted the earlier optimistic 
ones, researchers in behavioral genetics realized that changes were in order (Rieckmann, 2009).  ","Forscher et al. - The Benefits, Barriers, and Risks of Big Team Scie.pdf"
"6 
 The two decades that followed were a turning point for behavioral genetics because they 
gave rise to big team science in the form of large research consortia. Without pooling resources, 
researchers would have been unable to attain the scale of data necessary to advance our 
understanding of behavioral genetics. Before these consortia could attain such large-scale data, 
they needed to invest considerable research, time, and funds devising methodological workflows 
(Corvin et al., 2010), designing infrastructure to manage and harmonize datasets (Sullivan, 
2010), and developing processes that increased the accuracy of the measurement of genotypes 
and phenotypes (23andMe Research Team et al., 2018; Corvin et al., 2010). With these solutions 
in hand, the research consortia served as a conduit through which multiple labs pooled resources 
to achieve large, heterogeneous samples that would otherwise have been out of reach for any 
single lab working independently. The UK Biobank is a prominent example, hosting a repository 
for data from over 500,000 participants, a sample that dwarfs the sample of 52 used for the first 
5-HTTLPR study. The resulting findings have revolutionized our understanding of genes and 
behavior: we now know that, instead of single genetic variants exerting large influences, a large 
number of variants have small influences (23andMe Research Team et al., 2018). These impacts 
were subtle enough that they could not be observed and studied using the small, homogeneous 
samples typically employed in the early 1990s. Innovations in methods, workflows, and 
measurement allowed for a sea change in our understanding of the relationship between genes 
and behavior. 
Particle physics provides another, still larger-scale, example. The 1950s saw a growing 
recognition among particle physicists that further advances would require a scale of research and 
equipment that would strain the budgets of entire nations, let alone single laboratories (Krige, 
2004). For their part, Western governments saw cooperative large-scale investments in physics 
as a potential diplomatic tool; the projects served as a highly visible counterweight growing 
Soviet power, demonstrating the benefits that could accrue from an internationalist, democratic, 
and cooperative world order (Krige, 2004). These twin recognitions spurred the establishment of 
“mega-collaborations”: large, international collaborations spanning multiple countries. Such 
mega-collaborations could only realize their aims through both technical and social 
achievements to establish the infrastructure, documentation, and workflows necessary to 
coordinate the efforts of huge numbers of scientists (Bakker, 1955; Brumfiel, 2011; for a modern 
workflow, see Espinosa et al., 2020). One of the pre-eminent examples of these mega-
collaborations is the European Organization for Nuclear Research (CERN), which, together with 
Brookhaven National Laboratory, helped usher in a new era of particle physics: high-energy 
physics, in which large teams of scientists harness vast resources to probe the most fundamental 
constituents of matter (Bryant, 1994). 
 
 
 
 
 ","Forscher et al. - The Benefits, Barriers, and Risks of Big Team Scie.pdf"
"7 
  
Name Type Description  
Reproducibility Project: Psychology  
(Open Science Collaboration, 2015) Ad-hoc A large team-science project in psychology, which involved 270 
contributing authors, who replicated 100 effects.  
ManyLabs  (Ebersole et al., 2016; Klein et 
al., 2014, 2018, 2019)  Ad-hoc These projects involve dozens of researchers, each of whom collects 
data using their own local resources. The individual datasets are pooled 
to create a large common dataset.  
Many Smiles Collaboration  (Coles et 
al., 2019) Ad-hoc A collaborative project designed to find the best way to test the 
hypothesis that facial expressions influence emotions. The test was 
developed through consensus design. The resulting design uses multiple 
operationalizations and will be tested in a multi-site study. 
Research contests to reduce implicit 
race preferences  (Lai et al., 2014, 2016) Ad-hoc A series of contests to develop the most effective interventions to 
reduce implicit race preferences. Separate teams submitted 
interventions; all interventions were run together on a large online 
platform. 
Registered Replication Reports  (Simons 
et al., 2014) Standing An initiative to conduct multi-lab, preregistered close replications of 
previous studies. The initiative supervised by a hosting journal. 
Originally initiated at Perspectives at Psychological Science , other 
journals have adopted similar initiatives. Exact policies differ across 
journals. 
Collaborative Replication and 
Education Project  (Wagge, Baciu, et al., 
2019; Wagge, Brandt, et al., 2019) Standing A framework for engaging undergraduates in replication research. On 
consultation with student advisors, the CREP team selects target effects 
for replication. CREP develops a set of templates that guide the 
replication process and uses a team of reviewers to ensure that the 
methods for each individual lab are true to the template. 
Psychological Science Accelerator  
(Jones et al., 2021; Moshontz et al., 
2018) Standing A standing, democratically structured network of labs focused on 
improving the national diversity of psychology samples. 
ManyBabies  (Byers-Heinlein et al., 
2020) Standing A standing network that conducts multi-site infancy research. 
 
Table 2. Eight big team science initiatives in psychology.  
 
Big team science in psychology  
Psychology has also started to witness an increase in big team science projects. Although 
most of these projects are recent, they have already had an outsized impact on the field (see 
Table 2 for details). However, these efforts have revealed three categories of obstacles that must 
be overcome if big team science is to maintain a sustained presence in the research landscape. 
These obstacles are: incentivizing labor  within the collaboration; developing and maintaining 
infrastructure  to coordinate team science activities; and dealing with institutions  designed 
around research conducted by smaller teams. 
 
 ","Forscher et al. - The Benefits, Barriers, and Risks of Big Team Scie.pdf"
"8 
 Barrier Description Solutions 
Incentives Academic prestige goes 
disproportionately to the first-listed 
author on publications Consortium authorship 
Contributorship systems, like CRediT 
Infrastructure Coordinating team science requires 
dedicated infrastructure Google Suite of tools for general online collaboration 
Open Science Framework for sharing materials & data 
Zoom and other videoconferencing for online meetings 
formr for online deployment of big team science projects 
experimentum  for building experiments and managing projects 
ScienceVerse  for documenting big team science projects 
Institutions Big team science must navigate 
frictions created by institutional 
policies developed around solo PIs None at present 
Table 3. Barriers to big team science and some potential solutions. 
 
Incentives . Due to the central importance of prestige in obtaining academic rewards, 
academics are incentivized to obtain publications that can be used to enhance their prestige. This 
means that any collective research enterprise that wishes to direct the labor of academic scientists 
must either rely on sporadic volunteerism, find some other reward, like money, that substitute for 
prestige, or find a way to dole out prestige. We describe the risks of relying on volunteerism and 
the barriers to providing money as a reward in the sections on institutional barriers and risks of 
financial unsustainability. Here we deal more completely with prestige as a form of 
compensation. 
A central problem with prestige as an incentive for participation in big team science is 
that, under the current system for awarding credit for publications, the bulk of credit goes to the 
first-listed author. This reward structure does not effectively incentivize the labor of the 
numerous other people who are necessary to produce big team science publications. Moreover, 
big team science organizations must do a tremendous amount of administration and coordination 
that, while critical to the success of the collaboration as a whole, is not easily creditable on 
publications.  
We see at least two innovations that may help resolve this problem. The first is 
consortium authorship , in which a publication is credited to a collective entity rather than a 
group of individuals, or in which all members of the collaboration are listed alphabetically on all 
publications to render the individual subservient to the collective (Birnholtz, 2008). This was the 
approach to credit taken by the Open Science Collaboration when they published the ","Forscher et al. - The Benefits, Barriers, and Risks of Big Team Scie.pdf"
"9 
 Reproducibility Project: Psychology (Open Science Collaboration, 2015). This is also the 
approach taken by some large physics collaborations, especially for papers that document details 
of infrastructure or instrumentation (e.g., ALICE collaboration, 2010; Harry & the LIGO 
Scientific Collaboration, 2010; The ATLAS collaboration et al., 2018). This innovation 
incentivizes collaboration by attempting to flatten the credit reward structure. Consortium 
authorship systems come at some risk, however: they can under-reward highly active but less 
visible consortium members, or lead consortium members to seek informal routes of gaining 
recognition, thereby effectively substituting the flat, formal system of credit with a more 
arbitrary informal system (Birnholtz, 2008). 
The second innovation is to disclose contributions in a more fine-grained way through 
contributorship systems, such as CRediT (McNutt et al., 2018). These systems allow contributors 
to list within a manuscript the specific roles contributors play in projects. In principle, 
contributorship systems should allow people to develop reputations for filling certain types of 
project roles, such as data analysis or project management, and they should allow more fine-
grained accountability when errors are detected in the project. Finally, contributorship systems 
enable contributors to provide evidence of excellence in a particular project role when applying 
for grants, jobs, or other professional rewards.  
However, contributorship systems will only serve their purpose of giving recognition to 
excellent team scientists if people who control professional rewards, such as members of hiring 
committees, promotion committees, and funding committees, actually attend to and reward 
evidence of excellent team science contributions. Although we see promise in contributorship 
systems as a method to incentivize big team science labor (Holcombe, 2019), the existence of 
contributorship information is by itself not sufficient to ensure that incentives are aligned to 
reward big team science labor. The people who control professional rewards must also attend to 
these systems if they are to serve their intended functions.  
 Infrastructure . A second obstacle is the need for infrastructure to help facilitate and 
coordinate big team science projects. Some of this infrastructure already exists, such as the 
Slack, Google suite of collaboration tools, and Zoom. This existing infrastructure has been 
instrumental in propelling big team science to where it is today. For example, projects run 
through the Psychological Science Accelerator (Moshontz et al., 2018) use a combination of 
Slack and email for project coordination, Google Docs, Sheets, and Forms for collaborative 
project workflows (for a writing worklow, see Moshontz et al., 2019), a shared Google Drive for 
collaborative files management, and Zoom for conference calls. However, this existing 
infrastructure is general-purpose and therefore does not fully meet the specialized needs of 
behavioral science. For example, most psychology data collection platforms are designed for use 
by one or two users. This is insufficient in a team of researchers numbering in the hundreds. 
Although not every researcher needs to be part of the development of, for example, a survey, 
multiple users need to access survey instruments when helping with translation or other parts of 
the survey development process. 
Another example of where research infrastructure is insufficient is when recruiting 
samples of participants that vary linguistically – a common occurrence when working with 
worldwide collaborators. Often, translated versions of the target measures are simply 
unavailable. Creating and validating a greater array of translated measure versions will go a long ","Forscher et al. - The Benefits, Barriers, and Risks of Big Team Scie.pdf"
"10 
 way toward facilitating big team science projects. A lack of applicability across languages and 
countries apply to infrastructure across all stages of the research lifecycle. Data collection and 
management platforms in particular must cope with both issues involving language and 
translation and the varying ethical and legal standards that govern private data. 
Fortunately, some promising initiatives already exist that, if they are properly supported, 
may help resolve these more specialized problems. For example, the formr experiment platform 
(Arslan et al., 2019) relies on some of the infrastructure that is already in place, namely Google 
Sheets, to permit easy, flexible, and collaborative construction of online experiments. Another 
promising project in this vein is the experimentum  framework ( https://psa.psy.gla.ac.uk/ ), which 
aims to develop an experiment builder and project management framework that is specialized for 
large, cross-linguistic, big team science settings. Finally, the ScienceVerse  project (Lakens & 
DeBruine, 2021) aims to create a fully functional “grammar of science” that can be used for 
naming, describing, and identifying relationships between components of any scientific project. 
Such a grammar would be especially helpful for highly collaborative, cross-linguistic, big team 
science projects that must document a large number of components while navigating specialized 
ethical and legal standards. 
These initiatives will only succeed in meeting the specialized infrastructure needs of big 
team science in the social sciences if they are supported financially and on a sustainable basis. 
Funders should recognize that these and other initiatives that support big team science also 
provide benefits for small team science. For example, although formr is especially useful in team 
settings, it provides a useful platform for teams of any size. Smart and ongoing investment in 
these projects will go a long way toward facilitating both small team and big team science. 
Institutions . Another set of obstacles relates to the institutions that have developed 
assuming that projects are led by a solo (or small number of) Principal Investigators. These 
institutions cause friction in almost every aspect of large, collaborative science. The frictions 
start with funding mechanisms, most of which assume that projects are led by a sole PI and their 
staff. The European Research Council’s Consolidator Grant, for example, supports a single PI 
and their host institution, and the European Research Council enforces highly detailed 
accounting rules to ensure all money is used in support of a solo PI’s project. As another 
example, some grant funders, such as the US National Science Foundation, require applicants to 
list all their collaborators from a specified time period. This requirement places burdensome 
restrictions on prospective big team scientists whose collaborators can easily number in the 
hundreds. Even once money is awarded, administrative and legal barriers can make it difficult to 
send the money to the institutions and countries where it is needed. 
Similar frictions plague almost every part of the big team science research lifecycle. 
Academic psychology departments do not typically train scientists to operate in large, distributed 
team settings and do not have specialized training tracks for scientists who wish to specialize in 
the many specific roles that big team science requires. Ethical Review Boards are often not 
prepared to evaluate a project that will be executed at a hundred sites worldwide. A big team 
science project with 100 collaborating labs may need to submit 100 variations of the same 
application to 100 Ethical Review Boards – a process that leads to immense waste and 
duplication of effort (Ervin et al., 2016; Schneider, 2015, pg. 44). Even the content of ethical 
regulations themselves differs across locations due to varying laws and norms. Some ","Forscher et al. - The Benefits, Barriers, and Risks of Big Team Scie.pdf"
"11 
 organizations are not subject to clear ethical requirements at all – or, as is the case in East Africa, 
for example – they are only able to complete an ethical review in exchange for a costly fee that 
competes with other research costs (Kaplay, 2016; Osborne & Luoma, 2018).  
As another example, journal submission portals are built assuming that articles are 
written by a relatively small number of authors – not 200 – and the process of entering author 
information is cumbersome and wasteful. A similar barrier obstructs the entry of authorship 
information in preprint servers like PsyArXiv. Once research is complete, tenure and evaluation 
committees do not know how to evaluate publications with hundreds of authors. This decision is 
high stakes, because if researchers that prioritize big team science are penalized by these 
committees, these “big team scientists” will be effectively selected out of the ecology of science 
(Smaldino & McElreath, 2016). 
Here we are less certain what the future holds. If the institutions of psychological science 
adapt to the emergence of big team science, they will make this sort of science less costly and 
therefore more common. Alternatively, psychological science could develop an entirely new set 
of institutions, such as funders, ethical review boards, and journals, that are more 
accommodating of big team science. Whatever the future may hold, we hope that the institutions 
of science recognize the potential of big team science and act accordingly. 
Risks of big team science 
 Although we believe that big team science approaches have great promise, they also 
bring a unique set of risks that stem from the very feature that provides their main benefits: the 
fact that they coordinate and centralize resources to be deployed in a single project (Stokols et 
al., 2008). These risks are unaccountable leadership , management failures , conservatism , 
sustainability failures , and mega-mistakes .  
Unaccountable leadership . The primary virtue of big team science is its ability to deploy 
resources at a large scale. Often, this requires turning over resources to a single scientific 
organization, which carries the same risk as that entailed in turning over resources to a single 
governmental organization: the leadership of that organization could use the resources to 
centralize power within themselves and make themselves unaccountable to influence and 
criticism. 
The negative consequences of unaccountable leadership power can impact many features 
of the scientific process. For example, in the idea generation phase, unaccountable leadership can 
select their own pet topics for investigation without heeding the ideas of people with less power, 
leading to ideas that are less creative and impactful (Wu et al., 2019). Unaccountable leadership 
can also stifle diversity in the strategies that are selected to develop and test the ideas that are 
selected (Devezer et al., 2019), slowing the pace of discovery. Finally, unaccountable leadership 
can have negative consequences for individual scholars who do not conform to leadership’s 
perhaps narrow expectations of who is a proper “big team scientist” – especially if the leadership 
has influence over important mechanisms of career advancement (Azoulay et al., 2019). 
 
 ","Forscher et al. - The Benefits, Barriers, and Risks of Big Team Scie.pdf"
"12 
 Risk Description  Mitigation strategies  
Unaccountable leadership Leadership of big team science 
organizations make themselves 
unaccountable to influence and 
criticism Articulate organizational values 
Create structured bylaws 
Introduce democratic accountability 
Empower a board of directors 
Management failures Big team science organizations 
involve large numbers of 
relationships, and navigating the 
interpersonal issues that arise 
requires effective management of 
people Articulate roles and responsibilities 
Create structured methods of making decisions 
Create formal and informal ways to solicit team member feedback 
Obtain formal management training 
Conservatism Big team science organizations 
may make decisions that cater to 
the median desire of the people in 
their group Proactively partner with organizations that serve underrepresented groups 
Create mechanisms to facilitate mobility and advancement within the team 
Separate idea generation from project implementation 
Sustainability failures Big team science organizations 
require planning to be financially 
sustainable over the long term Pay people the people who are responsible for the organization's 
maintenance 
Create and follow a sustainability plan 
Mega-mistakes When big team science 
organizations make mistakes, they 
tend to be big ones Mitigate the other four types of risk 
Institute pro-active quality control processes 
Table 4. Risks of team science and some risk mitigation strategies. 
 
Mitigating this risk . At a baseline, big team science organizations should have a set of 
bylaws that lay out who is empowered to do what and a set of values. The bylaws need not 
specify a completely de-centralized power structure. Centralizing power within a small number 
of leaders can have important benefits, such as enabling those leaders to develop a coherent 
organizational strategy and allowing those leaders to make fast, agile decisions to adjust that 
strategy (Baum & Wally, 2003). However, to mitigate the risk of a lack of accountability, 
centralization of leadership must be accompanied with mechanisms to hold leadership 
accountable for its decisions if those leaders pursue directions with which important stakeholders 
disagree. 
An important tool for enabling such accountability is a clearly articulated statement of the 
values that guide the big team science organization’s mission. Once articulated, these values can ","Forscher et al. - The Benefits, Barriers, and Risks of Big Team Scie.pdf"
"13 
 allow the organization’s stakeholders to identify leadership mistakes that might require 
accountability by allowing those stakeholders to evaluate whether leadership actions do indeed 
fit with the organization’s values. Insofar as leaders do issue regular statements about their 
organizational strategy, a set of well-articulated values also allows stakeholders to evaluate 
whether that strategy is indeed pursuing those values.  
However, values are empty unless a group of organization stakeholders are empowered to 
take action if and when leaders violate those values. One example of an empowered stakeholder 
is a board of directors that is empowered by the organization’s bylaws to remove leaders with 
whom they are unsatisfied. This board of directors should receive regular updates on the status of 
the big team science organization so that they can ask leaders to make corrections before a crisis 
occurs. Another possibility is to introduce explicit democratic mechanisms into the organization, 
such as regular elections for leadership positions. Whatever the form, accountability must be 
built into the organization’s structure if it is to effectively check the risks of unaccountable 
leadership.  
Management failures . The number of relationships within a team increases 
combinatorially with the number of people within the team. Larger teams also create more 
opportunities for people with widely different backgrounds to collide – or for single trouble-
makers to create a toxic environment. Finally, larger teams are more likely to be characterized by 
specialized roles and communication channels. These features vastly increase the complexity of 
relationships in very large teams. In failure cases, this complexity can lead to management 
failures. 
The first of these failures is role ambiguity. Role ambiguity occurs when the information 
a person has about the expectations that go along with the role, the methods for fulfilling those 
expectations, and the consequences for violating those expectations are unclear (Van Sell et al., 
1981). Role ambiguity can occur in small groups, but the number of roles in a large scientific 
team, combined with the specialization of roles and complex decision-making apparatus, 
increase the risk of role ambiguity as teams increase in size. Role ambiguity increases the risk of 
errors because team members are uncertain which responsibilities apply to them. This 
uncertainly can lead to duplicated work on the one hand and unfulfilled tasks on the other. Role 
ambiguity also breeds feelings of dissatisfaction, leading people to grow disillusioned with the 
big team science organization and, in extreme failure cases, to leave it (Tubre & Collins, 2000).  
The second failure is ineffective management of interpersonal issues. Some interpersonal 
issues, such as conflict between team members, are inevitable due simply to the number of 
relationships involved in big team science organizations. Some level of conflict can even be 
constructive when it is managed properly and kept task-focused (Forsyth, 2014; Loughry & 
Amason, 2014; Rahim, 2003). However, conflict can also create organizational dysfunction, 
potentially polarizing the team and derailing entire projects (Rahim, 2003). In the presence of 
sharp power differences between team members and unclear mechanisms for accountability, 
conflict can also take on a darker guise in the form of harassment and abuse (Berdahl & Raver, 
2011). The risks of conflict can be especially high when team members have sharp cultural or 
epistemic differences, as might be expected in big team science settings that bring together 
people of varying personal, cultural, and disciplinary backgrounds (Bender et al., 2015). ","Forscher et al. - The Benefits, Barriers, and Risks of Big Team Scie.pdf"
"14 
 Mitigating this risk . The most fundamental step a big team science organization can take 
to mitigate the risk of organizational failures is to establish a well-defined leadership structure 
that clearly communicates and enforces the organization’s roles and decisions (Stokols, 2006). 
The leadership structure should also create both formal and informal ways of soliciting feedback 
from members of the big team science organization. These feedback mechanisms should include 
a means to report problematic behavior by individual team members, along with a means to hold 
problematic team members accountable, such as reporting the behavior to the team member’s 
workplace. The feedback mechanisms should establish a feedback loop between decision-makers 
and team members so that the organization responds effectively to issues as they arise. The goal 
of the feedback loop will not eliminate interpersonal issues, as some number of issues are simply 
part of the human condition, but rather to instill a sense that procedures are fair when issues do 
occur (Konovsky, 2000). Effective communication also fosters an environment of psychological 
safety in which team members feel empowered to express issues, concerns, and points of 
disagreement without creating outright fights (Frazier et al., 2017).  
Clarifying roles and instilling psychological safety both take time. However, their 
importance to effective collaboration means that team science organizations should prioritize 
creating structures that allow transparent communication, decision-making, and enforcement, 
ideally through a formal collaboration agreement  that is drafted before the collaboration begins. 
Structured methods of communication, decision-making, and enforcement lay the groundwork to 
allow team members to build the mutual trust that is necessary for collaboration (Astuti & Bloch, 
2012). 
Management of teams is a vital skill that is seldom taught in the academy. Universities 
should recognize the central role that management plays in the success of projects, especially the 
larger ones that typify big team science. If big team science is to take root in the social sciences, 
management training needs to become a central part of the behavioral science curriculum. 
Conservatism . Large organizations require large amounts of people to keep themselves 
running. If these organizations attempt to satisfy the desires of these large numbers of people, 
they will tend to cater to the median desire. This means that organizations will usually be 
conservative  – or at least conservative with respect to the people within the group. This 
conservatism can manifest in two ways: in the selection of personnel and the selection of 
projects. 
People tend to form social relationships with others who have similar characteristics and 
backgrounds (Kossinets & Watts, 2009; McPherson et al., 2001). Given that contemporary 
psychology is dominated by North America and Europe (Rad et al., 2018; Thalmayer et al., 
2020), this raises a risk that big team science organizations will be similarly dominated by people 
from those continents. For example, not a single first author of the papers in Table 2 comes from 
a nation outside North America and Europe. Once big team science organizations dominated by 
North America and Europe are established, they may inadvertently crowd out organizations from 
elsewhere with different goals and personnel. Systematically excluding large subsets of humanity 
from psychology perpetuates unfair systems of inequality and can lead to a science that focuses 
unduly on the preoccupations of a small subset of humanity (Medin et al., 2017). ","Forscher et al. - The Benefits, Barriers, and Risks of Big Team Scie.pdf"
"15 
 Conservatism can also manifest in project selection. The primary purpose of big team 
science is to scale up the resources that can be invested in a given project. However, this very 
virtue may induce big team science collaborations to select projects that they perceive as “safe” – 
both in terms of whether they can be feasibly executed and in terms of the degree to which they 
deviate from the scientific mainstream. Given that some degree of theoretical risk is necessary to 
spur scientific progress (Devezer et al., 2019; Meehl, 1978), this theoretical conservatism could 
feasibly slow scientific progress. 
Mitigating this risk.  To mitigate the risk of conservatism in personnel selection, we can 
draw on a general slate of strategies to improve the inclusion of underrepresented groups in 
science (Enriquez, 1979; Henrich et al., 2010; Syed & Kathawalla, 2020). These solutions must 
begin at the start of big team science organizations and must be continually re-evaluated 
throughout the organization’s lifecycle. At the organization’s founding, the founders must 
proactively and systematically partner with researchers in a broad array of non-Western and non-
elite institutions – and especially those who are not part of the “virtual academic commons” 
formed by Twitter, Facebook, and science blogs. Once the organization has been founded, the 
organization’s leadership must create mechanisms that allow for mobility within the team so that 
members of underrepresented groups can rise to positions of leadership. However, even these 
measures will not be sustainable without the direct investment of material resources into research 
infrastructure in under-resourced world regions.  
Diversity in big team science personnel will not ensure diversity in big team science 
projects. Mitigating the risk of this type of conservatism requires maintaining a separation 
between the idea generation and the project implementation  phases of big team science projects. 
During the idea generation phase, much smaller groups of scientists can identify problems and 
approaches unconstrained from a broader team consensus. The smaller teams then develop 
proposals based on their ideas and submit them for consideration by the larger consortium. The 
larger team can even explicitly build in mechanisms to solicit proposals from teams whose 
perspectives may differ from the scientific mainstream – such as those from outside North 
America and Europe. 
Sustainability failures. The history of science is littered with promising initiatives and 
organizations that, once established, could not be sustained (Borgman et al., 2016). This problem 
recurs so frequently because scientific organizations are typically public-minded: they wish to 
provide a public good at little or no cost. Yet, this very public-mindedness invites free-ridership 
– people who will use the organization’s service but who are either unwilling or unable to 
support the organization financially (Neylon, 2017). 
This dynamic also threatens big team science organizations. All the initiatives listed in 
Table 2 were formed because the founders thought that large-scale collaboration could lead to 
better science rather than out of a desire to make money. If the initiative is intended to last for a 
single project, the project could survive using an ad hoc organizational structure run on the back 
of volunteer labor. However, continually creating one-shot organizations is wasteful and 
inefficient because such a model prevents the accumulation of organizational knowledge. If an 
initiative is to last beyond a single project, its leadership must at some point figure out how to 
generate the money required to retain long-term staff while not compromising the vision that 
inspired the project in the first place.  ","Forscher et al. - The Benefits, Barriers, and Risks of Big Team Scie.pdf"
"16 
 Mitigating this risk.  The first step to mitigating the risk of a sustainability failure is to 
recognize that organizations cannot support themselves for free. If we want to have organizations 
that are capable of coordinating very large groups of scientists, those organizations must receive 
financial support so that they can generate and maintain the knowledge, staff, and other scientific 
resources necessary to fulfill their function. A corollary of this dictum is that, over the long haul, 
at least some skilled positions in a big team science organization must receive monetary 
compensation. Running entirely on volunteerism risks exploiting the idealistic people willing to 
donate their time (Kim et al., 2020), leading to burnout and interpersonal conflict. 
The second step to mitigating this risk involves creating and following a sustainability 
plan (for an example, see Forscher & IJzerman, 2021) that maps out how the organization will 
generate the funds necessary to maintain itself. The funding models could entail what is most 
familiar to academic scientists: designating some team members as grantwriters who spend part 
of their time identifying and applying for large grants to support the big team science 
organization. However, this grantwriting model puts big team science organizations at the whims 
of large funders, who are often only willing to spend money on projects that generate 
discoveries, not the infrastructure that makes the discoveries possible (Zakaria et al., 2021). Most 
science grantmakers also rely on competitive peer review to select the most “worthy” projects, a 
funding model that risks inefficiency because it encourages grantwriters to spend more time on 
grants than the activities the grants would fund (Gross & Bergstrom, 2019). For these reasons, 
we believe that a grantwriting-based funding model carries inherent risks of both research waste 
and instability in personnel due to feast-or-famine funding cycles. 
We believe that a more promising approach is to use one of the funding models that have 
led to sustainable funding for other large public-minded scientific organizations (Neylon, 2017). 
These funding models will likely involve either creating a system that “taxes” all beneficiaries of 
the big team science infrastructure by, say, imposing membership dues rather as a scientific 
society does, or, alternatively providing the infrastructure as a byproduct of selling another 
service. The other service could involve many things, but it should leverage what big team 
science organizations already do well – running multi-site studies. Thus, the organization might 
run some multi-site studies for a fee, or it might sell a service that it has needed to perfect in 
order to run multi-site studies, such as translation or project management. We believe that all 
these options are viable, though each has their own tradeoffs; membership dues could make the 
big team science organization inaccessible to lower-resourced members, whereas selling services 
could introduce conflicts of interest that threaten the organization’s mission. Choosing and 
following a sustainability plan is necessary to maintain any public-minded organization, but the 
tradeoffs of the available options must also be evaluated with care. 
Mega-mistakes . The final risk of big team science is that of what we term mega-mistakes . 
Big team science’s primary virtue is its ability to “scale up” small projects into big ones by 
pooling resources across labs. However, this very virtue makes errors all the more costly: these 
errors risk wasting much more resources than would be wasted on a smaller project. These errors 
can even occur at the time of project selection if the topic of the project is not one that deserves 
the high investment of resources that big team science brings. 
Errors in big team projects often have no one simple cause. Consider the example of the 
Human Brain Project. This project aimed to unlock massive advances for neuroscience by ","Forscher et al. - The Benefits, Barriers, and Risks of Big Team Scie.pdf"
"17 
 developing and building large-scale computer simulations of brain regions, and eventually of 
entire brains (Markram, 2012). The leader of the project, Henry Markram, set an ambitious 
agenda; in 2009, he claimed that, after only ten more years of development the technology that 
powered the project would allow the simulation of an entire rodent brain, an achievement that he 
believed would revolutionize neuroscience (Abbott, 2020). The project’s vision and ambition 
attracted major investment from both universities and funders; in 2015, the project had 112 
university partners and a project budget of over €1 billion (Abbott, 2015). 
Yet the project was plagued with problems. These problems spanned multiple domains, 
ranging from concerns that the project was monopolizing resources that could be allocated to 
other worthy projects, to disagreement over the project’s core aims, to dissatisfaction with the 
project’s autocratic leadership structure (Abbott, 2014). Regardless of the specific causes, by 
2015 most scientists agreed that the project was not living up to Henry Markram’s original vision 
(Abbott, 2015). Markram was removed from his leadership position the following year (Abbott, 
2020). The project is still ongoing, but whether it will produce the revolution in neuroscience 
that Markram promised is anything but clear. 
Mitigating this risk . As the example of the Human Brain Project illustrates, mega-
mistakes can emerge due to a failure to manage other risks of big team science, such as the risk 
of mis-management. Thus, mitigating other risks of big team science may be one useful way of 
mitigating the risks of mega-mistakes. 
However, big team science organizations can also proactively implement quality control 
processes that minimize the risks of mega-mistakes more directly. These quality control 
processes entail everything from formally reviewing project proposals, to instituting code 
review, pilot tests, and project “soft launches”, to formal methods of optionally stopping data 
collection to avoid overinvestment in bad ideas (Lakens, 2014; Schönbrodt et al., 2017). Quality 
control can also involve partnering with outside scientific organizations that implement their own 
methods of quality control, such as journals that administer Registered Reports.  
An additional promising strategy involves instituting audits of scientific processes by 
people who are formally independent of the big team science collaboration. These audits can 
take the form of research design review by outside experts, systems of back-translation to check 
the quality of forward-translation, formal code review, and even “red teams” who receive 
bounties for spotting bugs and other project flaws (Lakens, 2020).  
All these quality control methods have high value in solo PI projects, but take on renewed 
importance in projects that command resources on the scale of big team science. 
Conclusion  
 Big team science will never be a wholesale replacement to solo science – nor should it 
be. Absent the coordination constraints of a large team, solo scientists have the freedom to 
flexibly explore ideas that might be infeasible in a larger group setting without creating a risk of 
squandering a scale of resources that rises to the level of a “mega-mistake”. 
What big team science can do is vastly “scale up” the amount of resources – in other 
words, the money, person-hours, and specialized expertise – that can be deployed in a single, ","Forscher et al. - The Benefits, Barriers, and Risks of Big Team Scie.pdf"
"18 
 well-chosen and well-vetted project. This increased resource investment can result in projects 
that are larger, more rigorous, and more representative of humanity. We believe this virtue is not 
to be underestimated, as low resource investment is likely a key ultimate cause of why 
psychology’s progress has been slow. Moreover, other disciplines may find themselves facing 
similar problems. Ecologists identified similar problems in their own discipline, and team 
science is emerging as a potential solution (Fraser et al., 2013). Pre-clinical cancer biology faces 
problems even conducting the studies needed to assess whether a replicability problem exists 
(Friedl, 2019). A greater focus on big team science may help pre-clinical cancer biology “scale 
up” the resources devoted to the typical study in the discipline, which may help lay the 
groundwork to allow these replication studies to be conducted. 
We believe that big team science has the potential to simultaneously tackle the many 
challenges that psychology faces. However, to fulfill this potential, psychological scientists must 
recognize and manage the many barriers and risks that this approach entails. If properly managed 
to leverage its virtues while mitigating its risks, we believe that big team science can be 
instrumental in the movement to build more reliable, informative, and rigorous science. 
 
  ","Forscher et al. - The Benefits, Barriers, and Risks of Big Team Scie.pdf"
"19 
 References 
 
23andMe Research Team, COGENT (Cognitive Genomics Consortium), Social Science Genetic 
Association Consortium, Lee, J. J., Wedow, R., Okbay, A., Kong, E., Maghzian, O., 
Zacher, M., Nguyen-Viet, T. A., Bowers, P., Sidorenko, J., Karlsson Linnér, R., Fontana, 
M. A., Kundu, T., Lee, C., Li, H., Li, R., Royer, R., … Cesarini, D. (2018). Gene 
discovery and polygenic prediction from a genome-wide association study of educational 
attainment in 1.1 million individuals. Nature Genetics , 50(8), 1112–1121. 
https://doi.org/10.1038/s41588-018-0147-3 
Abbott, A. (2014). Row hits flagship brain plan. Nature, 511(7508), 133–134. 
https://doi.org/10.1038/511133a 
Abbott, A. (2015). Human Brain Project votes for leadership change. Nature, nature.2015.17060. 
https://doi.org/10.1038/nature.2015.17060 
Abbott, A. (2020). Documentary follows implosion of billion-euro brain project. Nature, 
588(7837), 215–216. https://doi.org/10.1038/d41586-020-03462-3 
ALICE collaboration. (2010). Alignment of the ALICE Inner Tracking System with cosmic-ray 
tracks. Journal of Instrumentation , 5(03), P03003–P03003. https://doi.org/10.1088/1748-
0221/5/03/P03003 
Arslan, R. C., Walther, M. P., & Tata, C. S. (2019). formr: A study framework allowing for 
automated feedback generation and complex longitudinal experience-sampling studies 
using R. Behavior Research Methods . https://doi.org/10.3758/s13428-019-01236-y 
Astuti, R., & Bloch, M. (2012). Anthropologists as Cognitive Scientists. Topics in Cognitive 
Science, 4(3), 453–461. https://doi.org/10.1111/j.1756-8765.2012.01191.x ","Forscher et al. - The Benefits, Barriers, and Risks of Big Team Scie.pdf"
"20 
 Azoulay, P., Fons-Rosen, C., & Zivin, J. S. G. (2019). Does Science Advance One Funeral at a 
Time? American Economic Review , 109(8), 2889–2920. 
https://doi.org/10.1257/aer.20161574 
Bakker, C. J. (1955). CERN: European Organization for Nuclear Research. Physics Today , 8, 8–
13. 
Baum, J. R., & Wally, S. (2003). Strategic decision speed and firm performance. Strategic 
Management Journal , 24(11), 1107–1129. https://doi.org/10.1002/smj.343 
Bender, A., Beller, S., & Nersessian, N. J. (2015). Diversity as Asset. Topics in Cognitive 
Science, 7(4), 677–688. https://doi.org/10.1111/tops.12161 
Berdahl, J. L., & Raver, J. L. (2011). Sexual harassment. In S. Zedeck (Ed.), APA handbook of 
industrial and organizational psychology, Vol 3: Maintaining, expanding, and 
contracting the organization.  (pp. 641–669). American Psychological Association. 
https://doi.org/10.1037/12171-018 
Birnholtz, J. (2008). When Authorship Isn’t Enough: Lessons from CERN on the Implications of 
Formal and Informal Credit Attribution Mechanisms in Collaborative Research. The 
Journal of Electronic Publishing , 11(1). https://doi.org/10.3998/3336451.0011.105 
Borgman, C. L., Darch, P. T., Sands, A. E., & Golshan, M. S. (2016). The Durability and 
Fragility of Knowledge Infrastructures: Lessons Learned from Astronomy. 
ArXiv:1611.00055 [Astro-Ph] . http://arxiv.org/abs/1611.00055 
Botvinik-Nezer, R., Holzmeister, F., Camerer, C. F., Dreber, A., Huber, J., Johannesson, M., 
Kirchler, M., Iwanir, R., Mumford, J. A., Adcock, A., Avesani, P., Baczkowski, B., 
Bajracharya, A., Bakst, L., Ball, S., Barilari, M., Bault, N., Beaton, D., Beitner, J., … ","Forscher et al. - The Benefits, Barriers, and Risks of Big Team Scie.pdf"
"21 
 Schonberg, T. (2019). Variability in the analysis of a single neuroimaging dataset by 
many teams  [Preprint]. Neuroscience. https://doi.org/10.1101/843193 
Brumfiel, G. (2011). High-energy physics: Down the petabyte highway. Nature, 469(7330), 
282–283. https://doi.org/10.1038/469282a 
Bryant, P. J. (1994). A Brief History and Review of Accelerators  (No. 261062). CERN. 
Button, K. S., Ioannidis, J. P. A., Mokrysz, C., Nosek, B. A., Flint, J., Robinson, E. S. J., & 
Munafò, M. R. (2013). Power failure: Why small sample size undermines the reliability 
of neuroscience. Nature Reviews Neuroscience , 14(5), 365–376. 
https://doi.org/10.1038/nrn3475 
Byers-Heinlein, K., Bergmann, C., Davies, C., Frank, M. C., Hamlin, J. K., Kline, M., 
Kominsky, J. F., Kosie, J. E., Lew-Williams, C., Liu, L., Mastroberardino, M., Singh, L., 
Waddell, C. P. G., Zettersten, M., & Soderstrom, M. (2020). Building a collaborative 
psychological science: Lessons learned from ManyBabies 1. Canadian 
Psychology/Psychologie Canadienne , 61(4), 349–363. 
https://doi.org/10.1037/cap0000216 
Coles, N. A., March, D. S., Marmolejo-Ramos, F., Arinze, N. C., Ndukaihe, I. L. G., Özdoğru, 
A. A., Aczel, B., Hajdu, N., Nagy, T., Basnight-Brown, D., Zambrano, D., Foroni, F., 
Willis, M., Pfuhl, G., Kaminski, G., IJzerman, H., Vezirian, K., Banaruee, H., Suarez, I., 
… Liuzza, M. T. (2019). A Multi-Lab Test of the Facial Feedback Hypothesis by The 
Many Smiles Collaboration  [Preprint]. PsyArXiv. https://doi.org/10.31234/osf.io/cvpuw 
Corvin, A., Craddock, N., & Sullivan, P. F. (2010). Genome-wide association studies: A primer. 
Psychological Medicine , 40(7), 1063–1077. https://doi.org/10.1017/S0033291709991723 ","Forscher et al. - The Benefits, Barriers, and Risks of Big Team Scie.pdf"
"22 
 Cuccolo, K., Irgens, M. S., Zlokovich, M. S., Grahe, J., & Edlund, J. E. (2021). What 
Crowdsourcing Can Offer to Cross-Cultural Psychological Science. Cross-Cultural 
Research, 55(1), 3–28. https://doi.org/10.1177/1069397120950628 
Dannlowski, U., Ohrmann, P., Bauer, J., Deckert, J., Hohoff, C., Kugel, H., Arolt, V., Heindel, 
W., Kersting, A., Baune, B. T., & Suslow, T. (2008). 5-HTTLPR Biases Amygdala 
Activity in Response to Masked Facial Expressions in Major Depression. 
Neuropsychopharmacology , 33(2), 418–424. https://doi.org/10.1038/sj.npp.1301411 
Devezer, B., Nardin, L. G., Baumgaertner, B., & Buzbas, E. O. (2019). Scientific discovery in a 
model-centric framework: Reproducibility, innovation, and epistemic diversity. PLOS 
ONE, 14(5), e0216125. https://doi.org/10.1371/journal.pone.0216125 
Donoho, D., Arian, M., Imam, R., Morteza, S., & Stodden, V. (2008). 15 Years of Reproducible 
Research in Computational Harmonic Analysis  (pp. 1–26) [Technical Report]. 
Department of Statistics, Stanford University. 
http://statweb.stanford.edu/~wavelab/DonohoEtAlCISESubmission.pdf 
Ebersole, C. R., Atherton, O. E., Belanger, A. L., Skulborstad, H. M., Allen, J. M., Banks, J. B., 
Baranski, E., Bernstein, M. J., Bonfiglio, D. B. V., Boucher, L., Brown, E. R., Budiman, 
N. I., Cairo, A. H., Capaldi, C. A., Chartier, C. R., Chung, J. M., Cicero, D. C., Coleman, 
J. A., Conway, J. G., … Nosek, B. A. (2016). Many Labs 3: Evaluating participant pool 
quality across the academic semester via replication. Journal of Experimental Social 
Psychology , 67, 68–82. https://doi.org/10.1016/j.jesp.2015.10.012 
Enriquez, V. G. (1979). Towards cross-cultural knowledge through cross-indigenous methods 
and perspective. Philippine Journal of Psychology , 12(1), 9–15. ","Forscher et al. - The Benefits, Barriers, and Risks of Big Team Scie.pdf"
"23 
 Ervin, A.-M., Taylor, H. A., & Ehrhardt, S. (2016). NIH Policy on Single-IRB Review—A New 
Era in Multicenter Studies. New England Journal of Medicine , 375(24), 2315–2317. 
https://doi.org/10.1056/NEJMp1608766 
Espinosa, J. P. A., Levcovitz, G. B., Bianchi, R.-M., Brock, I., Carli, T., Castro, N. F., Ciocio, 
A., Colautti, M., Menezes, A. C. D. S., da Fonseca, G. D. O., Alves, L. D. M., Hoecker, 
A., Ramos, B. L., Pinhão, G. L. L., Maidantchik, C., Malek, F., McPherson, R., Picco, G., 
& Santos, M. T. D. (2020). The ATLAS Publication Process Supported by Continuous 
Integration and Web Framework. ArXiv:2005.06989 [Hep-Ex] . 
http://arxiv.org/abs/2005.06989 
Fanelli, D. (2010). “Positive” results increase down the hierarchy of the sciences. PLoS ONE , 
5(4), e10068. https://doi.org/10.1371/journal.pone.0010068 
Finkel, E. J., Eastwick, P. W., & Reis, H. T. (2017). Replicability and other features of a high-
quality science: Toward a balanced and empirical approach. Journal of Personality and 
Social Psychology , 113(2), 244–253. https://doi.org/10.1037/pspi0000075 
Forscher, P. S., & IJzerman, H. (2021, January 11). How should we fund the PSA? 
Psychological Science Accelerator Blog . https://psysciacc.org/2021/01/11/how-should-
we-fund-the-psa/ 
Forsyth, D. R. (2014). Group dynamics  (6th ed). Wadsworth Cengage Learning. 
Fraser, L. H., Henry, H. A., Carlyle, C. N., White, S. R., Beierkuhnlein, C., Cahill, J. F., Casper, 
B. B., Cleland, E., Collins, S. L., Dukes, J. S., Knapp, A. K., Lind, E., Long, R., Luo, Y., 
Reich, P. B., Smith, M. D., Sternberg, M., & Turkington, R. (2013). Coordinated 
distributed experiments: An emerging tool for testing global hypotheses in ecology and ","Forscher et al. - The Benefits, Barriers, and Risks of Big Team Scie.pdf"
"24 
 environmental science. Frontiers in Ecology and the Environment , 11(3), 147–155. 
https://doi.org/10.1890/110279 
Frazier, M. L., Fainshmidt, S., Klinger, R. L., Pezeshkan, A., & Vracheva, V. (2017). 
Psychological Safety: A Meta-Analytic Review and Extension. Personnel Psychology , 
70(1), 113–165. https://doi.org/10.1111/peps.12183 
Friedl, P. (2019). Rethinking research into metastasis. ELife, 8, e53511. 
https://doi.org/10.7554/eLife.53511 
Gillespie, N. A., Whitfield, J. B., Williams, B., Heath, A. C., & Martin, N. G. (2005). The 
relationship between stressful life events, the serotonin transporter (5-HTTLPR) genotype 
and major depression. Psychological Medicine , 35(1), 101–111. 
https://doi.org/10.1017/S0033291704002727 
Gotlib, I. H., Joormann, J., Minor, K. L., & Hallmayer, J. (2008). HPA Axis Reactivity: A 
Mechanism Underlying the Associations Among 5-HTTLPR, Stress, and Depression. 
Biological Psychiatry , 63(9), 847–851. https://doi.org/10.1016/j.biopsych.2007.10.008 
Gross, K., & Bergstrom, C. T. (2019). Contest models highlight inherent inefficiencies of 
scientific funding competitions. PLOS Biology , 17(1), e3000065. 
https://doi.org/10.1371/journal.pbio.3000065 
Hardwicke, T. E., Mathur, M. B., MacDonald, K., Nilsonne, G., Banks, G. C., Kidwell, M. C., 
Hofelich Mohr, A., Clayton, E., Yoon, E. J., Henry Tessler, M., Lenne, R. L., Altman, S., 
Long, B., & Frank, M. C. (2018). Data availability, reusability, and analytic 
reproducibility: Evaluating the impact of a mandatory open data policy at the journal 
Cognition . Royal Society Open Science , 5(8), 180448. 
https://doi.org/10.1098/rsos.180448 ","Forscher et al. - The Benefits, Barriers, and Risks of Big Team Scie.pdf"
"25 
 Harry, G. M. & the LIGO Scientific Collaboration. (2010). Advanced LIGO: The next 
generation of gravitational wave detectors. Classical and Quantum Gravity , 27(8), 
084006. https://doi.org/10.1088/0264-9381/27/8/084006 
Heils, A., Teufel, A., Petri, S., Stöber, G., Riederer, P., Bengel, D., & Lesch, K. P. (1996). 
Allelic Variation of Human Serotonin Transporter Gene Expression. Journal of 
Neurochemistry , 66(6), 2621–2624. https://doi.org/10.1046/j.1471-
4159.1996.66062621.x 
Henrich, J., Heine, S. J., & Norenzayan, A. (2010). The weirdest people in the world? 
Behavioral and Brain Sciences , 33(2–3), 61–83. 
https://doi.org/10.1017/S0140525X0999152X 
Holcombe, A. O. (2019). Contributorship, Not Authorship: Use CRediT to Indicate Who Did 
What. Publications , 7(3), 48. https://doi.org/10.3390/publications7030048 
Jones, B. C., DeBruine, L. M., Flake, J. K., Liuzza, M. T., Antfolk, J., Arinze, N. C., Ndukaihe, 
I. L. G., Bloxsom, N. G., Lewis, S. C., Foroni, F., Willis, M. L., Cubillas, C. P., Vadillo, 
M. A., Turiegano, E., Gilead, M., Simchon, A., Saribay, S. A., Owsley, N. C., Jang, C., 
… Coles, N. A. (2021). To which world regions does the valence-dominance model of 
social perception apply? Nature Human Behaviour . https://doi.org/10.1038/s41562-020-
01007-2 
Kaplay, S. (2016, July 6). In clinical trials, for-profit review boards are taking over for hospitals. 
Should they? STAT. https://www.statnews.com/2016/07/06/institutional-review-boards-
commercial-irbs/ 
Kim, J. Y., Campbell, T. H., Shepherd, S., & Kay, A. C. (2020). Understanding contemporary 
forms of exploitation: Attributions of passion serve to legitimize the poor treatment of ","Forscher et al. - The Benefits, Barriers, and Risks of Big Team Scie.pdf"
"26 
 workers. Journal of Personality and Social Psychology , 118(1), 121–148. 
https://doi.org/10.1037/pspi0000190 
Klein, R. A., Cook, C. L., Ebersole, C. R., Vitiello, C. A., Nosek, B. A., Chartier, C. R., 
Christopherson, C. D., Clay, S., Collisson, B., Crawford, J., Cromar, R., Dudley, D., 
Gardiner, G., Gosnell, C., Grahe, J. E., Hall, C., Joy-Gaba, J. A., Legg, A. M., Levitan, 
C., … Ratliff, K. A. (2019). Many Labs 4: Failure to Replicate Mortality Salience Effect 
With and Without Original Author Involvement  [Preprint]. PsyArXiv. 
https://doi.org/10.31234/osf.io/vef2c 
Klein, R. A., Ratliff, K. A., Vianello, M., Adams, R. B., Bahník, Š., Bernstein, M. J., Bocian, K., 
Brandt, M. J., Brooks, B., Brumbaugh, C. C., Cemalcilar, Z., Chandler, J., Cheong, W., 
Davis, W. E., Devos, T., Eisner, M., Frankowska, N., Furrow, D., Galliani, E. M., … 
Nosek, B. A. (2014). Investigating Variation in Replicability: A “Many Labs” 
Replication Project. Social Psychology , 45(3), 142–152. https://doi.org/10.1027/1864-
9335/a000178 
Klein, R. A., Vianello, M., Hasselman, F., Adams, B. G., Adams, R. B., Alper, S., Aveyard, M., 
Axt, J. R., Babalola, M. T., Bahník, Š., Batra, R., Berkics, M., Bernstein, M. J., Berry, D. 
R., Bialobrzeska, O., Binan, E. D., Bocian, K., Brandt, M. J., Busching, R., … Nosek, B. 
A. (2018). Many Labs 2: Investigating Variation in Replicability Across Samples and 
Settings. Advances in Methods and Practices in Psychological Science , 1(4), 443–490. 
https://doi.org/10.1177/2515245918810225 
Konovsky, M. A. (2000). Understanding Procedural Justice and Its Impact on Business 
Organizations. Journal of Management , 26(3), 489–511. 
https://doi.org/10.1177/014920630002600306 ","Forscher et al. - The Benefits, Barriers, and Risks of Big Team Scie.pdf"
"27 
 Kossinets, G., & Watts, D. J. (2009). Origins of Homophily in an Evolving Social Network. 
American Journal of Sociology , 115(2), 405–450. https://doi.org/10.1086/599247 
Krige, J. (2004). I. I. Rabi and the Birth of CERN. Physics Today , 57(9), 44–48. 
https://doi.org/10.1063/1.1809091 
Lai, C. K., Marini, M., Lehr, S. A., Cerruti, C., Shin, J.-E. L., Joy-Gaba, J. A., Ho, A. K., 
Teachman, B. A., Wojcik, S. P., Koleva, S. P., Frazier, R. S., Heiphetz, L., Chen, E. E., 
Turner, R. N., Haidt, J., Kesebir, S., Hawkins, C. B., Schaefer, H. S., Rubichi, S., … 
Nosek, B. A. (2014). Reducing implicit racial preferences: I. A comparative investigation 
of 17 interventions. Journal of Experimental Psychology: General , 143(4), 1765–1785. 
https://doi.org/10.1037/a0036260 
Lai, C. K., Skinner, A. L., Cooley, E., Murrar, S., Brauer, M., Devos, T., Calanchini, J., Xiao, Y. 
J., Pedram, C., Marshburn, C. K., Simon, S., Blanchar, J. C., Joy-Gaba, J. A., Conway, J., 
Redford, L., Klein, R. A., Roussos, G., Schellhaas, F. M. H., Burns, M., … Nosek, B. A. 
(2016). Reducing implicit racial preferences: II. Intervention effectiveness across time. 
Journal of Experimental Psychology: General , 145(8), 1001–1016. 
https://doi.org/10.1037/xge0000179 
Lakens, D. (2014). Performing high-powered studies efficiently with sequential analyses: 
Sequential analyses. European Journal of Social Psychology , 44(7), 701–710. 
https://doi.org/10.1002/ejsp.2023 
Lakens, D. (2020). Pandemic researchers—Recruit your own best critics. Nature, 581(7807), 
121–121. https://doi.org/10.1038/d41586-020-01392-8 
Lakens, D., & DeBruine, L. M. (2021). Improving Transparency, Falsifiability, and Rigor by 
Making Hypothesis Tests Machine-Readable. Advances in Methods and Practices in ","Forscher et al. - The Benefits, Barriers, and Risks of Big Team Scie.pdf"
"28 
 Psychological Science , 4(2), 251524592097094. 
https://doi.org/10.1177/2515245920970949 
Landy, J. F., Jia, M. (Liam), Ding, I. L., Viganola, D., Tierney, W., Dreber, A., Johannesson, M., 
Pfeiffer, T., Ebersole, C. R., Gronau, Q. F., Ly, A., van den Bergh, D., Marsman, M., 
Derks, K., Wagenmakers, E.-J., Proctor, A., Bartels, D. M., Bauman, C. W., Brady, W. J., 
… Uhlmann, E. L. (2020). Crowdsourcing hypothesis tests: Making transparent how 
design choices shape research results. Psychological Bulletin , 146(5), 451–479. 
https://doi.org/10.1037/bul0000220 
LeBel, E. P., Berger, D., Campbell, L., & Loving, T. J. (2017). Falsifiability is not optional. 
Journal of Personality and Social Psychology , 113(2), 254–261. 
https://doi.org/10.1037/pspi0000106 
Loughry, M., & Amason, A. (2014). Why won’t task conflict cooperate? Deciphering stubborn 
results. International Journal of Conflict Management , 25(4), 333–358. 
https://doi.org/10.1108/IJCMA-01-2014-0005 
Markram, H. (2012). The Human Brain Project. Scientific American , 306(6), 50–55. JSTOR. 
McNutt, M. K., Bradford, M., Drazen, J. M., Hanson, B., Howard, B., Jamieson, K. H., Kiermer, 
V., Marcus, E., Pope, B. K., Schekman, R., Swaminathan, S., Stang, P. J., & Verma, I. M. 
(2018). Transparency in authors’ contributions and responsibilities to promote integrity in 
scientific publication. Proceedings of the National Academy of Sciences , 115(11), 2557–
2560. https://doi.org/10.1073/pnas.1715374115 
McPherson, M., Smith-Lovin, L., & Cook, J. M. (2001). Birds of a Feather: Homophily in Social 
Networks. Annual Review of Sociology , 27(1), 415–444. 
https://doi.org/10.1146/annurev.soc.27.1.415 ","Forscher et al. - The Benefits, Barriers, and Risks of Big Team Scie.pdf"
"29 
 Medin, D., Ojalehto, B., Marin, A., & Bang, M. (2017). Systems of (non-)diversity. Nature 
Human Behaviour , 1(5), 0088. https://doi.org/10.1038/s41562-017-0088 
Meehl, P. E. (1978). Theoretical risks and tabular asterisks: Sir Karl, Sir Ronald, and the slow 
progress of soft psychology. Journal of Consulting and Clinical Psychology , 46, 806–
834. https://doi.org/10.1037/0022-006X.46.4.806 
Moshontz, H., Campbell, L., Ebersole, C. R., IJzerman, H., Urry, H. L., Forscher, P. S., Grahe, J. 
E., McCarthy, R. J., Musser, E. D., Antfolk, J., Castille, C. M., Evans, T. R., Fiedler, S., 
Flake, J. K., Forero, D. A., Janssen, S. M. J., Keene, J. R., Protzko, J., Aczel, B., … 
Chartier, C. R. (2018). The Psychological Science Accelerator: Advancing Psychology 
Through a Distributed Collaborative Network. Advances in Methods and Practices in 
Psychological Science , 1(4), 501–515. https://doi.org/10.1177/2515245918797607 
Moshontz, H., Ebersole, C. R., Weston, S. J., & Klein, R. A. (2019). A Guide for Many Authors: 
Writing Manuscripts in Large Collaborations  [Preprint]. PsyArXiv. 
https://doi.org/10.31234/osf.io/92xhd 
Newell, A. (1973). You can’t play 20 questions with nature and win: Projective comments on the 
papers of this symposium . Visual Information Processing, Carnegie-Mellon University. 
Neylon, C. (2017). Sustaining Scholarly Infrastructures through Collective Action: The Lessons 
that Olson can Teach us. KULA: Knowledge Creation, Dissemination, and Preservation 
Studies, 1(1), 3. https://doi.org/10.5334/kula.7 
Obels, P., Lakens, D., Coles, N. A., Gottfried, J., & Green, S. A. (2019). Analysis of Open Data 
and Computational Reproducibility in Registered Reports in Psychology  [Preprint]. 
PsyArXiv. https://doi.org/10.31234/osf.io/fk8vh ","Forscher et al. - The Benefits, Barriers, and Risks of Big Team Scie.pdf"
"30 
 Open Science Collaboration. (2015). Estimating the reproducibility of psychological science. 
Science, 349(6251), aac4716–aac4716. https://doi.org/10.1126/science.aac4716 
Osborne, T. L., & Luoma, J. B. (2018). Overcoming a primary barrier to practice-based research: 
Access to an institutional review board (IRB) for independent ethics review. 
Psychotherapy , 55(3), 255–262. https://doi.org/10.1037/pst0000166 
Rad, M. S., Martingano, A. J., & Ginges, J. (2018). Toward a psychology of Homo sapiens: 
Making psychological science more representative of the human population. Proceedings 
of the National Academy of Sciences , 115(45), 11401–11405. 
https://doi.org/10.1073/pnas.1721165115 
Rahim, M. A. (2003). Toward a Theory of Managing Organizational Conflict. SSRN Electronic 
Journal. https://doi.org/10.2139/ssrn.437684 
Rieckmann, N. (2009). Gene-Environment Interactions and Depression. JAMA, 302(17), 1859. 
https://doi.org/10.1001/jama.2009.1578 
Schneider, C. (2015). The censor’s hand the misregulation of human-subject research . The MIT 
Press. 
Schönbrodt, F. D., Wagenmakers, E.-J., Zehetleitner, M., & Perugini, M. (2017). Sequential 
hypothesis testing with Bayes factors: Efficiently testing mean differences. Psychological 
Methods, 22(2), 322–339. https://doi.org/10.1037/met0000061 
Sears, D. O. (1986). College sophomores in the laboratory: Influences of a narrow data base on 
social psychology’s view of human nature. Journal of Personality and Social 
Psychology , 51(3), 515–530. https://doi.org/10.1037/0022-3514.51.3.515 
Silberzahn, R., Uhlmann, E. L., Martin, D. P., Anselmi, P., Aust, F., Awtrey, E., Bahník, Š., Bai, 
F., Bannard, C., Bonnier, E., Carlsson, R., Cheung, F., Christensen, G., Clay, R., Craig, ","Forscher et al. - The Benefits, Barriers, and Risks of Big Team Scie.pdf"
"31 
 M. A., Dalla Rosa, A., Dam, L., Evans, M. H., Flores Cervantes, I., … Nosek, B. A. 
(2018). Many Analysts, One Data Set: Making Transparent How Variations in Analytic 
Choices Affect Results. Advances in Methods and Practices in Psychological Science , 
1(3), 337–356. https://doi.org/10.1177/2515245917747646 
Simons, D. J., Holcombe, A. O., & Spellman, B. A. (2014). An Introduction to Registered 
Replication Reports at Perspectives on Psychological Science . Perspectives on 
Psychological Science , 9(5), 552–555. https://doi.org/10.1177/1745691614543974 
Smaldino, P. E., & McElreath, R. (2016). The natural selection of bad science. Royal Society 
Open Science , 3(9), 160384. https://doi.org/10.1098/rsos.160384 
Spellman, B. A. (2015). A Short (Personal) Future History of Revolution 2.0. Perspectives on 
Psychological Science , 10(6), 886–899. https://doi.org/10.1177/1745691615609918 
Stokols, D. (2006). Toward a Science of Transdisciplinary Action Research. American Journal 
of Community Psychology , 38(1–2), 79–93. https://doi.org/10.1007/s10464-006-9060-5 
Stokols, D., Misra, S., Moser, R. P., Hall, K. L., & Taylor, B. K. (2008). The Ecology of Team 
Science. American Journal of Preventive Medicine , 35(2), S96–S115. 
https://doi.org/10.1016/j.amepre.2008.05.003 
Sullivan, P. F. (2010). The Psychiatric GWAS Consortium: Big Science Comes to Psychiatry. 
Neuron, 68(2), 182–186. https://doi.org/10.1016/j.neuron.2010.10.003 
Syed, M., & Kathawalla, U.-K. (2020). Cultural Psychology, Diversity, and Representation in 
Open Science  [Preprint]. PsyArXiv. https://doi.org/10.31234/osf.io/t7hp2 
Teasley, S., & Wolinsky, S. (2001). Scientific Collaborations at a Distance. Science, 292(5525), 
2254–2255. https://doi.org/10.1126/science.1061619 ","Forscher et al. - The Benefits, Barriers, and Risks of Big Team Scie.pdf"
"32 
 Thalmayer, A. G., Toscanelli, C., & Arnett, J. J. (2020). The neglected 95% revisited: Is 
American psychology becoming less American? American Psychologist . 
https://doi.org/10.1037/amp0000622 
The ATLAS collaboration, Aaboud, M., Aad, G., Abbott, B., Abdinov, O., Abeloos, B., Abidi, S. 
H., AbouZeid, O. S., Abraham, N. L., Abramowicz, H., Abreu, H., Abreu, R., Abulaiti, 
Y., Acharya, B. S., Adachi, S., Adamczyk, L., Adelman, J., Adersberger, M., Adye, T., 
… Zwalinski, L. (2018). Searches for heavy ZZ and ZW resonances in the ℓℓqq and ννqq 
final states in pp collisions at s = 13 $$ \sqrt{s}=13 $$ TeV with the ATLAS detector. 
Journal of High Energy Physics , 2018(3), 9. https://doi.org/10.1007/JHEP03(2018)009 
Tiokhin, L., Hackman, J., Munira, S., Jesmin, K., & Hruschka, D. (2019). Generalizability is not 
optional: Insights from a cross-cultural study of social discounting. Royal Society Open 
Science, 6(2), 181386. https://doi.org/10.1098/rsos.181386 
Tiokhin, L., Panchanathan, K., Smaldino, P. E., & Lakens, D. (2021). Shifting the level of 
selection in science  [Preprint]. MetaArXiv. https://doi.org/10.31222/osf.io/juwck 
Tubre, T. C., & Collins, J. M. (2000). Jackson and Schuler (1985) Revisited: A Meta-Analysis of 
the Relationships Between Role Ambiguity, Role Conflict, and Job Performance. Journal 
of Management , 26(1), 155–169. https://doi.org/10.1177/014920630002600104 
Uhlmann, E. L., Ebersole, C. R., Chartier, C. R., Errington, T. M., Kidwell, M. C., Lai, C. K., 
McCarthy, R. J., Riegelman, A., Silberzahn, R., & Nosek, B. A. (2019). Scientific Utopia 
III: Crowdsourcing Science. Perspectives on Psychological Science , 14(5), 711–733. 
https://doi.org/10.1177/1745691619850561 ","Forscher et al. - The Benefits, Barriers, and Risks of Big Team Scie.pdf"
"33 
 Van Sell, M., Brief, A. P., & Schuler, R. S. (1981). Role Conflict and Role Ambiguity: 
Integration of the Literature and Directions for Future Research. Human Relations , 34(1), 
43–71. https://doi.org/10.1177/001872678103400104 
Vuorre, M., & Curley, J. P. (2018). Curating Research Assets: A Tutorial on the Git Version 
Control System. Advances in Methods and Practices in Psychological Science , 1(2), 219–
236. https://doi.org/10.1177/2515245918754826 
Wagge, J. R., Baciu, C., Banas, K., Nadler, J. T., Schwarz, S., Weisberg, Y., IJzerman, H., 
Legate, N., & Grahe, J. (2019). A Demonstration of the Collaborative Replication and 
Education Project: Replication Attempts of the Red-Romance Effect. Collabra: 
Psychology , 5(1), 5. https://doi.org/10.1525/collabra.177 
Wagge, J. R., Brandt, M. J., Lazarevic, L. B., Legate, N., Christopherson, C., Wiggins, B., & 
Grahe, J. E. (2019). Publishing Research With Undergraduate Students via Replication 
Work: The Collaborative Replications and Education Project. Frontiers in Psychology , 
10, 247. https://doi.org/10.3389/fpsyg.2019.00247 
Wu, L., Wang, D., & Evans, J. A. (2019). Large teams develop and small teams disrupt science 
and technology. Nature, 566(7744), 378–382. https://doi.org/10.1038/s41586-019-0941-9 
Yarkoni, T. (2019). The Generalizability Crisis  [Preprint]. PsyArXiv. 
https://doi.org/10.31234/osf.io/jqw35 
Zakaria, S., Grant, J., & Luff, J. (2021). Fundamental challenges in assessing the impact of 
research infrastructure. Health Research Policy and Systems , 19(1), 119. 
https://doi.org/10.1186/s12961-021-00769-z 
 ","Forscher et al. - The Benefits, Barriers, and Risks of Big Team Scie.pdf"
"Are some of science’s biggest questions 
simply unanswerable without 
redefining how research is done? 
This is the question that motivated 
the researchers who would later 
establish the ManyBabies Consortium: a 
grass-roots network of some 450 collabora -
tors from more than 200 institutions who 
pool resources to complete massive studies 
on infant development (see, for example, 
ref.  1). Human infants are perhaps the most 
powerful learning machines on the planet — 
and understanding how that learning occurs 
could inform artificial intelligence, public 
policy, education and more. Yet a full under-
standing of infant learning seemed difficult 
(if not impossible) under the current research 
model. Consider the question of what captures 
infants’ attention. Surely the probability that 
an infant will pay attention to, say, a rabbit, 
depends on presentation (for example, by 
a mother or a stranger), the child’s previ -
ous experiences with mammals, what else 
is present alongside the rabbit, and much 
more. Unpacking this effectively would 
require dozens of experimental conditions 
and hundreds of infant participants. But most 
research projects are run by individual prin -
cipal investigators and a shifting population 
of PhD students, meaning that data-collection 
efforts typically recruit fewer than 25 infants 
for each condition being tested2.
But what if researchers worked inter-
dependently and distributed work across 
many laboratories? Such consortia might be Researchers are creating 
grass-roots collaborative 
networks to tackle difficult 
questions in primate studies 
and more, but they need 
funding and other support.Build up big-team science
Nicholas A. Coles, J. Kiley Hamlin, Lauren L. Sullivan, Timothy H. Parker & Drew Altschul
Tamarins are one of more than 40 primate species that researchers can study through the ManyPrimates collaboration.GETTY
Nature | Vol 601 | 27 January 2022 | 505
Setting the agenda in research
Comment
©
 2022
 Springer
 Nature
 Limited.
 All
 rights
 reserved.",Coles et al. - 2022 - Build up big-team science.pdf
"able to answer questions that no individual 
lab could tackle alone. In a proof-of-concept 
study, the ManyBabies Consortium used word 
of mouth, social media and e-mail lists to amass 
a team of 69 labs to test whether infants across 
several world regions prefer ‘baby talk’: the 
high-pitched, sing-song speech that adults 
in many cultures use with babies. Data from 
2,329  infants in 16 countries provided a 
resounding yes, demonstrating that infants 
even prefer baby talk that is not in their native 
language3. This study, the largest of its kind, 
was cited more than 100 times within a year of 
its publication, according to Google Scholar. 
The ManyBabies Consortium is not a one-
off. It is part of a broader movement towards 
grass-roots big-team science: endeavours in 
which an unusually large number of research -
ers — often dispersed across institutions 
and world regions — self-organize to pool 
intellectual and material resources in pur -
suit of a common goal4. In addition to the 
ManyBabies Consortium, the authors have 
collectively been involved in creating the 
Psychological Science Accelerator (involv -
ing some 1,200 researchers)5, the Disturbance 
and Resources Across Global Grasslands net -
work (DRAGNet; around 100 researchers; 
https://dragnetglobal.weebly.com) and the 
ManyPrimates project (comprising about 
150 researchers6; see ‘Examples of big-team 
science’). These self-organized consortia 
pool resources to conduct massive studies in 
psychology, ecology and primatology, respec -
tively. They perform collaborative endeav -
ours similar to those of the Human Genome 
Project and groups within CERN, Europe’s 
particle-physics lab near Geneva, Switzer -
land, but have been founded without formal 
funding mechanisms or well-developed 
infrastructure.
We have found that grass-roots big-team 
science is capable of generating knowledge 
that is difficult to obtain — but faces several 
barriers to sustainability.
Barrier 1: rewarding team players
Michele Grigsby Coffey, a historian at the Uni -
versity of Memphis in Tennessee, has described 
academia as “a selfish sport” in which research -
ers “are rewarded for self-absorbed fixations” , 
and in which “prioritizing yourself at the 
expense of others is encouraged”7. Big-team 
science, however, is a team sport that often 
requires researchers to prioritize discovery 
over their own self-interests. For example, the 
first ManyPrimates study (of which D.A. is a 
co-author) examined the working-memory 
capacity of more than 40 species of primate 
by testing whether the animals could remem -
ber the locations of hidden food after short 
time delays8. D.A. estimates that he commit -
ted some 200 hours to the project. Yet on the 
resulting paper, the consortium is listed as the 
first author, the corresponding author e-mail is a shared mailbox and D.A. occupies one of 
79 slots in the alphabetically sorted author 
list. Such authorship arrangements highlight 
the accomplishments of the team over any 
individual.
Pursuing relatively selfless ideals of big-
team science can mean being penalized by the 
referees of the selfish sport of academia. For 
example, when one of us (N.A.C.) was nom-
inated to direct the Psychological Science 
Accelerator during a postdoctoral fellowship, 
a well-meaning adviser told him that it was an 
important role and that he was a great fit, but 
that pursuing it would “kill chances of get -
ting a tenure-track position” . The more senior 
co-authors of this manuscript ( J.K.H., L.L.S. 
and T.H.P.) have offered junior colleagues 
similar warnings. For instance, they have 
seen members of hiring committees baulk 
when a job candidate’s CV contains several 
papers in which their name is in the middle of 
a long list of authors. A selfish sport rewards 
stars — not those who have crucial supporting 
roles. Indeed, when one of us in a big-team 
effort expressed excitement about a recent 
milestone to a department head, the response 
was: “Great. Just make sure you have work 
coming out of your own research group. ”
Academia could change the game by 
rewarding researchers who make large con -
tributions to team efforts. Otherwise, teams 
will be forced to find other ways to increase 
benefits or decrease the costs of participa -
tion. For example, project leaders could fund 
collaborators, as is being done for a collabo -
ration designing tools to predict the replica-
bility of research findings in the social and 
behavioural sciences, supported by the US 
Defense Advanced Research Projects Agency 
(www.cos.io/score). Such solutions, however, 
prohibit researchers with fewer resources 
from leading big-team science efforts. As 
an alternative, some collaborations offer 
non-financial perks. For example, both 
DRAGNet and the Nutrient Network offer 
participating researchers exclusive access 
to the full project database. However, these 
policies conflict with goals to make science 
more open and inclusive. A reduction in 
costs could be accomplished by recruiting 
even more researchers to split the bill, but 
this makes coordination much more difficult.
Barrier 2: diversity 
One large potential benefit of this way of 
doing science is the opportunity to increase 
the diversity of participants, researchers and research questions. But we have noticed a 
worrisome trend: pre-existing inequality in 
science infrastructure seems to be perpetu -
ated in big-team science.
A 2021 analysis noted that researchers in 
previously colonized countries often lack the 
access to lab space and funding that are nec-
essary to participate in big-team science9. Not 
surprisingly, perhaps, these inequalities also 
seem to affect who leads these endeavours. 
Not a single behavioural-science big-team 
project included in this analysis was led by a 
researcher in a developing nation. Further -
more, the combined governing and steering 
boards of ManyBabies, the Psychological 
Science Accelerator, the Nutrient Network 
and DRAGNet include only 4 (of 32) mem -
bers from outside North America or Western 
Europe (17 are from the United States, 5 from 
Canada, 6 from Western Europe, 1 from Kenya, 
1 from Argentina, 1 from Australia and 1 from 
India). 
Big-team science should find ways to 
enable change. For example, the Psycholog -
ical Science Accelerator uses donations to 
award participation grants to researchers in 
under-represented regions. The ManyBabies 
Consortium launched an extension of its first 
study that provides funding, training and 
support for data collection in Africa — an 
operation that would have been impossible 
without support from the Jacobs Foundation 
in Zurich, Switzerland. DRAGNet minimizes 
costs at institutions that have few resources 
by getting them to ship seed samples for 
processing at better-resourced institutions. 
Many Primates fosters connections in the 
global south by participating in local meet -
ings and reaching the community through 
publications in languages such as Spanish 
and French. 
Researchers can also help to close the infra -
structure gap by training and supporting 
researchers in under-represented areas. For 
example, a big-team project testing how peo -
ple in various African regions evaluate moral 
transgressions is led by a PhD student from 
Nigeria, and is supported by several members 
of the Psychological Science Accelerator10.
Barrier 3: funding and sustainability
Despite well-recognized outputs, we all 
scramble constantly to keep our big-team 
initiatives going. These grass-roots projects 
can be established with little funding, but they 
are difficult to maintain without financial sup -
port. Big-team science needs funds to retain 
researchers who know how to coordinate the 
next wave of science, to support tools for man -
aging increasingly complex workflows, and to 
support participation from researchers who 
are not well resourced. 
For example, the first Psychological Science 
Accelerator study examined how people 
around the world judge others on the basis “Leading the big-team-
science movement can feel 
like climbing mountains 
without so much as a rope. ”
506  | Nature | Vol 601 | 27 January 2022
Comment
©
 2022
 Springer
 Nature
 Limited.
 All
 rights
 reserved. ©
 2022
 Springer
 Nature
 Limited.
 All
 rights
 reserved.",Coles et al. - 2022 - Build up big-team science.pdf
"of facial appearance11. The project involved 
241 collaborators and 11,570 participants 
spanning 41 countries. In principle, this study 
should have cost hundreds of thousands of 
dollars. If participants and research assistants 
were each paid just US$5 for every 30-minute 
data-collection session, the cost would be 
more than $115,000. The price tag gets much 
bigger when factoring in labour for project 
management, which included acquiring more 
than 150 ethics-approval documents, trans -
lating study materials into 23 languages and 
developing research tools to track progress 
and validate data from labs all over the world 
(see go.nature.com/3jcsutx). Yet the pro -
ject officially operated on less than $2,000; 
hundreds of collaborators donated their time 
and resources to make up the difference (see 
go.nature.com/3qstumf). 
Operations that run on shoestring dona -
tions are neither sustainable nor scalable. 
This hard truth became apparent at the begin -
ning of 2020, when the Psychological Science 
Accelerator received 66 urgent proposals for 
global research projects on the psychology 
of the COVID-19 pandemic. Financial consid -
erations meant that the network had to reject 
all but three. One rejected proposal aimed to 
test whether reminding people to consider 
accuracy before sharing news could help to 
curb COVID-19 misinformation in different 
world regions and demographics. Every time 
we see a post promoting false claims that 
the antiparasitic drug ivermectin prevents 
COVID-19, that pregnant women should not get 
vaccinated or that COVID-19 vaccines contain microchips, we are painfully reminded of the 
work we did not have the funds to support.
Why is it so hard to get funding for grass-
roots big-team science initiatives? Govern -
ment and philanthropic funders have provided 
various reasons. For instance, they worry that 
big-team science will ultimately prove to be 
unsustainable because of academia’s selfish 
rulebook. They say that big-team science is 
still not diverse enough in terms of researchers 
and research questions. They say that their 
systems are not set up to process proposals 
with hundreds of collaborators, or to handle 
funding requests that go out to dozens of 
research sites. Most frustratingly, they say that 
big-team science has managed so far without 
their support.
Leading the big-team-science movement 
can sometimes feel like climbing the world’s 
tallest mountains without so much as a rope. 
We have caught glimpses of the peaks and can 
imagine the views they might offer, but we lack 
the resources to climb higher. Every step for-
ward will become increasingly treacherous 
until academic institutions and funders 
provide long-overdue support. 
The authors
Nicholas A. Coles is a research scientist at 
Stanford University, California, USA, and 
the director of the Psychological Science 
Accelerator. J. Kiley Hamlin is a professor 
of psychology at the University of British 
Columbia, Vancouver, Canada, and a member of the ManyBabies Consortium Steering 
Committee. Lauren L. Sullivan is an assistant 
professor of biological sciences at the 
University of Missouri, Columbia, USA, and 
the principal investigator of the Disturbance 
and Resources Across Global Grasslands 
(DRAGNet) project. Timothy H. Parker is a 
professor at Whitman College, Walla Walla, 
Washington, USA, and a member of the 
Disturbance and Resources Across Global 
Grasslands (DRAGNet) project, among others. 
Drew Altschul is a postdoctoral fellow at the 
University of Edinburgh, UK, and a member of 
the ManyPrimates project. 
e-mail: ncoles@stanford.edu
1. Byers-Heinlein, K. et al. Can. Psychol. Can. 61, 349–363 
(2020).
2. Oakes, L. M. Infancy 22, 436–469 (2017).
3. ManyBabies Consortium. Adv. Methods Pract. Psychol. 
Sci. 3, 24–52 (2020).
4. Forscher, P. S. et al. Preprint at PsyArXiv https://doi.
org/10.31234/osf.io/2mdxh (2020).
5. Moshontz, H. et al. Adv. Methods Pract. Psychol. Sci. 1, 
501–515 (2018).
6. Many Primates et al. PLoS ONE 14, e0223675 (2019).
7. Grigsby Coffey, M. ‘The Unselfish Academic’ Auntie 
Bellum Magazine (3 December 2016).
8. Many Primates et al. Preprint at PsyArXiv https://doi.
org/10.31234/osf.io/5etnf (2021).
9. Silan, M. et al. Assoc. Psychol. Sci. Obs. 34 (6), 64–69 
(2021).
10. Adetula, A. et al. Preprint at AfricArXiv https://doi.
org/10.31730/osf.io/hxjbu (2021).
11. Jones, B. C. et al. Nature Hum. Behav. 5, 159–169 (2021). 
12. Wang, K. et al. Nature Hum. Behav. 5, 1089–1110 (2021). 
13. Borer, E. T. et al. Nature 508, 517–520 (2014). 
14. Coles, N. A. et al. Preprint at PsyArXiv https://doi.
org/10.31234/osf.io/cvpuw (2021).
15. Errington, T. M. et al. eLife 10, e71601 (2021).
The authors declare competing interests; see go.nature.
com/3tnjtrr for details.EXAMPLES OF BIG-TEAM SCIENCE
Large teams of researchers have come together in various ways to tackle difficult questions in science, from soil samples to cancer biology. 
Consortium or project name How and when organized Example of project finding or question Data collection
Psychological Science  
AcceleratorGrass-roots consortium launched by 
a 2017 blogpost. Now involves some 
1,200 researchers.‘Cognitive reappraisal’ improves 
emotional reactions to the COVID-19 
pandemic.Data from more than 20,000 people 
in 87 countries collected by more than 
450 researchers12. 
ManyBabies Consortium Grass-roots consortium launched by 
a 2015 blogpost. Now involves around 
450 researchers.Infants prefer ‘baby talk’ even when it’s 
not in their native language.Data from 2,329 infants collected by 
150 researchers in 16 countries across 
the world3.
ManyPrimates project Grass-roots consortium launched through 
2018 symposium, word of mouth, e-mail 
and social media; now involves about 
150 researchers.Among 41 closely related primate 
species, phylogeny matters more for 
short-term memory than do ecology or 
social factors. 81 researchers studied 421 primates8.
Nutrient Network (NutNet) Launched in 2006 through e-mail and 
Twitter requests to join the network. 
Data collection began in 2007.Does herbivory and light availability 
resolve the loss of plant species caused 
by nutrient addition? Using data from the broader Nutrient 
Network experiment (>130 collaborating 
sites), researchers documented effects 
of controlled combinations of nutrient 
addition and herbivore exclusion on plant 
diversity at 40 sites across the globe13. 
Many Smiles Collaboration Grass-roots effort launched in 2018. 
Collaborators formed adversarial teams 
recruited through social media and e-mail.Does changing facial expression affect 
emotions?Nearly 50 researchers collected data from 
3,878 participants across 19 countries14.
Reproducibility Project:  
Cancer BiologyLaunched in 2013 through funding provided 
by Arnold Ventures to the Center of Open 
Science and Science Exchange.Can the results of experiments from 
high-impact cancer-biology papers be 
reproduced? 200 collaborators attempted to 
replicate 158 effects from 50 preclinical 
experiments15.
Disturbance and Resources  
Across Global Grasslands  
network (DRAGNet)Grass-roots consortium conceived in 2018. 
Data collection began in 2019; network built 
through e-mail and Twitter.When grasslands are disturbed by tilling 
and nutrient additions, how do they 
respond?Some 90 researchers monitor 70 sites 
in 18 countries (https://dragnetglobal.
weebly.com). 
Nature | Vol 601 | 27 January 2022 | 507
©
 2022
 Springer
 Nature
 Limited.
 All
 rights
 reserved. ©
 2022
 Springer
 Nature
 Limited.
 All
 rights
 reserved.",Coles et al. - 2022 - Build up big-team science.pdf
"https://doi.org/10.1177/1745691619850561Perspectives on Psychological Science
2019, Vol. 14(5) 711 –733
© The Author(s) 2019
Article reuse guidelines: sagepub.com/journals-permissions
DOI: 10.1177/1745691619850561
www.psychologicalscience.org/PPSASSOCIATION FOR
PSYCHOLOGICAL SCIENCE
There is no perfect study. Scientists, in their effort to 
understand nature, are constrained by limited time, resources, and expertise. This constraint may produce a dilemma between choosing a lower quality, expedient approach or conducting a better powered, more inten-sive investigation allowing for stronger inferences. Ideals of the scientific process can be outweighed by the pragmatic reality of scientists’ available resources and pursuit of career advancement. Scientists are rewarded for being the originators of new ideas and evidence through the authorship of articles. These cultural incen-tives foster a focus on novelty and authorship that  
can come at the expense of rigor and foster question-able practices (Bakker, van Dijk, & Wicherts, 2012;  
Greenland & Fontanarosa, 2012; Nosek, Spies, & Motyl, 2012; Open Science Collaboration, 2015). One alterna-tive is for researchers to take more time for individual studies, expend more resources on each project, and publish fewer findings. Scientists could also work more collectively, combining resources across more contribu-tors. But such choices have implications for productiv-ity, individual credit, and career advancement.
Here we consider the standard model of scientific 
investigation and describe a complementary model—crowdsourcing science. Crowdsourced approaches seek to maximize the use of available resources, diversify 850561 PPSXXX10.1177/1745691619850561Uhlmann et al.Crowdsourcing Science
research-article 2019
Corresponding Authors:
Eric Luis Uhlmann, INSEAD, Organisational Behaviour Area, 1 Ayer Rajah Ave., 138676 Singapore E-mail: eric.luis.uhlmann@gmail.com
Brian A. Nosek, University of Virginia, Department of Psychology, Box 
400400, Charlottesville, VA 22904-4400 E-mail: nosek@virginia.eduScientific Utopia III:  
Crowdsourcing Science
Eric Luis Uhlmann1, Charles R. Ebersole2,  
Christopher R. Chartier3, Timothy M. Errington4,  
Mallory C. Kidwell5, Calvin K. Lai6, Randy J. McCarthy7,  
Amy Riegelman8, Raphael Silberzahn9, and  
Brian A. Nosek2,4
1Organizational Behaviour Area, INSEAD, Singapore; 2Department of Psychology, University of Virginia; 
3Department of Psychology, Ashland University; 4Center for Open Science, Charlottesville, Virginia; 5Department 
of Psychology, University of Utah; 6Department of Psychological and Brain Sciences, Washington University in 
St. Louis; 7Center for the Study of Family Violence and Sexual Assault, Northern Illinois University; 8University 
Libraries, University of Minnesota; and 9Department of Business and Management, University of Sussex
Abstract
Most scientific research is conducted by small teams of investigators who together formulate hypotheses, collect data, conduct analyses, and report novel findings. These teams operate independently as vertically integrated silos. Here we argue that scientific research that is horizontally distributed can provide substantial complementary value, aiming to maximize available resources, promote inclusiveness and transparency, and increase rigor and reliability. This alternative approach enables researchers to tackle ambitious projects that would not be possible under the standard model. Crowdsourced scientific initiatives vary in the degree of communication between project members from largely independent work curated by a coordination team to crowd collaboration on shared activities. The potential benefits and challenges of large-scale collaboration span the entire research process: ideation, study design, data collection, data analysis, reporting, and peer review. Complementing traditional small science with crowdsourced approaches can accelerate the progress of science and improve the quality of scientific research.
Keywords
crowdsourcing, collaboration, teams, methodology, metascience
",Uhlmann et al. - Scientific Utopia III Crowdsourcing Science.pdf
"712 Uhlmann et al.
contributions, enable big science, and increase trans-
parency and reliability. The adaptation of cultural norms and incentives to promote crowdsourcing as a comple-ment to the standard model promises to make science more rigorous and inclusive and accelerate discovery.
Two Models of Doing Science
Standard model: vertical integration
Some academic research resembles a vertically inte-
grated business. An individual or small research team conceives a research question, designs studies to inves-tigate the question, implements the studies, analyzes the data, and writes a report of what was found. The closed team conducts the entire process from conceiv-ing the idea to reporting the outcomes. The team  
members responsible for these steps are active collabo-rators and coauthors on a manuscript reporting the research. The sought-after reward is acceptance and publication in the most widely read and prominent journal possible.
This model has several notable characteristics. It is 
localized, with funding distributed to particular labs and institutions, and resource intensive, with the project work divided among a few individuals. Access to pro-ductive research pipelines is constrained, and experi-ence and status lead to opportunities to engage in research collaborations (Merton, 1968). It produces a large quantity of small science with teams of limited size conducting projects that are correspondingly lim-ited in scope—a small team can collect only so much data, carry out only so many analyses, and consider only so many alternatives to their methodology. Finally, contribution is recognized and rewarded through authorship on the final publication.
The standard model is akin to the philosopher model 
of scholarly contribution. An independent thinker con-ceives and generates a stand-alone piece of scholarship. After peer review by a small number of select col-leagues, that scholarship is entered into the market-place of ideas for others to examine, discuss, critique, and extend. Independence in developing and enacting the idea allows the scholar to dig deeply into a question or idea without interference, and credit allocation is straightforward. Scholars are evaluated on the basis of the reception of their work in the idea marketplace. Outstanding ideas and evidence may become perma-nently linked to the scholar’s identity, securing a lasting reputation and impact.
So what is wrong with the standard approach to sci-
ence? For many research questions and contributions, nothing. Independently generated contributions are an efficient means of getting initial evidence for many ideas into the marketplace. Indeed, the decentralized nature of science is presumed to feed the productive generation and culling of ideas by the independent actions of scholars with different priors, assumptions, expertise, and interests. Small teams often work together repeatedly and develop cospecializations that enable deep dives into a methodology or phenomenon. A com-munity of scientists then shares its work, exchanges feedback, and serially builds on each other’s findings.
At the same time, for some research questions and 
contributions, the standard model may limit progress. Individual researchers and small teams must consider certain trade-offs when directing their research efforts. They could vary design elements and stimuli instead of holding them constant, collect larger samples for fewer studies instead of smaller samples for more stud-ies, and they could replicate their findings across mul-tiple conditions or contexts rather than demonstrate a phenomenon and then move on. Researchers inevitably weigh these trade-offs against the potential rewards. And because the present culture prizes innovation and discovery (Bakker et al., 2012), some behaviors that would foster research credibility and cumulative prog-ress are performed ineffectively or infrequently. Under -
performed behaviors include collecting large, cross-cultural samples to evaluate generalizability and estimate effect sizes precisely (Henrich, Heine, & Norenzayan, 2010), replicating findings systematically in independent laboratories (Klein et al., 2014; Makel, Plucker, & Hegarty, 2012; Mueller-Langer, Fecher,  
Harhoff, & Wagner, 2019; Simons, 2014), obtaining sev-eral different perspectives on how to analyze the same data (Silberzahn et al., 2018), and using a wide variety of study designs and stimuli (Judd, Westfall, & Kenny, 2012; Wells & Windschitl, 1999).
Alternative model: horizontal distribution
The alternate model—crowdsourcing—eschews vertical integration and embraces the horizontal distribution of ownership, resources, and expertise (Howe, 2006). In a distributed collaboration, numerous researchers each carry out specific components of a larger project, usu-ally under the direction of a core coordination team (such that crowd projects are rarely perfectly horizon-tally distributed). Modern science is already stretching the standard model in more collaborative directions (see Supplement 1 in the Supplemental Material avail-able online). Solo authorship is now the exception in most fields. This is partly due to the diversification of expertise required to conduct research with modern tools (Börner et al., 2010). Across disciplines, team size almost doubled from 1.9 in the 1960s to 3.5 in 2005 ",Uhlmann et al. - Scientific Utopia III Crowdsourcing Science.pdf
"Crowdsourcing Science 713
(Valderas et al., 2007; Wuchty, Jones, & Uzzi, 2007), and 
working in teams is associated with greater individual career success (Kniffin & Hanks, 2018). Team-authored articles are more cited than solo-authored articles, and this gap in scholarly impact has increased over time (Valderas et al., 2007; Wuchty et al., 2007).
Rather than two qualitatively distinct categories of 
research, the vertically integrated and horizontally dis-tributed approaches are better conceived as a contin-uum, with variation in the depth of contribution by any given individual and the number of individuals contrib-uting to the project. New opportunities and challenges emerge when moving further across the continuum from singular, independent scholars to a distributed, interdependent community. Crowdsourcing carefully selected research questions, in parallel to the necessar -
ily far greater number of small team projects, holds several potential benefits for science, among which are enabling the conduct of large-scale research projects, democratizing who contributes to science, and assess-ing the robustness of findings.
Enabling big science. An inclusive, diversified contri-
bution model enables ambitious projects that would be 
unattainable by individuals or small teams working in isolation. Combining resources enables crowdsourced teams to enact research designs that vastly exceed what could be accomplished locally. Instead of holding sam-pling, stimulus, or procedural variables constant and hoping they do not matter, crowdsourced teams can allow them to vary and test whether they do. Instead of carrying out a low-powered, imprecise test, crowd-sourced teams can conduct high-powered, precise stud-ies and draw confident conclusions. Crowdsourcing complex activities seeks to mobilize the crowd’s compe-tencies, knowledge, and skills and may leverage under -
used resources such as a better way to analyze the data, access to hard-to-recruit populations, knowledge of unpublished research or articles published in other lan-guages, and translation of research materials into local languages and dialects. Crowdsourcing flips research planning from “what is the best we can do with the resources we have to investigate our question?” to “what is the best way to investigate our question, so that we can decide what resources to recruit?”
Democratizing science. Although personal factors 
(Clemente, 1973; Hirsch, 2007; Williamson & Cable, 2003) 
and merit play a role in success in science, scientific careers also exhibit a Matthew effect (Merton, 1968). Early advantages in doctoral institution rank, professional connections, and grant funding accumulate benefits over time (Bol, De Vaan, & van de Rijt, 2018; Clauset, Arbesman,  
& Larremore, 2015). Grant funding is overallocated to elite universities, and evidence suggests that returns on investment would be greater if the funds were distributed more evenly (Wahls, 2018). Early-career researchers from less well-known institutions, underrepresented demo-graphic groups, and countries that lack economic resources may never have a fair chance to compete (Petersen, Jung, Yang, & Stanley, 2011; Wahls, 2018). Academic fields are generally rich in talent, such that globally distributed proj -
ects can recruit individuals with advanced training and much to offer yet too few resources to enact the vertical model competitively on their own. Few people enjoy the resource benefits of research-intensive institutions, includ -
ing laboratory space, professional staff to support grant writing and management, graduate students, light teach-ing loads, and a community of colleagues for developing ideas and sharing infrastructure. Crowdsourcing aims to provide a new avenue through which those outside of major research institutions can contribute to high-profile projects, increasing inclusiveness, merit, and returns on investment (Chargaff, 1978; Feyerabend, 1982).
Assessing the robustness of findings. A crowdsour ced 
approach is uniquely advantaged in determining the reli-
ability and generalizability of findings. The ecosystem of standard science leads to the publication of massive num-bers of small-sample studies (Pan, Petersen, Pammolli, & Fortunato, 2016), each with observations typically drawn from a single population (e.g., undergraduates from the researchers’ home institution in the case of behavioral experiments; Sears, 1986). Combined with the filter of an academic review process that primarily permits statisti-cally significant results to appear in the published record (Fanelli, 2010), the end result is a research literature filled with inaccurately estimated effect sizes as a result of pub-lication bias (Ioannidis, 2005, 2008). The standard approach to science is also susceptible to issues such as study designs generated from a single theoretical perspective (Monin, Pizarro, & Beer, 2007), unconsidered cultural dif -
ferences (Henrich et al., 2010), and researcher degrees of freedom in data analysis (Gelman & Loken, 2014; Simmons, Nelson, & Simonsohn, 2011). Large-scale collaboration helped transform epi demiology into a more reliable field 
(Ioannidis, Tarone, & McLaughlin, 2011; Panagiotou, Willer, Hirschhorn, & Ioannidis, 2013), and this process is cur -
rently under way in psychology and other scientific disci-plines. Multilab collaborations facilitate directly replicating findings (same materials and methods, new observations; Ebersole et al., 2016; Klein et al., 2014) and conceptually replicating them (new approach to testing the same idea; Landy et al., 2018). Crowdsourcing research is a part of a changing landscape of science that seeks to improve research reliability and advance the credibility of academic research (LeBel, McCarthy, Earp, Elson, & Vanpaemel, 2018; Nosek et al., 2012).",Uhlmann et al. - Scientific Utopia III Crowdsourcing Science.pdf
"714 Uhlmann et al.
At the same time, there are opportunity costs and 
diminishing returns involved in organizing many labo-
ratories to carry out a single scientific investigation. Organizing a collective for a globally distributed project can create bureaucracy and transaction costs. For the same effort, a larger number of ideas with initial sup-porting evidence could have been introduced into the literature by smaller teams working separately. Crowd-sourcing allows for systematically examining cross-population variability, but it is important to begin by making sure the effect emerges reliably in at least one location. It will often be beneficial to rely on research from small teams for these reasons, especially when it comes to new areas of inquiry. Crowd projects with dozens or even hundreds of authors also create credit ambiguity and lack extrinsic incentives for participation, topics we address in depth later when we discuss struc-tural reforms to encourage greater crowdsourcing. We believe the two models should coexist, with individual investigators and small teams generating initial evi-dence for new ideas and crowdsourced initiatives implemented to select particularly critical questions for intense examination. A diverse array of scientific proj-ects, everywhere along the continuum from lone researchers to huge collectives, may produce the great-est return of useful knowledge from the resources invested. The remainder of this article discusses circum-stances in which crowdsourcing offers particular oppor -
tunities and challenges as a complement to the standard model.
Forms of Scientific Crowdsourcing
Rather than supplanting the standard approach, orga-nizing many individuals and laboratories into shared projects seeks to offset some of the weaknesses of vertically integrated science. Crowd initiatives vary on multiple dimensions that can create advantages and disadvantages depending on the research application (Lakhani, Jeppesen, Lohse, & Panetta, 2007; Muffatto, 2006; Salganik, 2017; Srinarayan, Sugumaran, &  
Rajagopalan, 2002; Surowiecki, 2005). For example, crowdsourced projects vary in terms of the degree of communication between project members, from largely independent work curated by a coordination team to crowd collaboration on shared activities. Crowd-science initiatives also vary in their inclusivity, from open calls for collaborators to carefully chosen groups of topic experts.
Figure 1 crosses the horizontal dimension of com-
munication (anchored at the left end by curated con-tributions and at the right by crowd collaboration) with 
the vertical dimension of selectivity to create a 2 ×  2 
matrix. Examples of relevant crowdsourced projects are placed in this matrix as illustrations. These projects are described in greater detail in the next section and in Tables 1 and 2 (see also Supplements 1 and 2 in the Supplemental Material). Citizen-science initiatives that include anyone willing to collect data involve a high degree of independence between actors and thus fall into the bottom-left quadrant (Gura, 2013). Posing a research question to specialists (e.g., moral-judgment researchers) and asking them to independently design studies to test the same idea falls into the top-left quad-rant (Landy et al., 2018). Iterative contests in which topic experts work together to improve experimental interventions (Lai et al., 2014) and the collective devel-opment of open-source software (Muffatto, 2006) are in the top-right quadrant, and more inclusive forms of crowd writing (Christensen & van Bever, 2014) are in the bottom-right quadrant. Open peer review, in which anyone can publicly comment on a scientific manu-script or article, falls into the bottom-right quadrant, and crowd review by experts carefully chosen by a journal editor falls into the top-right quadrant. Tradi-tional small-team research, with unrestricted commu-nication and select membership, falls outside the extreme top-right corner of the matrix at the far end of both axes.
Multistage projects may operate in different locations 
in this space during the research life cycle. For example, to explore consensus building about disparate findings from the same data set, Silberzahn et al. (2018) segued from isolated individual work to round-robin feedback and then open-group debate. Indeed, much crowd-sourced science moves gradually from left to right on the communication dimension over the life course of the project, culminating in collective e-mail exchanges and editing of the manuscript draft. Likewise, crowd projects tend to rely more on selective expertise over time (i.e., move up the vertical axis), as project coor -
dinators and specialized subteams of statistical experts check the collective work for errors and play leading roles in producing the final report.
On the vertical dimension, greater inclusivity facili-
tates scaling up for massive initiatives. In contrast, selectivity in project membership prioritizes specific areas of expertise for contribution. It is not yet clear under what conditions involving large crowds of con-tributors (i.e., moving downward on the vertical axis) compromises overall project quality relative to applying mild or strong selectivity standards for contribution (Budescu & Chen, 2015; Mannes, Soll, & Larrick, 2014). Research done by lone scientists and small teams is already known to be prone to error (Bakker & Wicherts, 2011; Berle & Starcevic, 2007; Garcia-Berthou & Alcaraz, 2004; Salter et al., 2014; Westra et al., 2011), and the quality-quantity trade-off that can accompany scaling ",Uhlmann et al. - Scientific Utopia III Crowdsourcing Science.pdf
"Crowdsourcing Science 715
up is potentially offset by the numerous eyes available 
to catch mistakes (e.g., Silberzahn et al., 2018). The available evidence suggests that data collected by citi-zen scientists are comparable in error rates and general quality to those assembled by professionals (Kosmala, Wiggins, Swanson, & Simmons, 2016; Thelen & Thiet, 2008). Online coders and political scientists reach near-perfect agreement on policy positions in political mani-festos (Benoit, Conway, Lauderdale, Laver, & Mikhaylov, 2016), Wikipedia entries are as accurate as the Ency-clopedia Britannica (Giles, 2005), highly published and less prolific researchers are similarly likely to Selective Projects, Low Communication
•  Crowdsourcing Designs of Experiments
•  Prepublication Independent Replication•  Crowd Replication Initiatives (e.g., RP:P) 
•  Crowdsourcing Data Analysis
•  Solution ContestsSelective Projects, High Communication
•  Coordinated Analyses•  Peer Review by Select Crowd of Experts•  Intervention Contests
•  Assembling Resources Using Online Platforms 
    (e.g., StudySwap)
•  Polymath Projects
•  Open-Source Software Development
Inclusive Projects, Low Communication
•  Prediction and Decision Markets for Scientiﬁc 
    Results•  Leveraging Class Projects to Conduct 
    Replications (e.g., CREP)•  Citizen ScienceInclusive Projects, High Communication
•  Crowdsourced Generation and Selection of
    Ideas•  Crowd Writing
•  Open Peer Review
Degree of Communication Between Project MembersCurated 
ContributionsCrowd 
CollaborationsInclusiveness vs. Selectivity
Open to AnyoneSelect Experts Only
Fig. 1. Forms and examples of crowdsourcing. Curated contributions refers to projects in which project coordinators collect the indi-
vidual work of a crowd of contributors whose communication with one another is limited to nonexistent. Crowd collaborations refers 
to projects in which a large group of contributors engage in regular communication regarding their shared work. CREP =  Collaborative 
Replication and Education Project; RP:P =  Reproducibility Project: Psychology.
Table 1.  Crowdsourcing Different Stages of the Research Process
Stage of research How crowds are leveraged
Ideation Crowds are used to generate novel research ideas and solutions to problems
Assembling resources Online exchanges are used to match investigators with needs with partner laboratories who 
have that resource
Study design The same research hypothesis is given to different scientists, who independently design 
studies to test it
Data collection Numerous collaborators aid in obtaining research participants, observations, or samples
Data analysis A network of researchers carries out statistical analyses to address the same research question
Replicating findings before 
publicationThe same methodology is repeated in independent laboratories to confirm the finding before 
its publication
Writing research reports A large group of contributors collectively writes a research article
Peer review A large group of commentators writes public feedback on a scientific article
Replicating published findings The same methods and materials from published articles are repeated in independent 
laboratories to assess the robustness of the findings
Deciding future directions Crowd predictions about future research outcomes are factored into decisions about how to 
allocate research resources for maximum impact",Uhlmann et al. - Scientific Utopia III Crowdsourcing Science.pdf
"716 Uhlmann et al.
Table 2.  Examples of Crowdsourced Scientific Initiatives
Source Method Key result(s)
Ideation
Sobel (2007) Starting in 1714, the British Parliament 
launched an open competition to solve how 
to calculate the longitude of a ship at seaDevelopment of the marine chronometer
Polymath (2012, 2014) Mathematical challenges are posted online for 
open crowd collaborationA new combinatorial proof to the density 
version of the Hales-Jewett theorem, among other solved mathematical problems
Schweinsberg, Feldman, 
et al. (2018)Crowd of researchers asked to nominate 
hypotheses for testing with a complex data setThe crowd was able to generate interesting 
hypotheses for later testing
InnoCentive.com Scientific problems are posted online, and 
prizes are offered for the best solution30% of 166 scientific problems solved via crowd 
competitions for prizes
Assembling resources
Science Exchange Online marketplace that enables scientists to 
identify and outsource specific research 
needsProgram to independently validate antibodies; 
partnership with the Center for Open Science to conduct the Reproducibility Project: 
Cancer Biology
StudySwap Platform for posting brief descriptions of 
resources available for use by others or 
needed resources another researcher may 
haveUsed to gather resources for both crowdsourced 
and small team projects
Study design
Landy et al. (2018) Independent research teams separately design 
experiments to test the same hypothesis; research participants are then randomly assigned to different study versionsDifferent study designs associated with widely 
dispersed effect-size estimates for the same research question; for four out of five hypotheses examined, the materials from 
different teams returned significant effects in 
opposite directions
Data collection
Olmstead (1834) In 1833, Denison Olmsted used letter 
correspondence to recruit citizen scientists 
to help document a meteor showerDetailed documentation of the great meteor 
storm of 1833; birth of citizen-science movement
Kanefsky, Barlow, and 
Gulick (2001)Clickworkers website from the National 
Aeronautics and Space Administration asks volunteers to help classify imagesMapping of craters on Mars based on images 
from the Viking Orbiter
Church (2005) The Personal Genome Project recruits 
everyday people willing to publicly share their personal genome, health, and trait data 
as a public-research resourceCollection of data from 10,000 volunteers; full 
analyses of the genomes of 56 participants 
with identification of potential health impacts 
in 25% of cases; ongoing project to link genetics, memory, and attention
Cooper et al. (2010) Online game FoldIt in which more than 50,000 
players compete to fold proteinsThe best human players outperform a computer 
in terms of determining protein structures
Price, Turner, Stencel, 
Kloppenborg, and 
Henden (2012)Citizen sky project recruits amateur 
astronomers to help professionals gather observations of the planets, moons, 
meteors, comets, stars, and galaxiesGathering observations of Epsilon Aurigae, an 
unusual multiple star system, among other 
targets
Kim et al. (2014) Video game EyeWire in which players 
reconstruct part of an eye cell using three-
dimensional images of microscopic bits of retinal tissueData from more than 2,000 elite gamers used 
to collectively map neural connections in the retina, contributing to a better understanding of how the eye detects motion
MetaSUB International 
Consortium (2016)Commuters are enlisted to obtain samples 
from surfaces in subways and other public areasIdentification of new species and novel 
biosynthetic gene clusters; global maps of antimicrobial resistance markers
(continued)",Uhlmann et al. - Scientific Utopia III Crowdsourcing Science.pdf
"Crowdsourcing Science 717
Source Method Key result(s)
Sørensen et al. (2016) Video game Quantum Moves in which the 
player moves digital renditions of quantum 
atomsThe data produced by the more than 200,000 
users has been leveraged to develop better quantum algorithms
Moshontz et al. (2018) Psychological Science Accelerator (PSA), a 
network of more than 300 laboratories to conduct replications and collect other data 
for crowdsourced projectsThe first large-scale PSA project will seek to 
replicate earlier findings that people rate 
faces on the basis of valence and dominance
Zooniverse Online platform where citizen volunteers assist 
professional researchers with projectsEnables citizen-science initiatives such as 
“Mapping Prejudice,” in which project 
volunteers identify racially restrictive property 
deeds
Galaxy Zoo Asks volunteers to help classify galaxies on 
the basis of imagesCollection of more than 100 million 
classifications of galaxies based on 
shape, structure, and intensity; identifying 
supernovas and potential interactions between galaxies
Audubon Christmas Bird 
CountBeginning with the Audubon Christmas Bird 
Count of 1900, amateur birdwatchers have been used to collect data on bird migrationsLarge data set on bird migrations leveraged for 
scientific publications
Data analysis
Stolovitzky, Monroe, and 
Califano (2007)In the Dialogue for Reverse Engineering 
Assessments and Methods Challenges, 
organizers provide a test data set and a 
particular question to be addressed to many independent analysts and then apply the 
analytic strategies to a hold-out data set to 
evaluate their robustnessImproved prediction of survival of breast-cancer 
patients, drug sensitivity in breast-cancer cell 
lines, and biomarkers for early-Alzheimer’s 
disease cognitive decay
Hofer and Piccinin (2009) Coordinated analysis: network of researchers 
use the same target constructs, model, and covariates on different longitudinal data sets to address the same research questionChanges in physical activity over time affect 
cognitive function; education may not be a protective factor against cognitive decline
Schweinsberg, Feldman, 
et al. (2018)42 analysts were asked to test hypotheses 
related to gender, status, and science using a complex data set on academic debatesRadical effect-size dispersion, with analysts in 
some cases reporting significant effects in opposite directions for the same hypothesis 
tested with the same data
Silberzahn et al. (2018) The same data set was distributed to 29 analysis 
teams, who separately analyzed it to address 
the same research question (“Do soccer 
referees give more red cards to dark skin 
toned players than light skin toned players?”)Effect-size estimates ranging from slightly 
negative to large positive effects; 69% of analysts reported statistically significant 
support for the hypothesis, and 31% reported 
nonsignificant results
Replicating findings before publication
Schweinsberg et al. (2016) 25 independent laboratories attempted to 
replicate 10 unpublished findings from one 
research group6 of 10 findings were robust and generalizable 
across cultures according to the preregistered replication criteria
Writing research reports
Christensen and van Bever 
(2014)Online collaboration platform used to 
collect ideas and comments regarding 
why companies often do not invest in innovations that create new marketsThe article “The Capitalist’s Dilemma,” which 
argues this occurs because companies incentivize their managers to find efficiency innovations that eliminate jobs and pay off 
fast, rather than market innovations that pay 
off years later
(continued)Table 2.  (Continued)",Uhlmann et al. - Scientific Utopia III Crowdsourcing Science.pdf
"718 Uhlmann et al.
Source Method Key result(s)
Peer review
List (2017) Synlett implemented a crowdsourced 
reviewing process to allow more than 
100 referees to respond to articles after 
they were posted to an online forum for reviewersThe crowd review was faster and provided 
more comprehensive feedback than the 
traditional peer-review process
Replicating published findings
Steward, Popovich, Dietrich, 
and Kleitman (2012)Initiative to replicate spinal-cord-injury 
research in independent laboratories2 successful replications out of 12 targeted 
studies
Alogna et al. (2014) Registered Replication Report: attempt by 
many laboratories to replicate the verbal 
overshadowing effectVerbal overshadowing successfully replicated, 
but with a smaller effect size than in the original article
Klein et al. (2014) Many Labs 1: 36 laboratories attempted to 
replicate 13 psychology findings10 of 13 findings replicated
Open Science Collaboration 
(2015)Reproducibility project that attempted to 
replicate 97 original effects from top 
psychology journals in independent 
laboratories36% of findings successfully replicated
Camerer et al. (2016) Experimental Economics Replication Project: 
initiative to replicate prominent findings in experimental economics in independent laboratories61% of findings successfully replicated
Ebersole et al. (2016) Many Labs 3: 20 laboratories attempted to 
replicate 10 psychology findings at different times of the semester3 of 10 findings replicated; most unaffected by 
time of semester
McCarthy et al. (2018) Registered Replication Report: attempt by 
many laboratories to replicate the effects of priming hostility on impression formationFailure to replicate the hostility priming effect, 
with low heterogeneity in effect sizes across laboratories
Nosek and Errington (2017) Reproducibility Project: Cancer Biology: an 
initiative to replicate prominent findings in cancer biologyOf 12 replications thus far, 4 reproduced 
important parts of the original article, 4 replicated some parts of the original article but not others, 2 were not interpretable, and 
2 did not replicate the original findings
Camerer et al. (2018) Social Sciences Replication Project: an initiative 
to replicate 21 social-science findings in Science and Nature13 (62%) of findings successfully replicated
Klein et al. (2018) Many Labs 2: 28 psychology findings 
replicated across 125 sites14 of 28 findings replicated; heterogeneity in 
effect-size estimates was highest for large 
effect sizes and low for nonreplicable effects
Cova et al. (2018) Initiative to replicate prominent findings in 
experimental philosophy in independent laboratories78% of findings successfully replicated
O’Donnell et al. (2018) Registered Replication Report: attempt by 
many laboratories to replicate the effect of priming professors on intellectual 
performanceFailure to replicate the professor priming effect, 
with low heterogeneity in effect sizes across 
laboratories
Wagge et al. (2019) Collaborative Replications and Education 
Project initiative to replicate social-psychology findings in student methods 
classesFailure to replicate earlier findings that women 
are more attracted to men in photographs 
with red borders
Deciding future directions
Dreber et al. (2015) Prediction market to see whether independent 
scientists could forecast the results of the Reproducibility Project: PsychologyAggregated predictions accurately anticipated 
replication results
(continued)Table 2.  (Continued)",Uhlmann et al. - Scientific Utopia III Crowdsourcing Science.pdf
"Crowdsourcing Science 719
successfully replicate a given behavioral effect (Bench, 
Rivera, Schlegel, Hicks, & Lench, 2017; see also Klein, Vianello, Hasselman, & Nosek, 2018), and crowds of investigators do not exhibit measurably different “flair” at designing studies that obtain significant findings (Landy et al., 2018).
These null findings are surprising—there must be 
some point at which a crowd project becomes overly inclusive and insufficiently expert members compro-mise overall quality. One possibility is that coordinators of the crowd projects thus far have chosen the degree of inclusiveness and communication best suited to their research question (i.e., the correct location in Fig. 1), leading to judicious scaling without losses in quality. Logically, only individuals with specialized training (e.g., with physiological equipment) would be recruited to collect data for certain projects (e.g., pooling data from fMRI across laboratories; top-left quadrant of Fig. 1). Even with an open call, potential contributors may volunteer for projects in which they feel they can add value (e.g., an avid bird watcher volunteers to help track migrations), leading to self-screening based on relevant skill sets. Testing the conditions under which crowdsourcing increases and decreases project quality will inform future investments in crowdsourced research.
In contrast, there is little direct evidence regarding 
the consequences of information exchange between project members in crowdsourced scientific initiatives. Nevertheless, potential costs and benefits of crowd communication are suggested by the literature on group influence and decision making. One of the virtues of crowds of independent agents, especially demographi-cally and intellectually diverse ones, is their tendency Source Method Key result(s)
Camerer et al. (2016) Prediction market to see whether independent 
scientists could forecast replication results in 
experimental economicsAggregated predictions accurately anticipated 
replication results
DellaVigna and Pope 
(2018a)Prediction survey to see whether forecasters 
could anticipate the effects of treatment conditions on worker productivityAggregated predictions anticipated research 
outcomes; expert behavioral scientists, doctoral students, and Mechanical Turk 
workers similarly accurate
Eitan et al. (2018) Prediction survey to see whether scientists 
could forecast the size of political biases in scientific abstracts and to gauge their 
reactions to the research resultsForecasters accurately predicted that 
conservatives would be explained more, 
and explained in more negative terms, in 
scientific abstracts in social psychology; they also significantly overestimated the size of 
both effects but updated their beliefs in light 
of the new evidence
Landy et al. (2018) Prediction survey to see whether independent 
scientists could predict the results of 
conceptual replicationsAggregated predictions accurately anticipated 
overall outcomes, including variability in results across different study designs testing 
the same hypothesis
Camerer et al. (2018) Prediction market to see whether independent 
scientists could forecast results replications of social-science articles in Science and NatureAggregated predictions accurately anticipated 
replication results
DellaVigna and Pope 
(2018b)Prediction survey to see whether forecasters 
could anticipate the effects of treatment conditions on worker productivity as 
well as moderation by their demographic 
characteristicsAggregated predictions anticipated treatment 
effects but overestimated the importance of demographic moderators; academic seniority 
did not moderate forecasting accuracy
Forsell et al. (2018) Prediction market to see whether independent 
scientists could predict the results of the Many Labs 2 replication initiativeAggregated predictions accurately anticipated 
replication results
Lai et al. (2014) Contest to identify the most effective 
intervention to reduce implicit preferences for Whites over Blacks8 of 17 interventions effective in the short term 
but none effective a day or more after the intervention; teams were able to iteratively 
improve their interventions between rounds.Table 2.  (Continued)",Uhlmann et al. - Scientific Utopia III Crowdsourcing Science.pdf
"720 Uhlmann et al.
to balance out individual biases and errors in the  
aggregate (Galton, 1907; Larrick, Mannes, & Soll, 2012;  
Surowiecki, 2005). Crowdsourcing scientific investiga-
tions with little to no communication between project members (i.e., the far-left regions of Fig. 1) may help to avoid the potentially biasing effect of individuals’ overcommitment to intellectual claims (Berman & Reich, 2010; Luborsky et al., 1999; Manzoli et al., 2014; Mynatta, Dohertya, & Tweneya, 1977) and path depen-dencies in which knowledge of others’ approaches has an inordinate influence (Derex & Boyd, 2016). The effectiveness of crowds is more difficult to evaluate in situations that lack normatively correct answers or objective measures of accuracy. Yet even then, the diversity in approaches and results on the part of inde-pendent scientists, for example in analytic choices and study designs, is at least made transparent to the reader (Landy et al., 2018; Silberzahn et al., 2018).
That the “wisdom of the crowd” effect is spoiled 
when peer influence between members of the crowd is possible (Lorenz, Rauhut, Schweitzer, & Helbing, 2011) suggests that the more one moves toward crowd collaborations (i.e., right on the horizontal axis), the more conformity and deference to authority become risks. The one crowdsourced project that has tracked individual beliefs under conditions of gradually increas-ing communication found little evidence of conver -
gence over time, beyond what would be expected from sensitivity to new evidence (see Fig. 4 in Silberzahn et al., 2018). The circumstances under which conformity effects occur in crowd science remains an open empiri-cal question, and future projects should consider manipulating factors such as task interdependence and anonymity of communications.
Allowing information exchange and creating inter -
dependencies between project members also comes with potential important benefits. One of the hypoth-esized benefits of crowd collaboration is the ability of members of the community to learn from each other (Wenger, 1998). For example, teams in the Lai et al. (2014) intervention contest observed the effectiveness of others’ interventions between rounds and used those insights to improve their own interventions. Likewise, the round-robin feedback between different analytic teams in the crowdsourcing data-analysis initiative  
(Silberzahn et al., 2018) helped several analysts to iden-tify clear errors and adopt improved specifications. These are only anecdotal examples, and further research is needed to examine when peer learning occurs sys-tematically in iterative, multistage crowd collaborations and how it might best be facilitated. As reviewed next, evidence of the viability of crowdsourcing across all stages of the research process has accumulated rapidly in recent years.Crowdsourcing Science in Action
Science can benefit from crowdsourcing activities that span the entire research process (see Table 1). These include coming up with research ideas, assembling the research team, designing the study, collecting and ana-lyzing the data, replicating the results, writing the arti-cle, obtaining reviewer feedback, and deciding next steps for the program of research. Table 2 and Supple-ment 2 in the Supplemental Material summarize some recent crowdsourced scientific initiatives, organized by the respective stages on which they focused their crowd efforts.
Ideation
Crowds of scientists can be organized to collaborate virtually on complex problem-solving challenges, each proposing ideas for solving components of the problem and commenting on each other’s suggestions (open communication; the far-right regions of Fig. 1). This approach has been used to great effect in the Polymath projects, resulting in several important mathematical proofs (Ball, 2014; Polymath, 2012, 2014; Tao, Croot, & Helfgott, 2012). Like how they are used in product-design contests (Poetz & Schreier, 2012), crowds of researchers can also be used to generate original research hypotheses and select which ideas are most likely to be of broad interest and impact (Jia et al., 2018; Schweinsberg, Feldman, et  al., 2018). This approach may be particularly useful when it comes to data sets that for legal or ethical reasons cannot be publicly posted or further distributed—for instance, the personnel records of a private firm, who might agree to share them with one research team or institution but not for general distribution. Even in such cases, the core coordination team who serves as custodians of the data can post an overview of the variables and sample online and publicly solicit ideas for testing (Jia et al., 2018). The crowdsourced generation and selection of research ideas is one way to open up data sets and collaboration opportunities that would otherwise remain closed to most scientists.
Assembling resources
Genome-wide association studies distribute the task of investigating the entire genome across many collabora-tors and institutions with specialized roles, leading to important discoveries related to genes and pathways of common diseases (Visscher, Brown, McCarthy, & Yang, 2012). Consider the innumerable lost opportunities for similarly combining resources across laboratories in other scientific fields. For instance, a researcher at one ",Uhlmann et al. - Scientific Utopia III Crowdsourcing Science.pdf
"Crowdsourcing Science 721
institution may have a great idea but lacks access to the 
right equipment or sample of subjects to test it. Else-where, another team finds they have an excess of research resources (e.g., they compensate participants for a 30-min session for completing a 15-min study). Some researchers have resources that could produc-tively be used by other researchers who need those resources to meet their research goals. One way to attempt to minimize the collective waste and maximize researchers’ collective ability to meet their research goals is to match “haves” with “needs” using online platforms such as Science Exchange (https://www.science  
exchange.com) and StudySwap (http://osf.io/view/Study  
Swap). Such exchanges, which could be expanded into full-scale online academic labor markets similar to oDesk or Elance (Horton, 2010), seek to push academic com-munities into the top-right quadrant of Figure 1 by open-ing novel lines of communication and creating opportunities to connect resources and expertise.
Study design
Another limitation to standard science is narrow sam-pling of the constructs of interest (Baribault et al., 2018; Judd et al., 2012; Monin & Oppenheimer, 2014; Wells & Windschitl, 1999). A small team is at risk of generat-ing a limited set of stimuli, operationalizations of vari-ables, and study designs. Another team might have carried out a very different test of the same idea because of different prior training and theoretical assumptions. Even seemingly small differences in methods might produce substantial differences in research results. An alternative crowd approach is to assign the same research question to different experts, who then inde-pendently design studies aimed at answering it (low communication combined with high expertise; top-left corner of Fig. 1). Landy et al. (2018) did precisely this, finding that variability in effect sizes due to researcher design choices was consistently high. Indeed, study designs from different researchers produced significant effects in opposite directions for four of five research questions related to negotiation, moral judgment, and implicit cognition. Crowdsourcing conceptual replica-tions more effectively reveals the true consistency in support for a scientific claim.
Data collection
Online platforms for crowdsourced labor such as Ama-zon’s Mechanical Turk have become widely used as a source of inexpensive research participants and coders (Stewart, Chandler, & Paolacci, 2017; see Supplement 3 in the Supplemental Material). Rather than merely serving as research subjects, members of the general public can also be recruited to collect data and obser -
vations. This strategy moves the project into the bot-tommost left corner of Figure 1 of inclusive projects with low communication, with anyone willing to help being included as a project member. The tradition of citizen science dates back to Denison Olmsted’s use of observations from a crowd of both amateur and profes-sional astronomers to track the great meteor storm of 1833 (Littmann & Suomela, 2014; Olmsted, 1934). Citi-zen science today is a movement to democratize sci-ence (Chargaff, 1978; Feyerabend, 1982), engage the public, create learning opportunities, and gather data and solve problems at minimal cost with the aid of a host of volunteers (Cavalier & Kennedy, 2016; Gura, 2013). Amateur scientists participate actively in scien-tific investigations in biology, astronomy, ecology, con-servation, and other fields, working under the direction of professionals at research institutions. A related approach is to gamify scientific problems and recruit citizen scientists to aid in cracking them, as in the video game Quantum Moves, in which players move digital 
renditions of atoms (Sørensen et al., 2016), the online game EyeWire, in which players help reconstruct eye 
cells (Kim et al., 2014), and the protein-folding game FoldIt (Cooper et al., 2010). Note that for some types of citizen-science projects, contributors may have sub-stantial skills and knowledge—or even formal training, such as an advanced degree—and in such cases are far from novices. One of the strengths of crowdsourcing is the ability to tap into the expertise of individuals outside of mainstream academia who are able and will-ing to contribute to science.
Data analysis
Researchers working with a complex data set are con-fronted with a multitude of choices regarding potential statistical approaches, covariates, operationalizations of conceptual variables, and the like. In a quantitative review, Carp (2012a, 2012b) found that 241 published articles on fMRI used 223 distinct analytic strategies. Researchers may consciously or unconsciously choose statistical specifications that yield desired results, in particular statistically significant results, in support of a favored theory (Bakker et al., 2012; Ioannidis, 2005; Ioannidis & Trikalinos, 2007; Simmons et al., 2011; Simonsohn, Nelson, & Simmons, 2014). One way to maximize transparency is to turn the analysis of data over to a crowd of experts. The same data set is dis-tributed to numerous scientists who are asked to test the same theoretical hypothesis, at first without know-ing the specifications used by their colleagues (high expertise combined with low communication; top-left quadrant of Fig. 1). This offers an opportunity to assess ",Uhlmann et al. - Scientific Utopia III Crowdsourcing Science.pdf
"722 Uhlmann et al.
how even seemingly minor differences in choices may 
affect research outcomes and reduces the pressure to observe any particular outcome—at least for the pur -
poses of publishability. Silberzahn et al. (2018) found that 29 different teams of analysts used 29 distinct specifications and returned effect-size estimates for the same research question (“Do dark skin toned soccer players receive more red cards?”) that ranged from slightly negative to large positive effects. Crowdsourc-ing the analysis of the data reveals the extent to which research conclusions are contingent on the defensible yet subjective decisions made by different analysts.
The growth of large-scale data has created opportu-
nities to leverage this diversity to identify the most robust means of analyzing such complex and massive data sets. Crowdsourced challenges have been used by researchers for benchmarking new computational meth-ods, as with, for instance, the Dialogue for Reverse Engineering Assessments and Methods (DREAM) Chal-lenge focused on predicting the survival of breast-  
cancer patients (Saez-Rodriguez et al., 2016; Stolovitzky, Monroe, & Califano, 2007). Organizers provide a test data set and a particular question to be addressed to many independent analysts (a top-left-quadrant approach) and then apply the analytic strategies to a hold-out data set to evaluate their robustness.
Another innovative method is to hold constructs, 
models, and covariates constant and leverage a network of researchers to carry out this same analysis on differ -
ent existing data sets (a coordinated analysis; Hofer & Piccinin, 2009). This approach was pioneered by the Integrative Analysis of Longitudinal Studies on Aging network (Lindwall et al., 2012). Testing a research ques-tion of common interest (e.g., “Does education protect against cognitive decline?”; Piccinin et al., 2013) on existing data sets that include the same constructs (e.g., measures of cognitive function such as memory, reason-ing, and fluency) and yet measure them in disparate ways in different populations (e.g., Sweden, Austria, the Netherlands, and the United Kingdom) far more systematically assesses the generalizability of the results than relying on a single data source. Because members of this network of experts communicate extensively to agree on their shared analytic approach and measures to use from each longitudinal data set, a coordinated analysis falls into the top-right quadrant of Figure 1.
Note that all of these approaches are qualitatively 
different from fields in which many researchers inde-pendently leverage a central data source (e.g., the Gen-eral Social Survey, or GSS). In fields such as political science, resources such as the GSS are used to investi-gate separate research questions, such that aggregation and metascientific comparisons are less informative. Crowdsourcing is especially useful, we suggest, for fields that rely on local resources that can remain siloed. That said, the data corpus generated by crowdsourced projects often serves as a public resource after the publication of the article (e.g., Open Science Collabora -
tion, 2015; Tierney et al., 2016).
Replicating findings before publication
Individual laboratories are typically constrained in the amount and type of data they can collect. Repli-cating unpublished findings in independent labora-tories before they are submitted for publication (Schooler, 2014; Tierney, Schweinsberg, & Uhlmann, 2018) addresses power and generalizability directly. Authors can specify a priori in which replication sam-ples and laboratories they expect their findings to emerge; for example, they might select only topic experts as their replicators and thus moving up the vertical axis of Figure 1. This approach, which thus far returns a modest reproducibility rate even under the seemingly best of conditions (Schweinsberg et al., 2016), has recently been integrated into graduate and undergraduate methods classes (Schweinsberg, Vignanola, et al., 2018), thus traveling downward along the vertical axis toward greater inclusiveness. Such crowdsourced pedagogical initiatives are one means of turning replication into a commonplace aspect of how science is conducted and students are educated  
(Everett & Earp, 2015; Frank & Saxe, 2012; Grahe et al., 2012).
Writing research reports
The conceptualization, drafting, and revision of research articles represents another opportunity to leverage dis-tributed knowledge. The article “The Capitalist’s Dilemma,” conceptualized and written by two profes-sors and 150 of their MBA students, is one example (Christensen & van Bever, 2014). As with other forms of collaborative writing online, such as Wikipedia, channeling the contributions of many collaborators into a quality finished article requires a few group leaders who complete a disproportionate amount of the work and organize and edit the written material of others (Kittur & Kraut, 2008; Kittur, Lee, & Kraut, 2009). Our personal experience with articles with many authors is that a large number of contributors commenting pub-licly on the draft greatly facilitates working out a solid framework and set of arguments, identifying relevant articles and literatures to cite (especially unpublished work), ferreting out quantitative and grammatical errors, and tempering claims appropriately. More radically, efforts such as CrowdForge suggests that nonexperts (e.g., elite Mechanical Turk workers) are surprisingly ",Uhlmann et al. - Scientific Utopia III Crowdsourcing Science.pdf
"Crowdsourcing Science 723
capable at drafting quality summaries of scientific find-
ings for lay readers (Kittur, Smus, Khamkar, & Kraut, 2011). Such quality raw material could be carefully vetted and included in reviews of scientific research for practitioners and lay audiences. This suggests cautious optimism in moving down the vertical axis of Figure 1 to allow for written work from unconventional con -
tributors, with the degree of inclusiveness varying by the technical expertise and topic knowledge required for a given article.
Peer review
In the current system of academic peer review, an unpublished manuscript is submitted to a journal and evaluated by the editor and usually two to five external referees, each of whom provides detailed feedback, often over multiple rounds of revisions and serially across multiple journals. Even when successful, it can be a slow and arduous process taking months or years. For example, Nosek and Bar-Anan (2012) reported a case study of a researcher’s corpus of publications and found that the average time from manuscript submis-sion to ultimate publication was 677 days. There is little doubt that detailed feedback from colleagues can be immensely helpful, yet it remains unknown whether research reports are consistently improved by the review process (“Revolutionizing Peer Review?” 2005). Empirical studies indicate that the interrater reliability of independent assessors is low, with median reliability coefficients of .30 for journal articles and .33 for grant reviews (Bornmann, Mutz, & Daniel, 2010; Cicchetti, 1991; Marsh, Jayasinghe, & Bond, 2008) and that there is bias in favor of authors with strong networks  
(Wenneras & Wold, 1997). There are also the diminish-ing returns on time investments to consider—completing  
iterative rounds of review and revisions consumes time that might have been better allocated to pursuing a novel scientific discovery. The reviewers, typically anonymous, receive minimal professional benefit from their work, and the broader community may never hear worthy criticisms left unaddressed in the published ver -
sion of the article. Ultimately, publication in a presti-gious outlet is a poor signal of an article’s scholarly impact, with journal impact factors driven by outlier articles and only a weak predictor of the citations accrued by the typical article in the journal (Baum, 2011; Holden, Rosenberg, Barker, & Onghena, 2006; Seglen, 1997).
An alternative is to open scientific communication 
and crowdsource the peer-review process (Nosek & Bar-Anan, 2012). This moves rightward on the horizon-tal axis by opening communication and downward on the vertical axis to the extent the review process is inclusive of many commentators. Both might be accom-plished simultaneously using a centralized platform for review and discussion of research reports, with a con-tent feed similar to social-media sites (e.g., Facebook, Twitter) and users able to comment on and evaluate content as with the websites run by Reddit, Yelp,  
Amazon, and others (Buttliere, 2014). Posted files could include not only manuscripts but also data sets, code, and materials and reanalyses, replications, and critiques by other scientists. Peer review would be open, cred-ited, and citable, and prominent articles that attract attention would be evaluated by a potentially more reliable crowd of scientists rather than a small group of select colleagues. Further, reviewers would have access to the underlying data, facilitating the early iden-tification of errors (Sakaluk, Williams, & Biernat, 2014). Measures of contribution would be diverse, with schol-arly reputation enhanced not just via citations to authored manuscripts but also intellectual impact via proposals of novel ideas, the posting of data and code that others find useful, insightful feedback on others’ work, and the cura-tion of content related to specialized topic areas (e.g., replicability of the effects of mood on helping behaviors; LeBel et al., 2018). Original authors would have the opportunity to update their article in light of new evi-dence or arguments, with older versions archived, as in the Living Reviews group of journals in physics.
In contrast to such a radical bottom-right-quadrant 
approach (open communication, highly inclusive), top-right-quadrant versions of peer review would invite a crowd of topic experts carefully selected by a journal editor. However, in this more conservative scenario journal reviews would still be public, citable, and greater in number than is currently the norm. Open and citable reviews allow readers who weight traditional credentials highly to do so, whereas individuals lower in formal expertise but whose comments are high in quality have the opportunity to be recognized. The barriers to wider experimentation are not so much  
technological—there are already platforms that facilitate open scientific communication (Wolfman-Arent, 2014)—but rather social, with current professional reward structures still encouraging publication via the tradi-tional process and outlets. Only by experimenting with diverse approaches, some staying close in important respects to traditional academic review and others departing radically, can we identify the most effective ways to communicate scientific ideas and knowledge.
Replicating published findings
Among the best known uses of crowdsourcing are large-scale initiatives to directly replicate published research in psychology, biomedicine, economics, and ",Uhlmann et al. - Scientific Utopia III Crowdsourcing Science.pdf
"724 Uhlmann et al.
other fields (e.g., Alogna et al., 2014; Errington et al., 
2014; McCarthy et al., 2018; O’Donnell et al., 2018). In these crowdsourced projects, up to 100 laboratories attempt to repeat the methodology of previous studies, collecting much larger samples to provide improved statistical power to detect the hypothesized effect. Aggregating across six major replication initiatives in the social sciences, examining 190 effects in total, crowdsourced teams successfully replicated 90 (47%; Camerer et al., 2016, 2018; Ebersole et al., 2016; Klein et al., 2014, 2018; Open Science Collaboration, 2015).
A crowdsourced approach to replicability reveals that 
high levels of heterogeneity in effect-size estimates across laboratories are observed primarily for large effects, not small ones (Klein et al., 2018). In other words, effects that fail to be replicated tend to consis-tently fail to be replicated across cultures and demo-graphic populations, which casts doubt on the argument that as-yet-unidentified moderators explain the disap-pointing results. The lack of consistent laboratory dif-ferences in effect-size estimates (i.e., some research teams are not “better” than others at obtaining support for the original hypothesis; Bench et al., 2017; Klein et al., 2014, 2018) suggests that cautious scaling (e.g., moving downward on the vertical axis of Figure 1 toward greater inclusiveness) ought to be considered. The Collaborative Replications and Education Project (CREP; Grahe et al., 2013; Wagge et al., 2019) seeks to achieve this by organizing undergraduate experimental methods classes into research teams, an approach that promises to radically scale up data collection for rep-lications by integrating this activity into student educa-tion (Everett & Earp, 2015; Frank & Saxe, 2012). The Psychological Science Accelerator, an international net -
work of more than 300 psychological-science labora-tories, have committed to contributing to large-scale collaborations on an ongoing basis, including regularly involving their students via the Accelerated CREP initia -
tive (Moshontz et al., 2018).
Deciding what findings to pursue further
Faced with a voluminous and constantly growing research literature—more than 30 million academic articles have been published since 1965 (Pan et al., 2016)—and evidence that many published findings are less robust than initially thought (Begley & Ellis, 2012; Errington et al., 2014; Open Science Collaboration, 2015; Prinz, Schlange, & Asadullah, 2011), researchers must determine how best to distribute limited replica-tion resources. Viable options include focusing on highly cited articles, findings covered in student text-books, results that receive widespread media coverage, or on research with practical relevance (e.g., for gov-ernment policies or interventions to reduce demo-graphic gaps in educational attainment). The replication value of a study might be calculated on the basis of the impact of the finding relative to the strength of the available evidence (e.g., statistical power of the original demonstrations; Nosek et al., 2012).
Another complementary rather than competing 
approach is to leverage the collective wisdom of the scientific community. The aggregated estimates of crowds perform surprisingly well at predicting future outcomes—such as election results, news and sporting events, and stock-market fluctuations—because in many cases, the aggregation cancels out individual errors (Galton, 1907; Mellers et al., 2014; Surowiecki, 2005). Likewise, the averaged independent predictions of scientists regarding research outcomes—based solely on examinations of short summaries of the findings, research abstracts, or study materials—are remarkably well aligned with realized significance levels and effect sizes (Camerer et al., 2016; DellaVigna & Pope, 2018a, 2018b; Dreber et al., 2015; Forsell et al., 2018; Landy et al., 2018). Senior academics (e.g., full professors) and junior academics (e.g., graduate students and research assistants) exhibit similar forecasting accuracy  
(DellaVigna & Pope, 2018a, 2018b; Landy et al., 2018), suggesting the feasibility of an inclusive bottom-left-quadrant approach. It may be reasonable to avoid allo-cating replication resources to findings considered either clearly spurious or well-established by a hetero-geneous crowd of scientists and focus on findings about which beliefs are conflicting or uncertain.
A decision market might be used to select among 
the many available options for independent replication, the idea being to allocate resources as efficiently as possible. Crowdsourced replications will be most useful when a clear, widely agreed-on question of broad inter -
est is present. Large-scale efforts seem less appropriate for findings the community considers highly unlikely to be true (e.g., extrasensory perception) or not par -
ticularly theoretically interesting if true. Such crowd-based selection might be ongoing, with attention dynamically shifting away from effects that have expe-rienced repeated replication failures and for which the community’s expectations drop below a predetermined threshold (Dreber et al., 2015). This would help prevent cases in which numerous laboratories conduct replica-tions of an effect, collecting many thousands of partici-pants, when fewer tests would have already led to strong inferences. Decision markets might also be used to select the most and least likely populations an effect should emerge in as an initial test of universality (Norenzayan & Heine, 2005).",Uhlmann et al. - Scientific Utopia III Crowdsourcing Science.pdf
"Crowdsourcing Science 725
Crowd science can also be used to make gradual 
improvements to existing research paradigms and inter -
ventions. Lai and colleagues (Lai et al., 2014, 2016) held 
a series of crowdsourced contests to identify the best interventions for reducing implicit racial biases. Begin-ning in the top-left quadrant of Figure 1 (low commu-nication, high expertise), research teams submitted 17 interventions to reduce implicit biases (e.g., exposure to positive exemplars, perspective taking, empathy). Of those interventions, 8 successfully reduced implicit intergroup bias in the short term. Moving horizontally into the top right of the quadrant by adding the element of information exchange, teams were able to observe and learn from each other’s approaches between rounds of data collection. Several teams used this opportunity to improve their own intervention, leading to progres-sively greater effectiveness in reducing intergroup bias across rounds. We believe this contest model holds widespread applicability for identifying and improving upon practical interventions to address societal chal-lenges. We envision a future scientific landscape in which forecasting surveys and decision markets are run in tandem with research contests and other large-scale empirical data collections on an ongoing basis.
Reforms to Facilitate Large-Scale 
Collaboration
We believe most researchers have an intrinsic interest 
in contributing to the accumulation of knowledge and are not solely driven by prestige. At the same time, professional reward systems can be updated in ways to encourage voluntary participation in large-scale col-laboration and better align intrinsic and extrinsic motives. The current culture and reward system impose pressures for researchers to act independently as opposed to collectively and pursue initial evidence for novel findings rather than engage in systematic verifica-tion, more than is ideal for scientific progress. Further, although merit matters in science, there are also  
Matthew effects (Bol et al., 2018; Clauset et al., 2015;  
Merton, 1968; Petersen et al., 2011; Wahls, 2018). The resulting hierarchical and network-based arrangements interfere with inclusivity for researchers who have much to offer but come from disadvantaged back-grounds and/or lack resources. Thus, we advocate for changes to include greater rewards for collective engagement.
Distribution of grant funding
Empirical evidence suggests that distributing grant funding more evenly would increase the total return on investment in terms of scientific knowledge (Wahls, 2018). The receipt and renewal of such funds could be further linked to evidence of ongoing contributions to open science. These might include publicly posting data and materials (Simonsohn, 2013), disclosing data exclusions and stopping rules (Simmons et al., 2011), running highly powered studies (Stanley, Carter, &  
Doucouliagos, 2018), preregistering studies and analy-sis plans (Nosek, Ebersole, DeHaven, & Mellor, 2018; Nosek & Lakens, 2014; Wagenmakers, Wetzels,  
Borsboom, van der Maas, & Kievit, 2012), conducting replications, helping to develop new methods, sharing resources on platforms such as StudySwap, and partici-pating in crowdsourced initiatives, among other options. A more equitable distribution of financial support for research could reward merit and encourage excellence, not only by providing additional opportunities for those with useful skills and knowledge to contribute (Wahls, 2018) but also by directly incentivizing emerging best practices. To avoid the diffusion of responsibility on projects with many collaborators, not only authorship but also grant funding might be made contingent on specific deliverables (e.g., minimum number of partici-pants collected, provision of annotated analysis code others can reproduce).
Author contribution statements
Although some especially elaborate crowd projects involve specialized subteams who are able to publish a separate report of their work (e.g., Dreber et al., 2015; Forsell et al., 2018), these are atypical cases. Articles with many authors that report large-scale projects require reforms in how intellectual credit is allocated. Input can be documented through careful and detailed author contribution statements, which academic jour -
nals increasingly require. A good starting point for the crafting of clear contribution statements is the CRediT taxonomy (Brand, Allen, Altman, Hlava, & Scott, 2015), in which contributions throughout the full research life cycle are represented in categories such as conceptu-alization, data curation, writing, and visualization. Pro-viding information about which coauthors contributed to which CRediT categories allows collaborators to transparently communicate how authorship was deter -
mined and which author deserves credit for which com-ponents of a research project. This sort of detailed accounting is a necessary precursor for the acceptance of increasingly long author lists that are already com-monplace in fields such as high-energy physics.
Selection and promotion criteria
In addition to traditional metrics of scholarly merit, search and promotion committees should take into ",Uhlmann et al. - Scientific Utopia III Crowdsourcing Science.pdf
"726 Uhlmann et al.
account an applicant’s contributions to conducting rig-
orous research and making science better. In some fields, a demonstrated commitment to open science and scientific reform is already starting to be factored into selection and promotion decisions (Nosek, 2017;  
Schönbrodt, 2018). One way in which applicants might choose to fulfill these criteria is by participating in crowdsourced initiatives to replicate findings, reanalyze data, generate and select ideas, and so forth. Compre-hensive shifts in incentives will require that hiring and tenure and promotion committees rely more on specific indicators of contribution (Brand et al., 2015), such as the author contribution statements described above, rather than heuristics of counting articles and whether the person was first, last, or somewhere in the middle of an authorship list. In this way, individuals who led an important subcomponent of a massive project (e.g., the subteam that conducted the forecasting survey, qualitative analyses, or Bayesian meta-analysis) can be more fairly recognized.
Another more radical option is making entire project 
workflows open and linked to each contributor (some-thing possible through the Open Science Framework; http://osf.io/) and for hiring and promotion committees to examine these workflows before making their deci-sions. In a future in which open peer review becomes commonplace, online links to feedback provided on the articles of colleagues might be formally listed on one’s curriculum vitae (CV) as further evidence of intel-lectual contribution and service to the field. If the mul-tifold aspects of an academic’s workflow are made transparent, decision makers can move beyond heuris-tics and use more complete information to better allo-cate rewards on the basis of merit.
Integrating crowd science into pedagogy
Another way to encourage crowd science is to build such initiatives into activities that scientists in many fields already do routinely, such as collecting data in methods classes for student projects and analyzing complex data sets as part of graduate education (Everett & Earp, 2015; Frank & Saxe, 2012; Grahe et al., 2012; Mavor et al., 2016). The CREP (Grahe et al., 2013; Wagge et al., 2019) and Pipeline Projects (Schweinsberg et al., 2016; Schweinsberg, Viganolla, et al., 2018) offer opportunities to leverage such activities for articles with many authors that report crowdsourced replications. In these cases, for both students and course instructors, being the middle author on a report of an interesting initiative is better than no author credit at all. Crowd-sourcing avoids letting the students’ hard work collect-ing data go to waste through repeating established paradigms (e.g., the Stroop effect) in unpublishable class projects the results of which are low in informa-tion gain. As a further incentive, the second Pipeline Project offers course instructors a free curriculum they can use in their lectures, reducing course preparation time (https://osf.io/hj9zr). Whether graduate pro-grams provide opportunities for experiential educa-tion and authored work on crowd-science projects could potentially be factored into their rankings and accreditations.
Changes in publication criteria
Top-down changes in publication requirements at jour -
nals (e.g., disclosure rules and open-science badges) are already changing how science is done and what gets published (Everett & Earp, 2015; Nosek et al., 2015). Such systematic shifts in policies help to avoid collective-action problems such that only a subset of scientists engage in best practices that increase research quality but may also reduce productivity, which risks placing them at a professional disadvantage (Kidwell et al., 2016). One option, aimed at encouraging pre-publication independent replication (Schweinsberg et al., 2016), is to include independent verification of findings in another laboratory as a publication criterion at the most prestigious empirical journals (Mogil & Macleod, 2017). It is often useful to get initial evidence for a finding out there to be examined and debated by the scientific community, and individual careers should continue to advance primarily in this way. However, it is also reasonable for those publication outlets that pro-vide the most professional benefit to authors and are perhaps perceived as most authoritative (e.g., Science, Nature, Proceedings of the National Academy of Sciences) to set the bar higher. Prominent journal outlets are also increasingly recognizing the value of metascientific work that relies on a crowd approach, a trend that promises to encourage future crowdsourced projects. A more gen-eral shift in emphasis toward rigorous verification, rela-tive to novelty, as a publication criterion would incentivize high-powered crowd projects well positioned to assess the replicability and generalizability of findings.
Developing infrastructure
Another avenue is to create infrastructure and tools to make crowdsourcing easier and more efficient. Online platforms such as the Harvard Dataverse and Open Science Framework are available to host data, research and teaching materials, and preregistrations and docu-ment workflows. Journal mechanisms such as Regis-tered Reports that review methodology and accept articles in principle before data collection have now been adopted at scores of outlets (https://cos.io/rr), ",Uhlmann et al. - Scientific Utopia III Crowdsourcing Science.pdf
"Crowdsourcing Science 727
and journals are increasingly experimenting with inno-
vative formats such as open review, crowd review, and updatable articles. Recently introduced tools such as StudySwap and standing laboratory networks such as the Psychological Science Accelerator likewise hold promise to change the landscape of everyday science.
These approaches to encourage large-scale colla-
boration are important complements to reforms in how  small-team science is conducted and funded. Larger samples (Stanley et al., 2018), disclosure rules (Simmons et al., 2011), preregistration (Nosek et al., 2018; Wagenmakers et al., 2012), and Registered Report formats at journals (Chambers, 2013; Nosek & Lakens, 2014) promise to increase the true positive rate for small studies, with scaling up for crowd projects then allowing for strong inferences about the generalizability versus context sensitivity of particularly important find-ings. At the same time, crowdsourced metascientific investigations can help to assess the effectiveness of new practices intended to improve science but that may also have unwanted side effects. For instance, prereg-istration might reduce false-positive results but could also negatively affect the rate of novel discoveries by dampening creativity (Brainerd & Reyna, 2018). A crowdsourced project in progress (Ebersole et al., 2018) will randomly assign researchers to preregister their analyses of a complex data set to empirically assess the costs and benefits of this proposed reform. Finally, the encouragement of large-scale collaborations to help democratize participation in research is a complement to supporting research at teaching institutions through grants, addressing gender gaps in representation, and other efforts to reduce systematic inequalities in science.
Conclusion
Crowdsourcing holds the potential to greatly expand the scale and impact of scientific research. It seeks to promote inclusion in science, maximize material and human resources, and make it possible to tackle prob-lems that are orders of magnitude greater than what could be solved by individual minds working indepen-dently. Although most commonly used in the data-  
collection phase of research and for conducting replica-tions, opportunities to take advantage of a distributed, interdependent collective span the entire scientific endeavor—from generating ideas to designing studies, analyzing the data, replicating results, writing research reports, providing peer feedback, and making decisions about what findings are worth pursuing further. Crowd-sourcing is the next step in science’s progression from individual scholars to increasingly larger teams and now massive globally distributed collaborations. The crowdsourcing movement is not the end of the tradi-tional scholar or of the vertically integrated model. Rather, it seeks to complement this standard approach to provide more options for accelerating scientific discovery.
Action Editor
Timothy McNamara served as action editor for this article.
Author Contributions
The study was outlined and the literature review conducted 
through a crowdsourced process to which all authors con-tributed. The third through ninth authors are listed alphabeti-
cally in the byline. E. L. Uhlmann, C. R. Ebersole, C. R. 
Chartier, T. M. Errington, C. K. Lai, R. J. McCarthy, A. Riegel-man, R. Silberzahn, and B. A. Nosek drafted the body of the manuscript. C. R. Ebersole created the figure and tables. All of the authors provided critical edits and revisions and approved the final manuscript for publication.
ORCID iD
Timothy M. Errington  https://orcid.org/0000-0002-4959-5143
Declaration of Conflicting Interests
The author(s) declared that there were no conflicts of interest with respect to the authorship or the publication of this article.
Funding
This research was supported by the Center for Open Science, John Templeton Foundation, Templeton World Charity Foun-dation, Templeton Religion Trust, Arnold Ventures, James 
McDonnell Foundation, and a research and development 
grant from INSEAD.
Supplemental Material
Additional supporting information can be found at http://journals.sagepub.com/doi/suppl/10.1177/1745691619850561
References
Alogna, V. K., Attaya, M. K., Aucoin, P., Bahnik, S., Birch, S., 
Birt, A. R., . . . Zwaan, R. A. (2014). Registered replication report: Schooler & Engstler-Schooler (1990). Perspectives 
on Psychological Science, 9, 556–578.
Bakker, M., van Dijk, A., & Wicherts, J. M. (2012). The rules 
of the game called psychological science. Perspectives on 
Psychological Science, 7, 543–554.
Bakker, M., & Wicherts, J. M. (2011). The (mis)reporting 
of statistical results in psychology journals. Behavior 
Research Methods, 43, 666–678.
Ball, P. (2014). Crowd-sourcing: Strength in numbers. Nature, 
506, 422–423.",Uhlmann et al. - Scientific Utopia III Crowdsourcing Science.pdf
"728 Uhlmann et al.
Baribault, B., Donkin, C., Little, D. R., Trueblood, J., Oravecz, 
Z., van Ravenzwaaij, D., . . . Vandekerckhove, J. (2018). 
Metastudies for robust tests of theory. Proceedings of 
the National Academy of Sciences, USA, 15, 2607–2612. 
doi:10.1073/pnas.1708285114.
Baum, J. A. (2011). Free-riding on power laws: Questioning 
the validity of the impact factor as a measure of research quality in organization studies. Organization, 18, 449–
466.
Begley, C. G., & Ellis, L. M. (2012). Drug development: Raise 
standards for preclinical cancer research. Nature, 483, 
531–533.
Bench, S. W., Rivera, G. N., Schlegel, R. J., Hicks, J. A., 
& Lench, H. C. (2017). Does expertise matter in repli-
cation? An examination of the reproducibility project: Psychology. Journal of Experimental Social Psychology, 
68, 181–184.
Benoit, K., Conway, D., Lauderdale, B. E., Laver, M., & 
Mikhaylov, S. (2016). Crowd-sourced text analysis: Reproducible and agile production of political data. American Political Science Review, 110, 278–295.
Berle, D., & Starcevic, V. (2007). Inconsistencies between 
reported test statistics and p -values in two psychiatry 
journals. International Journal of Methods in Psychiatric 
Research, 16, 202–207. doi:10.1002/mpr.225
Berman, J. S., & Reich, C. M. (2010). Investigator allegiance 
and the evaluation of psychotherapy outcome research. European Journal of Psychotherapy and Counselling, 12, 11–21.
Bol, T., De Vaan, M., & van de Rijt, A. (2018). The Matthew 
effect in science funding. Proceedings of the National Academy of Sciences, USA, 15, 4887–4890. doi:10.1073/
pnas.1719557115.
Börner, K., Contractor, N., Falk-Krzesinski, H. J., Fiore, S. M., 
Hall, K. L., Keyton, J., . . . Uzzi, B. (2010). A multi-level systems perspective for the science of team science. Science Translational Medicine, 2(49), Article 49cm24. 
doi:10.1126/scitranslmed.3001399
Bornmann, L., Mutz, R., & Daniel, H.-D. (2010). A reliability-
generalization study of journal peer reviews: A multilevel meta-analysis of inter-rater reliability and its determinants. PLOS ONE, 5(12), Article e14331. doi:10.1371/journal  
.pone.0014331
Brainerd, C. J., & Reyna, V. F. (2018). Replication, registra-
tion, and scientific creativity. Perspectives on Psychological Science, 13, 428–432. doi:10.1177/1745691617739421
Brand, A., Allen, L., Altman, M., Hlava, M., & Scott, J. (2015). 
Beyond authorship: Attribution, contribution, collabora-
tion, and credit. Learned Publishing, 28, 151–155.
Budescu, D. V., & Chen, E. (2015). Identifying expertise to 
extract the wisdom of crowds. Management Science, 61, 
267–280. doi:10.1287/mnsc.2014.1909
Buttliere, B. T. (2014). Using science and psychology to 
improve the dissemination and evaluation of scientific 
work. Frontiers in Computational Neuroscience, 8, Article 
82. doi:10.3389/fncom.2014.00082
Camerer, C. F., Dreber, A., Forsell, E., Ho, T. H., Huber, J., 
Johannesson, M., . . . Wu, H. (2016). Evaluating replicability  of laboratory experiments in economics. Science, 351, 
1433–1436.
Camerer, C. F., Dreber, A., Holzmeister, F., Ho, T. -H., Huber, 
J., Johannesson, M., . . . Wu, H. (2018). Evaluating replica-bility of social science experiments in Nature  and Science 
between 2010 and 2015. Nature Human Behaviour, 2, 
637–644.
Carp, J. (2012a). On the plurality of (methodological) worlds: 
Estimating the analytic flexibility of fMRI experiments. Frontiers in Neuroscience, 6, Article 149. doi:10.3389/
fnins.2012.00149
Carp, J. (2012b). The secret lives of experiments: Methods 
reporting in the fMRI literature. NeuroImage, 63, 289–300. 
doi:10.1016/j.neuroimage.2012.07.004
Cavalier, D., & Kennedy, E. (2016). The rightful place of sci-
ence: Citizen science. Tempe, AZ: Consortium for Science, 
Policy & Outcomes.
Chambers, C. D. (2013). Registered reports: A new publishing 
initiative at Cortex. Cortex, 49, 609–610.
Chargaff, E. (1978). Heraclitean fire: Sketches from a life before 
nature. New York, NY: Rockefeller University Press.
Christensen, C. M., & van Bever, D. (2014). The capitalist’s 
dilemma. Harvard Business Review, 92, 60–68.
Church, G. M. (2005). The personal genome project. Molecular 
Systems Biology, 1, Article 2005.0030. doi:10.1038/
msb4100040
Cicchetti, D. V. (1991). The reliability of peer review for 
manuscript and grant submissions: A cross-disciplinary 
investigation. Behavioral & Brain Sciences, 14, 119–135.
Clauset, A., Arbesman, S., & Larremore, D. B. (2015). 
Systematic inequality and hierarchy in faculty hiring networks. Science Advances, 1(1), Article e1400005. 
doi:10.1126/sciadv.1400005
Clemente, F. (1973). Early career determinants of research 
productivity. American Journal of Sociology, 79, 409–419.
Cooper, S., Khatib, F., Treuille, A., Barbero, J., Lee, J., Beenen, 
M., . . . Popovic, Z. (2010). Predicting protein structures with a multiplayer online game. Nature, 466, 756–760.
Cova, F., Strickland, B., Abatista, A., Allard, A., Andow, J., 
Attie, M., . . . Zhou, X. (2018). Estimating the reproduc-ibility of experimental philosophy. Review of Philosophy and Psychology. Advance online publication. doi:10.1007/s13164-018-0400-9
Davis, R., Espinosa, J., Glass, C., Green, M. R., Massague, 
J., Pan, D., & Dang, C. V. (2018). Reproducibility proj-
ect: Cancer biology. Retrieved from https://elifesciences  
.org/collections/9b1e83d1/reproducibility-project-cancer-biology
DellaVigna, S., & Pope, D. G. (2018a). Predicting experi -
mental results: Who knows what? Journal of Political Economy, 126, 2410–2456.
DellaVigna, S., & Pope, D. G. (2018b). What motivates effort? 
Evidence and expert forecasts. The Review of Economic Studies, 85, 1029–1069.
Derex, M., & Boyd, R. (2016). Partial connectivity increases 
cultural accumulation within groups. Proceedings of the National Academy of Sciences, USA, 113, 2982–2987. 
doi:10.1073/pnas.1518798113",Uhlmann et al. - Scientific Utopia III Crowdsourcing Science.pdf
"Crowdsourcing Science 729
Dreber, A., Pfeiffer, T., Almenberg, J., Isaksson, S., Wilson, 
B., Chen, Y., . . . Johannesson, M. (2015). Using pre-
diction markets to estimate the reproducibility of scien-
tific research. Proceedings of the National Academy of 
Sciences, USA, 112, 15343–15347.
Ebersole, C. R., Atherton, O. E., Belanger, A. L., Skulborstad, 
H. M., Allen, J. M., Banks, J. B., . . . Nosek, B. A. 
(2016). Many Labs 3: Evaluating participant pool quality 
across the academic semester via replication. Journal of Experimental Social Psychology, 67, 68–82.
Ebersole, C. R., et al. (2018). Experimentally examining the 
consequences of preregistered analyses. Manuscript in 
preparation.
Eisenman, I., Meier, W. N., & Norris, J. R. (2014). A spuri-
ous jump in the satellite record: Has Antarctic sea ice 
expansion been overestimated? Cryosphere, 8, 1289–1296. 
doi:10.5194/tc-8-1289-2014
Eitan, O., Viganola, D., Inbar, Y., Dreber, A., Johanneson, 
M., Pfeiffer, T., . . . Uhlmann, E. L. (2018). Is scientific research politically biased? Systematic empirical tests and a forecasting tournament to address the controversy. 
Journal of Experimental Social Psychology, 79, 188–199.
Errington, T. M., Iorns, E., Gunn, W., Tan, F. E., Lomax, 
J., & Nosek, B. A. (2014). An open investigation of the 
reproducibility of cancer biology research. eLife, 3, Article 
e04333. doi:10.7554/eLife.04333
Everett, J. A. C., & Earp, B. D. (2015). A tragedy of the (aca-
demic) commons: Interpreting the replication crisis in psychology as a social dilemma for early-career research-ers. Frontiers in Psychology, 6, Article 1152. doi:10.3389/
fpsyg.2015.01152
Fanelli, D. (2010). “Positive” results increase down the 
Hierarchy of the Sciences. PLOS ONE, 5(4), Article e10068. 
doi:10.1371/journal.pone.0010068
Feyerabend, P. (1982). Science in a free society. London, 
England: New Left Books.
Forsell, E., Viganola, D., Pfeiffer, T., Almenberg, J., Wilson, 
B., Chen, Y., . . . Dreber, A. (2018). Predicting replication outcomes in the Many Labs 2 study. Journal of Economic 
Psychology. Advance online publication. doi:10.1016/  
j.joep.2018.10.009
Frank, M., & Saxe, R. (2012). Teaching replication. Perspectives 
on Psychological Science, 7, 600–604.
Galton, F. (1907). Vox populi. Nature, 75, 450–451.
Garcia-Berthou, E., & Alcaraz, C. (2004). Incongruence 
between test statistics and p-values in medical papers. 
BMC Medical Research Methodology, 4, Article 13. doi:10  
.1186/1471-2288-4-13
Gelman, A., & Loken, E. (2014). The statistical crisis in sci-
ence. American Scientist, 102, 460–465.
Giles, J. (2005). Internet encyclopedias go head to head. 
Nature, 438, 900–901.
Grahe, J. E., Brandt, M. J., IJzerman, H., Cohoon, J., Peng, 
C., Detweiler-Bedell, B., . . . Weisberg, Y. (2013). Collaborative Replications and Education Project (CREP). 
Retrieved from the Open Science Framework website: https://osf.io/wfc6u
Grahe, J. E., Reifman, A., Herman, A., Walker, M., Oleson, 
K., Nario-Redmond, M., & Wiebe, R. (2012). Harnessing the undiscovered resource of student research projects. Perspectives on Psychological Science, 7, 605–607.
Greenland, P., & Fontanarosa, P. B. (2012). Ending honor-
ary authorship. Science, 337, 1019. doi:10.1126/science  
.1224988
Gura, T. (2013). Citizen science: Amateur experts. Nature, 
496, 259–261.
Hand, E. (2010). Citizen science: People power. Nature, 466, 
685–687.
Henrich, J., Heine, S. J., & Norenzayan, A. (2010). The weird-
est people in the world? Behavioral & Brain Sciences, 
33, 61–83.
Hirsch, J. E. (2007). Does the h  index have predictive power? 
Proceedings of the National Academy of Sciences, USA, 104, 19193–19198.
Hofer, S. M., & Piccinin, A. M. (2009). Integrative data analysis 
through coordination of measurement and analysis proto-col across independent longitudinal studies. Psychological Methods, 14, 150–164. doi:10.1037/a0015566
Holden, G., Rosenberg, G., Barker, K., & Onghena, P. (2006). 
An assessment of the predictive validity of impact factor 
scores: Implications for academic employment decisions 
in social work. Research on Social Work Practice , 16, 
613–624.
Horton, J. (2010). Online labor markets. In A. Saberi (Ed.), 
Workshop on internet and network economics (pp. 515–522). Basel, Switzerland: Springer.
Howe, J. (2006, June 1). The rise of crowdsourcing. Wired 
Magazine. Retrieved from http://www.wired.com/wired/archive/14.06/crowds.html
Ioannidis, J. P., Tarone, R., & McLaughlin, J. K. (2011). The 
false-positive to false-negative ratio in epidemiologic studies. Epidemiology, 22, 450–456.
Ioannidis, J. P. A. (2005). Why most published research 
findings are false. PLOS Medicine , 2(8), Article e124. 
Retrieved from http://www.plosmedicine.org/article/info%3Adoi%2F10.1371%2Fjournal.pmed.0020124
Ioannidis, J. P. A. (2008). Why most discovered true associa-
tions are inflated. Epidemiology, 19, 640–648.
Ioannidis, J. P. A., & Trikalinos, T. A. (2007). An exploratory 
test for an excess of significant findings. Clinical Trials, 4, 245–253.
Jia, M., Ding, I., Falcão, H., Schweinsberg, M., Chen, Y., 
Pfeiffer, T., . . . Uhlmann, E. L. (2018). The crowdsourced generation, evaluation, and testing of research hypotheses. Manuscript in preparation.
Judd, C. M., Westfall, J., & Kenny, D. A. (2012). Treating stim-
uli as a random factor in social psychology: A new and 
comprehensive solution to a pervasive but largely ignored 
problem. Journal of Personality and Social Psychology, 
103, 54–69.
Kanefsky, B., Barlow, N. G., & Gulick, V. C. (2001, March). 
Can distributed volunteers accomplish massive data anal -
ysis tasks? Poster presented at the Proceedings of the 32nd Annual Lunar and Planetary Science Conference, Houston, TX.
Kidwell, M. C., Lazarevic ´, L. B., Baranski, E., Hardwicke, 
T. E., Piechowski, S., Falkenberg, L.-S., . . . Nosek, B. A. (2016). Badges to acknowledge open practices: A simple,  ",Uhlmann et al. - Scientific Utopia III Crowdsourcing Science.pdf
"730 Uhlmann et al.
low-cost, effective method for increasing transparency. 
PLOS Biology, 14(5), Article e1002456. doi:10.1371/  
journal.pbio.1002456doi:10.1371/journal.pbio.1002456
Kim, J. S., Greene, M. J., Zlateski, A., Lee, K., Richardson, M., 
Turaga, S. C., . . . Seung, H. S. (2014). Space-time wir-ing specificity supports direction selectivity in the retina. Nature, 509, 331–336.
Kittur, A., & Kraut, R. E. (2008). Harnessing the wisdom of 
crowds in Wikipedia: Quality through coordination. In Proceedings of the 2008 ACM Conference on Computer Supported Cooperative Work (pp. 37–46). New York, NY: Association for Computing Machinery.
Kittur, A., Lee, B., & Kraut, R. E. (2009). Coordination in col-
lective intelligence: The role of team structure and task interdependence. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (pp. 1495–1504). 
New York, NY: Association for Computing Machinery.
Kittur, A., Smus, B., Khamkar, S., & Kraut, R. E. (2011). 
CrowdForge: Crowdsourcing complex work. In Proceed-
ings of the 24th Annual ACM Symposium on User Interface Software and Technology  (pp. 43–52). New York, NY: 
Association for Computing Machinery.
Klein, R. A., Ratliff, K. A., Vianello, M., Adams, R. B., Jr., 
Bahník, Š., Bernstein, M. J., . . . Nosek, B. A. (2014). Investigating variation in replicability: A “many labs” rep-
lication project. Social Psychology, 45, 142–152.
Klein, R. A., Vianello, M., Hasselman, F., . . . Nosek, B. A. 
(2018). Many Labs 2: Investigating variation in replicabil-
ity across sample and setting. Advances in Methods and 
Practices in Psychological Science, 1, 443–490.
Kniffin, K. M., & Hanks, A. S. (2018). The trade-offs of 
teamwork among STEM doctoral graduates. American Psychologist, 73, 420–432. doi:10.1037/amp0000288
Kosmala, M., Wiggins, A., Swanson, A., & Simmons, B. (2016). 
Assessing data quality in citizen science. Frontiers in Ecology and the Environment, 14, 551–560. doi:10.1002/
fee.1436
Lai, C. K., Marini, M., Lehr, S. A., Cerruti, C., Shin, J. L., Joy-
Gaba, J. A., . . . Nosek, B. A. (2014). Reducing implicit 
racial preferences: I. A comparative investigation of 
17 interventions. Journal of Experimental Psychology: General, 143, 1765–1785.
Lai, C. K., Skinner, A. L., Cooley, E., Murrar, S., Brauer, M., 
Devos, T., . . . Nosek, B. A. (2016). Reducing implicit racial preferences: II. Intervention effectiveness across time. Journal of Experimental Psychology: General, 145, 
1001–1016.
Lakhani, K. R., Jeppesen, L. B., Lohse, P. A., & Panetta, J. A. 
(2007). The value of openness in scientific problem solv-
ing (Working Paper 07-050). Retrieved from the Harvard Business School website: https://www.hbs.edu/faculty/Publication%20Files/07-050.pdf
Landry, J. (2000, September/October). Profiting from open 
source. Harvard Business Review. Retrieved from https://
hbr.org/2000/09/profiting-from-open-source
Landy, J. F., Jia, M., Ding, I. L., Viganola, D., Tierney, W., 
Ebersole, C. R., . . . Uhlmann, E. L. (2018). Crowdsourcing 
hypothesis tests. Manuscript submitted for publication.Larrick, R. P., Mannes, A. E., & Soll, J. B. (2012). The social 
psychology of the wisdom of crowds. In J. I. Krueger 
(Ed.), Frontiers in social psychology: Social judgment 
and decision making (pp. 227–242). New York, NY: Psychology Press.
LeBel, E. P., McCarthy, R., Earp, B., Elson, M., & Vanpaemel, 
W. (2018). A unified framework to quantify the credibility 
of scientific findings. Advances in Methods and Practices 
in Psychological Science, 1, 389–402.
Lindwall, M., Cimino, C. R., Gibbons, L. E., Mitchell, M., 
Benitez, A., Brown, C. L., . . . Piccinin, A. M. (2012). Dynamic associations of change in physical activity and 
change in cognitive function: Coordinated analyses of 
four longitudinal studies. Journal of Aging Research, 2012, Article 49359812. doi:10.1155/2012/493598
List, B. (2017). Crowd-based peer review can be good and 
fast. Nature, 546, 9. doi:10.1038/546009a
Littmann, M., & Suomela, T. (2014). Crowdsourcing, the great 
meteor storm of 1833, and the founding of meteor sci-ence. Endeavour, 38, 130–138.
Lorenz, J., Rauhut, H., Schweitzer, F., & Helbing, D. (2011). 
How social influence can undermine the wisdom of crowd effect. Proceedings of the National Academy of Sciences, USA, 108, 9020–9025.
Luborsky, L., Diguer, L., Seligman, D. A., Rosenthal, R., 
Krause, E. D., Johnson, S., . . . Schweizer, E. (1999). The researcher’s own therapy allegiances: A “wild card” in comparisons of treatment efficacy. Clinical Psychology: Science and Practice, 6, 95–106.
Makel, M. C., Plucker, J. A., & Hegarty, B. (2012). Replications 
in psychology research: How often do they really occur? Perspectives in Psychological Science, 7, 537–542.
Mannes, A. E., Soll, J. B., & Larrick, R. P. (2014). The wis-
dom of select crowds. Journal of Personality and Social 
Psychology, 107, 276–299.
Manzoli, L., Flacco, M. E., D’Addario, M., Capasso, L., 
DeVito, C., Marzuillo, C., . . . Ioannidis, J. P. (2014). 
Non-publication and delayed publication of randomized trials on vaccines: Survey. British Medical Journal, 348, 
Article g3058. doi:10.1136/bmj.g3058
Marsh, H. W., Jayasinghe, U. W., & Bond, N. W. (2008). 
Improving the peer-review process for grant applications: Reliability, validity, bias, and generalizability. American 
Psychologist, 63, 160–168.
Mavor, D., Barlow, K., Thompson, S., Barad, B. A., Bonny, 
A. R., Cario, C. L., . . . Fraser, J. S. (2016). Determination 
of ubiquitin fitness landscapes under different chemical stresses in a classroom setting. eLife, 5, Article e15802. 
doi:10.7554/eLife.15802
McCarthy, R. J., Skowronski, J. J., Verschuere, B., Meijer, E. H., 
Jim, A., Hoogesteyn, K., . . . Yildiz, E. (2018). Registered replication report on Srull & Wyer (1979). Advances in 
Methods and Practices in Psychological Science, 1, 321–
336.
Mellers, B., Ungar, L., Baron, J., Ramos, J., Gurcay, B., Fincher, 
K., & Murray, T. (2014). Psychological strategies for win-
ning a geopolitical forecasting tournament. Psychological 
Science, 25, 1106–1115.",Uhlmann et al. - Scientific Utopia III Crowdsourcing Science.pdf
"Crowdsourcing Science 731
Merton, R. K. (1968). The Matthew effect in science. Science, 
159, 56–63.
MetaSUB International Consortium. (2016). The Metagenomics 
and Metadesign of the Subways and Urban Biomes 
(MetaSUB) International Consortium inaugural meeting report. Microbiome, 4, Article 24. doi:10.1186/s40168-
016-0168-z
Mogil, J. S., & Macleod, M. R. (2017). No publication without 
confirmation. Nature, 542, 409–411.
Monin, B., & Oppenheimer, D. M. (2014). The limits of direct 
replications and the virtues of stimulus sampling. Social 
Psychology, 45, 299–300.
Monin, B., Pizarro, D., & Beer, J. (2007). Deciding vs. react-
ing: Conceptions of moral judgment and the reason-affect debate. Review of General Psychology, 11, 99–111.
Moshontz, H., Campbell, L., Ebersole, C. R., IJzerman, H., 
Urry, H. L., Forscher, P. S., . . . Chartier, C. R. (2018). The psychological science accelerator: Advancing psychology through a distributed collaborative network. Advances 
in Methods and Practices in Psychological Science , 1, 
501–515.
Mueller-Langer, F., Fecher, B., Harhoff, D., & Wagner, G. G. 
(2019). Replication studies in economics—How many and which papers are chosen for replication, and why? Research Policy, 48, 62–83.
Muffatto, M. (2006). Open source: A multidisciplinary 
approach. London, England: Imperial College Press.
Mynatta, C. R., Dohertya, M. E., & Tweneya, R. D. (1977). 
Confirmation bias in a simulated research environment: An experimental study of scientific inference. Quarterly 
Journal of Experimental Psychology, 29, 85–95.
Norenzayan, A., & Heine, S. J. (2005). Psychological univer-
sals: What are they and how can we know? Psychological 
Bulletin, 135, 763–784.
Nosek, B. A. (2017, July 14). Are reproducibility and open 
science starting to matter in tenure and promotion review?  
Retrieved from the Center for Open Science website: https://cos.io/blog/are-reproducibility-and-open-science-starting-matter-tenure-and-promotion-review
Nosek, B. A., Alter, G., Banks, G. C., Borsboom, D., Bowman, 
S. D., Breckler, S. J., . . . Yarkoni, T. (2015). Promoting an open research culture. Science, 348, 1422–1425. 
doi:10.1126/science.aab2374
Nosek, B. A., & Bar-Anan, Y. (2012). Scientific utopia: I. 
Opening scientific communication. Psychological Inquiry , 
23, 217–223.
Nosek, B. A., Ebersole, C. R., DeHaven, A., & Mellor, D. M. 
(2018). The preregistration revolution. Proceedings of 
the National Academy of Sciences, USA, 115, 2600–2606. 
doi:10.1073/pnas.1708274114
Nosek, B. A., & Errington, T. M. (2017). Reproducibility in 
cancer biology: Making sense of replications. eLife, 6, 
Article e23383. doi:10.7554/eLife.23383
Nosek, B. A., & Lakens, D. (2014). Registered reports: A 
method to increase the credibility of published results. 
Social Psychology, 45, 137–141.
Nosek, B. A., Spies, J. R., & Motyl, M. (2012). Scientific uto-
pia: II. Restructuring incentives and practices to promote truth over publishability. Perspectives on Psychological 
Science, 7, 615–631.
O’Donnell, M., Nelson, L. D., Ackermann, E., Aczel, B., Akhtar, 
A., Aldrovandi, S., . . . Zrubka, M. (2018). Registered rep-lication report: Dijksterhuis & van Knippenberg (1998). Perspectives on Psychological Science, 13, 268–294. 
doi:10.1177/1745691618755704
Olmsted, D. (1834). Observations on the meteors of November 
13th, 1833. American Journal of Science and Arts , 26, 
354–411.
Open Science Collaboration. (2015). Estimating the repro-
ducibility of psychological science. Science, 349(6251), 
Article aac4716. doi:10.1126/science.aac4716
Pan, R. K., Petersen, A. M., Pammolli, F., & Fortunato, S. 
(2016). The memory of science: Inflation, myopia, and the knowledge network. Journal of Informetrics, 12, 
656–678.
Panagiotou, O. A., Willer, C. J., Hirschhorn, J. N., & Ioannidis, 
J. P. (2013). The power of meta-analysis in genome-wide association studies. Annual Review of Genomics and 
Human Genetics, 14, 441–465.
Petersen, A. M., Jung, W.-S., Yang, J.-S., & Stanley, H. E. 
(2011). Quantitative and empirical demonstration of the Matthew effect in a study of career longevity. Proceedings of the National Academy of Sciences, USA, 108, 18–23.
Piccinin, A. M., Muniz, G., Clouston, S. A., Reynolds, C. A., 
Thorvaldsson, V., Deary, I., . . . Hofer, S. M. (2013). Integrative analysis of longitudinal studies on aging: Coordinated analysis of age, sex, and education effects on change in MMSE scores. Journal of Gerontology: 
Psychological Sciences, 68, 374–390. doi:10.1093/geronb/
gbs077
Poetz, M. K., & Schreier, M. (2012). The value of crowdsourc -
ing: Can users really compete with professionals in gen-erating new product ideas? Journal of Product Innovation 
Management, 29, 245–256.
Polymath, D. H. J. (2012). A new proof of the density Hales-
Jewett theorem. Annals of Mathematics, 175, 1283–1327.
Polymath, D. H. J. (2014). New equidistribution estimates 
of Zhang type. Algebra & Number Theory, 9, 2067–2199.
Price, A., Turner, R., Stencel, R. E., Kloppenborg, B. K., & 
Henden, A. A. (2012). The origins and future of the citi-zen sky project. Journal of the American Association of 
Variable Star Observers, 40, 614–617.
Prinz, F., Schlange, T., & Asadullah, K. (2011). Believe it or 
not: How much can we rely on published data on poten-
tial drug targets? Nature Reviews. Drug Discovery , 10, 712.
Revolutionizing peer review? (2005). Nature Neuroscience, 8, 
397. doi:10.1038/nn0405-397
Saez-Rodriguez, J., Costello, J. C., Friend, S. H., Kellen, M. R., 
Mangravite, L., Meyer, P., . . . Stolovitzky, G. (2016). Crowdsourcing biomedical research: Leveraging commu-
nities as innovation engines. Nature Reviews Genetics, 
17, 470–486.
Sakaluk, J. K., Williams, A. J., & Biernat, M. (2014). Analytic 
review as a solution to the misreporting of statistical results in psychological science. Perspectives on Psychological 
Science, 9, 652–660.",Uhlmann et al. - Scientific Utopia III Crowdsourcing Science.pdf
"732 Uhlmann et al.
Salganik, M. J. (2017). Bit by bit: Social research in the digital 
age. Princeton, NJ: Princeton University Press.
Salter, S. J., Cox, M. J., Turek, E. M., Calus, S. T., Cookson, 
W. O., Moffatt, M. F., . . . Walker, A. W. (2014). Reagent 
and laboratory contamination can critically impact sequence-based microbiome analyses. BMC Biology, 12, 
Article 87. doi:10.1186/s12915-014-0087-z
Schönbrodt, F. (2018, June 25). Hiring policy at the LMU 
Psychology Department: Better have some open science track record. Nicebread. Retrieved from http://www.nicebread.de/
open-science-hiring-policy-lmu
Schooler, J. (2014). Metascience could rescue the ‘replication 
crisis.’ Nature, 515, 9.
Schweinsberg, M., Feldman, M., Staub, N., Prasad, V., 
Ravid, A., van den Akker, O., . . . Uhlmann, E. (2018). Crowdsourcing data analysis: Gender, status, and science. 
Manuscript in preparation.
Schweinsberg, M., Madan, N., Vianello, M., Sommer, S. A., 
Jordan, J., Tierney, W., . . . Uhlmann, E. L. (2016). The 
pipeline project: Pre-publication independent replica-tions of a single laboratory’s research pipeline. Journal 
of Experimental Social Psychology, 66, 55–67.
Schweinsberg, M., Viganola, D., Prasad, V., Dreber, A., 
Johannesson, M., Pfeiffer, T., . . . Uhlmann, E. L. (2018). 
The pipeline project 2: Opening pre-publication indepen-
dent replication to the world. Manuscript in preparation. 
Retrieved from https://osf.io/skq2b/
Sears, D. O. (1986). College sophomores in the labora-
tory: Influences of a narrow data base on psychology’s view of human nature. Journal of Personality and Social 
Psychology, 51, 515–530.
Seglen, P. O. (1997). Why the impact factor of journals should 
not be used for evaluating research. British Medical 
Journal, 314, 498–502.
Silberzahn, R., Uhlmann, E. L., Martin, D., Anselmi, P., 
Aust, F., Awtrey, E., . . . Nosek, B. A. (2018). Many ana-lysts, one data set: Making transparent how variations in analytic choices affect results. Advances in Methods and Practices in Psychological Science, 1, 337–356. 
doi:10.1177/2515245917747646
Simmons, J. P., Nelson, L. D., & Simonsohn, U. (2011). 
False-positive psychology: Undisclosed flexibility in data collection and analysis allows presenting any-
thing as significant. Psychological Science, 22, 1359–
1366.
Simons, D. J. (2014). The value of direct replication. 
Perspectives on Psychological Science, 9, 76–80.
Simonsohn, U. (2013). Just post it: The lesson from two 
cases of fabricated data detected by statistics alone. 
Psychological Science, 24, 1875–1888.
Simonsohn, U., Nelson, L. D., & Simmons, J. P. (2014). 
P-curve: A key to the file drawer. Journal of Experimental 
Psychology: General, 143, 534–547.
Sobel, D. (2007). Longitude: The true story of a lone genius 
who solved the greatest scientific problem of his time. London, England: Bloomsbury Publishing.
Sørensen, J. J., Pedersen, M. K., Munch, M., Haikka, P., 
Jensen, J. H., Planke, T., & Sherson, J. F. (2016). Exploring the quantum speed limit with computer games. Nature, 
532, 210–213.Srinarayan, S., Sugumaran, V., & Rajagopalan, B. (2002). A 
framework for creating hybrid-open source software com -
munities. Information Systems Journal, 12, 7–25.
Stanley, T. D., Carter, E. C., & Doucouliagos, H. (2018). What 
meta-analyses reveal about the replicability of psycho-logical research. Psychological Bulletin, 144, 1325–1346.
Steward, O., Popovich, P. G., Dietrich, W. D., & Kleitman, 
N. (2012). Replication and reproducibility in spinal cord injury research. Experimental Neurology, 233, 597–605.
Stewart, N., Chandler, J., & Paolacci, G. (2017). Crowdsourcing 
samples in cognitive science. Trends in Cognitive Sciences, 21, 736–748.
Stolovitzky, G. A., Monroe, D., & Califano, A. (2007). Dialogue 
on reverse-engineering assessment and methods: The DREAM of high-throughput pathway inference. Annals of the New York Academy of Sciences, 1115, 1–22.
Surowiecki, J. (2005). The wisdom of crowds. New York, NY: 
Anchor Books.
Tao, T., Croot, E., & Helfgott, H. (2012). Deterministic meth-
ods to find primes. Mathematics of Computation, 81, 
1233–1246.
Thelen, B. A., & Thiet, R. K. (2008). Cultivating connection: 
Incorporating meaningful citizen science into Cape Cod National Seashore’s estuarine research and monitoring programs. Park Science, 25, 74–80.
Tierney, W., Schweinsberg, M., Jordan, J., Kennedy, D. M., 
Qureshi, I., Sommer, S. A., . . . Uhlmann, E. L. (2016). Data from a pre-publication independent replication ini-tiative examining ten moral judgment effects. Scientific Data, 3, Article 160082. doi:10.1038/sdata.2016.82
Tierney, W., Schweinsberg, M., & Uhlmann, E. L. (2018). 
Making prepublication independent replication main-stream [Commentary]. Behavioral & Brain Sciences, 41, 
Article e153. doi:10.1017/S0140525X18000894
Valderas, J. M., Buckley, R., Wray, K. B., Wuchty, S., Jones, 
B. F., & Uzzi, B. (2007). Why do team authored papers get cited more? Science, 317, 1496–1498.
Visscher, P. M., Brown, M. A., McCarthy, M. I., & Yang, J. 
(2012). Five years of GWAS discovery. American Journal of 
Human Genetics, 90, 7–24. doi:10.1016/j.ajhg.2011.11.029
Wagenmakers, E.–J., Wetzels, R., Borsboom, D., van der Maas, 
H. L. J., & Kievit, R. A. (2012). An agenda for purely 
conﬁrmatory research. Perspectives on Psychological 
Science, 7, 627–633.
Wagge, J. R., Baciu, C., Banas, K., Nadler, J. T., Schwarz, S.,  
Weisberg, Y., . . . Grahe, J. (2019). A demonstration of 
the Collaborative Replication and Education Project: Replication attempts of the red-romance effect. Collabra, 
5(1), Article 5. doi:10.1525/collabra.177
Wahls, W. P. (2018). High cost of bias: Diminishing mar-
ginal returns on NIH grant 3 funding to institutions . 
Unpublished manuscript.
Wells, G. L., & Windschitl, P. D. (1999). Stimulus sampling 
in social psychological experimentation. Personality and Social Psychology Bulletin, 25, 1115–1125.
Wenger, E. (1998). Communities of practice: Learning, 
meaning, and identity. Cambridge, England: Cambridge 
University Press.
Wenneras, C., & Wold, A. (1997). Nepotism and sexism in 
peer-review. Nature, 387, 341–343.",Uhlmann et al. - Scientific Utopia III Crowdsourcing Science.pdf
"Crowdsourcing Science 733
Westra, H.-J., Jansen, R. C., Fehrmann, R. S. N., te Meerman, 
G. J., van Heel, D., Wijmenga, C., & Franke, L. (2011). 
MixupMapper: Correcting sample mix-ups in genome-
wide datasets increases power to detect small genetic 
effects. Bioinformatics, 27, 2104–2111.
Williamson, I. O., & Cable, D. M. (2003). Predicting early 
career research productivity: The case of management 
faculty. Journal of Organizational Behavior, 24, 25–44.Wolfman-Arent, A. (2014, June 5). Frustrated scholar creates 
new way to fund and publish academic work. Chronicle of 
Higher Education . Retrieved from https://www.chronicle  
.com/blogs/wiredcampus/frustrated-scholar-creates-new-route-for-funding-and-publishing-academic-work/53073
Wuchty, S., Jones, B. F., & Uzzi, B. (2007). The increasing 
dominance of teams in the production of knowledge. 
Science, 316, 1036–1038.",Uhlmann et al. - Scientific Utopia III Crowdsourcing Science.pdf
" 
1       Open Science in Developmental Science   Lisa A. Gennetian Duke University  Michael Frank Stanford University  Catherine S. Tamis-LeMonda New York University  Acknowledgements We thank Ryder Buttry and Taylor Cole for their assistance in compiling some of the background information and data for this paper. CTL acknowledges funding from the National Institutes of Child Health and Development (Grant number: R01HD094830).   ",Gennetian et al. - 2022 - Open Science in developmental science.pdf
" 
2  Abstract Open science policies have proliferated in the social and behavioral sciences in recent years, including sharing study designs, protocols, and data, and pre-registering hypotheses. Developmental research has moved more slowly than some other disciplines in adopting open science practices, in part because developmental science is often descriptive, and does not always strictly adhere to a confirmatory approach. We assess the state of open science practices in developmental science and offer a broader definition of open science that includes replication, reproducibility, data reuse, and global reach.   ",Gennetian et al. - 2022 - Open Science in developmental science.pdf
" 
3  “Progress in science is marked by reducing uncertainty about nature.” (Nosek et al., 2018)  Developmental science seeks to understand how children learn and change across the environments of everyday life. Towards this goal, the field aims to accumulate increasingly precise observations and integrate them into theories that expand the breadth, depth, and predictive accuracy of knowledge (Munafò et al., 2017). However, findings in developmental psychology – as many other fields – do not always conform to a model of cumulative science (Ioannidis, 2005; Moody et al., 2022).  Many findings may not be replicable in new samples, meaning that new data yield results inconsistent with the original (Nosek et al., 2022; Open Science Collaboration, 2015). Perhaps even more troublingly, some scientific findings may not be computationally reproducible, meaning that their numerical conclusions cannot be duplicated by a new researcher starting from the same dataset (Hardwicke, 2019; Hardwicke et al., 2021; Stodden et al., 2018). Data reuse – in which researchers share their data to foster new questions and analyses – is also rare, leading to reduced impact of scientific investments. Finally, the lens of developmental science tends to be narrow in global reach, with most studies conducted by scientists in North America and Europe on children from a handful of countries (Kidd & Garcia, 2021; Nielsen et al., 2017). These four Rs – broad goals for an open science movement – are summarized in Table 1. Research practices that interfere with these four goals are exacerbated in child development research (Davis-Kean & Ellis, 2019; Frank et al., 2017). Developmental studies often have low statistical power due to small samples of convenience that arise from the costs and challenges of recruiting and studying minors. Children are not always compliant participants, and coupled with typical challenges related to time, scheduling, and life circumstances that interrupt ",Gennetian et al. - 2022 - Open Science in developmental science.pdf
" 
4  adults and caregivers, high attrition rates magnify the complexities of securing valid and unbiased data. Furthermore, because direct instruction is not possible with young participants (e.g., infants), measurements may be inconsistent, and researchers are left to infer the meaning of highly variable responses (e.g., looking time, object manipulation, heart rate, or head-turns). And, because much of developmental science relies on video-recorded observations, sharing of raw data requires careful ethical consideration because faces and names are identifiable. Finally, disproportionate resources and access to research by investigators from different regions of the world limits the reach of developmental science.  Open science practices are designed to address these challenges, but scientific disciplines differ in the extent to which they value and adopt such practices. Moreover, within-discipline differences may exist depending on research design (e.g., pre-registration is more widely used and accepted in randomized control trials). Thus, given the wide range of what falls under the umbrella of developmental science, variation among researchers in open science practices is no surprise. The adoption of open science is not all or none, however. Instead, each additional practice moves the field incrementally towards the broader goals of reproducibility, replicability, reuse, and global reach (Klein et al., 2018). Open science practices are, if anything, even more critically central to developmental science than to other subfields. Children as research participants are a vulnerable population of great societal importance and investment. And although public and private stakeholders uniformly agree on the need to protect children’s well-being, there is no consensus on how best to do so, even within the scientific community. Thus, cultivating public trust in the scientific enterprise is paramount. Furthermore, the substantial financial and time investments required to study children’s learning and development (including costs of equipment and time burden to parents ",Gennetian et al. - 2022 - Open Science in developmental science.pdf
" 
5  and children) mean that researchers have an ethical obligation to maximize the value of public investment and to accelerate scientific discoveries through open science practices.  In this review we present key goals for the broad application of open science practices in developmental science; address the challenges in doing so; and survey the current state of the field. Our review complements existing publications (Asarnow et al., 2018; Branney et al., 2019; Gehlbach & Robinson, 2021; Klein et al., 2018; Nosek et al., 2022; Tackett et al., 2019) and adds to the wealth of available resources on open science practices by addressing issues specific to developmental science. Notably, we move beyond narrow and rigid interpretations of open science that have likely impeded its wider acceptance and adoption: Open science is not, and should not be, narrowly defined by any specific policy, whether it be data sharing, preregistration, or badges rewarding such practices (Kidwell et al., 2016). A broad and flexible view of open science must include practices that increase transparency at all stages of the scientific enterprise (Soska et al., 2021).  The Goals of Open Science: The 4 R’s The open science movement pursues scientific transparency policies to enhance replicability, improve reproducibility, accelerate discovery through reuse, and broaden global reach. In this section, we briefly review policies and practices in the service of each of these goals. Box 1 provides select examples of precedent-setting initiatives in developmental science that work towards these goals.  Enhancing the Replicability of Results in New Samples One major catalyst for the open science movement has been the failure of independent researchers to replicate the findings of many published studies (Moody et al., 2022; Nosek et al., 2022; Open Science Collaboration, 2015). Much ink has been spilled around what precisely ",Gennetian et al. - 2022 - Open Science in developmental science.pdf
" 
6  constitutes replication and when such a study should be considered successful (Nosek & Errington, 2020; Zwaan et al., 2018). In developmental science, not every finding should be replicable across samples and contexts. Yet failure of replication due to statistical issues in the original study is preventable: If a reported result is due to random sampling variation rather than true signal in the data, it is by definition un-replicable – leading to wasted efforts by teams that follow up on it. The practice of “p-hacking” – taking advantage of undisclosed analytic flexibility to report statistically significant findings – may be a major source of replication failures (Nosek et al., 2022; Open Science Collaboration, 2015).  Preregistration – traditionally defined as specifying outcomes and analyses in an open and independent repository before collecting data – is one response to the challenge of p-hacking. By reporting all conditions and measures in a study and planning hypothesis tests, experimenters can remove extra “degrees of freedom” that may lead to spurious discoveries (Nosek et al., 2018; Simmons et al., 2011). Preregistration is mandated in clinical trials (e.g., via clinicaltrials.gov) to avoid incorrect conclusions that could incur harm to patients; empirically, this policy has reduced publication and reporting bias (Kaplan & Irvin, 2015). Registered reports, in which manuscripts undergo peer review prior to data collection, are one especially promising form of preregistration for some study types (Nosek & Lakens, 2014). Not all research in developmental science adheres to a hypothesis testing approach, however. Much research is descriptive or exploratory, with researchers annotating events based on video or audio recordings or language transcriptions, and sometimes live observations, in the absence of any expectations about what they will find. Such work provides critical insights into developmental processes and can spur new hypotheses that are grounded in careful documentation of behavior. Although descriptive and exploratory research (rather than ",Gennetian et al. - 2022 - Open Science in developmental science.pdf
" 
7  confirmatory experiments) may not fit with current norms of pre-registration, many aspects of research plans can be registered before embarking on an exploratory or descriptive study. For example, researchers can pre-register (1) the source of data, study design, methods, and stimuli/materials; (2) behaviors they plan to code from video-recorded observations or language transcripts; (3) plans for calculating reliability, (4) approaches to data quality assurance; and (5) tentative plans for analysis, perhaps accompanied by a note that additional analyses will be conducted based on what researchers find (Kosie & Lew-Williams, 2022). Researchers who are unsure about certain components of their research plans can engage in sequential pre-registration (Nosek et al., 2018). Such steps increase transparency about pre-planned aspects of a study while maintaining the flexibility necessary for description and discovery in rich datasets. Increasing Analytic Reproducibility Developmental research is rich in methodological and analytic breadth that continues to expand. For example, developmental research uses neuroscience, physiology, and computational models; the great reliance on analytic code in such work amplifies concerns about computational reproducibility. Because developmental psychology as a field has a newer culture around computational methods, cultural transfer of good coding practices such as the use of version control and code review may be less common than in other subfields (Poldrack et al., 2017). Thus, open science practices that seek to decrease errors and increase reproducibility are critical for developmental research.  Unfortunately, errors in data analysis and reporting are common. An analysis of research texts found that approximately half of published psychology articles contain at least one statistical reporting error (defined as a set of reported statistical values that are internally inconsistent with one another), an estimate that is likely conservative (Nuijten et al., 2016). Furthermore, when ",Gennetian et al. - 2022 - Open Science in developmental science.pdf
" 
8  teams of analysts download data from papers (which presupposes that the data are available), only around a third of findings can be fully reproduced without assistance from the authors (Chang & Li, 2017; Hardwicke et al., 2021). These findings suggest that when errors exist, they typically cannot be detected and corrected.  Transparency – sharing both analytic code and the data on which it operates – is both a motivator for better organization and documentation by the original authors and a step that allows others to identify errors or verify results (Klein et al., 2018). The creation and availability of computational technical tools that assist with reproducing analyses have facilitated the sharing of code and data together (Aust & Barth, 2018; Kluyver et al., 2016). In some cases, requiring authors to make data available as a condition of publication acceptance or the receipt of study funding has also helped researchers incorporate time and resources costs as part of the production of research output.  The availability of code and data not only facilitates the exact reproduction of a finding, it also allows analysts to check the robustness of the finding. Robustness in this case is a term used to describe how sensitive a finding is to specific analytic decisions (including, for example, coding of the dependent variable, inclusion or exclusion of certain independent variables or covariates, and related modeling decisions). A finding can be analytically reproducible but nevertheless of limited utility if it only emerges when certain control variables are entered into a regression model (Duncan, 2014). Increasingly, when their findings depend on a complex analytic model, researchers are encouraged to include robustness checks and perhaps even report a ”multiverse” analysis in which they conduct a large set of different analyses to examine sensitivity to certain modeling or analytic decisions more formally (Steegen et al., 2016). ",Gennetian et al. - 2022 - Open Science in developmental science.pdf
" 
9  Accelerating Discovery Through Reuse A third goal of open science is to maximize the value of research and enable new discoveries by leveraging prior research investments. A lack of transparency and sharing of research methods and raw data creates considerable inefficiencies in the scientific enterprise by benefitting individual investigators while decreasing value for the broader scientific community. The rich data and methods employed by developmental scientists may make reuse opportunities even more important in developmental science than in other fields (Gilmore & Qian, 2022). As one example, video recordings of children and/or their interactions with caregivers are a cornerstone of many developmental studies that capture behaviors in the whole child and surrounding context. Their wide availability can foster video-based behavioral coding and scientific insights beyond the original motivations or primary research questions (Adolph, 2020). Similarly, readily available and publicly accessible data from resources such as the National Longitudinal Survey of Youth, the child supplement of the Panel Study of Income Dynamics, the NICHD Study of Child Care and Youth Development and the National Longitudinal Study of Adolescent Health have also been fundamental to accelerating discovery about the roles of children’s broader family and social environments in development.  Open science practices also allow researchers to aggregate data across children and developmental contexts by combining available datasets to better understand mechanisms of developmental change (Frank et al., 2021). Among its many possible contributions, aggregation allows researchers to build a sufficient sample to investigate developmental processes that affect narrow populations (e.g., children with impaired vision or hearing). Policies that require researchers to share materials, data, and code in findable, accessible, interoperable, and reusable (FAIR) repositories have been much more successful and effective thansharing “on demand” or ",Gennetian et al. - 2022 - Open Science in developmental science.pdf
" 
10  “on request” (Wicherts et al., 2006). Such sharing maximizes the scientific value of a dataset, a goal that is relevant for both research participants and funders (Brakewood & Poldrack, 2013).  Finally, repeating and building on a study may require recreating materials developed by another investigator. Because developmental research can be sensitive to the nuances of particular stimuli, dissemination of materials can play an outsized role in guiding new experiments. Although proprietary attitudes about stimuli are common – perhaps because of a desire for exclusive access to stimuli that were costly to create or to avoid questions about original results (e.g., Phillips et al., 2015) – sharing of stimulus materials can be done in ways that credit creators and substantially increase the impact of published work. For example, all materials shared on sites like Databrary and OSF contain a Digital Object Identifier (DOI) that should be cited to credit the researcher who has shared resources (Simon et al., 2015).  Expanding Reach and Building Global Capacity A final objective of open science practices is to expand the reach and inclusivity of developmental studies to represent researchers and children from regions across the globe. Variety in contexts, perspectives, and backgrounds is critical to building a global representative developmental science. However, most developmental samples are typically drawn from a narrow range of cultures, populations, and languages, which stymies  efforts to draw generalizable conclusions about the nature of developmental variation and change (Kidd & Garcia, 2021; Nielsen et al., 2017). Nurturing scientific studies from a broad range of scholars and contexts can ensure the inclusion of different populations of children through their knowledge of and sensitivity toward their cultural community. Perhaps even more importantly, researchers from different cultural communities contribute new perspectives and questions that may be important in their context but overlooked in others. Practices that expand the reach of developmental science ",Gennetian et al. - 2022 - Open Science in developmental science.pdf
" 
11  include open access to scholarly products, open sharing of research materials (from protocols to raw data of all kinds), and democratization of research evaluation.  Open access to scholarly products means that researchers around the globe can access scientific research in a timely fashion. Much literature is published behind “paywalls”, requiring an expensive institutional subscription for individuals to browse the literature freely. Individuals without institutional affiliations or those from less-resourced institutions often cannot access the most recent literature, a clear obstacle to their scientific contributions.  Open sharing of research products (e.g., protocols, stimuli, behavioral coding manuals, analytic code, video recordings, and raw and processed flat-tile data) reduces the cost of research for individuals in lower-resource contexts. Open sharing allows researchers to leverage existing materials and data in repositories for study replication, analytic reproducibility, and data reuse (e.g., coding new behaviors from video recordings of infants or by applying new analyses to longitudinal datasets). Furthermore, large collaborative projects and repositories can bring prominence to the contributions of researchers from across the globe. Finally, open science practices that focus on transparency can shift emphasis away from the evaluation of research products based on institutional and individual reputation towards evaluation of the contribution of research products. For example, by providing third-party endorsement of quality standards met by the study, clearinghouses and “what works” registries (Hill & Buckley, 2021; Home Visiting Evidence of Effectiveness Review, n.d.) ease demands on users of research and incentivize transparency.  The State of Open Science in Developmental Science Given the benefits of open science practices and many recent papers on the topic, where does the field of developmental science currently stand? In 2016, the Society for Research in ",Gennetian et al. - 2022 - Open Science in developmental science.pdf
" 
12  Child Development (SRCD, home to flagship journals of Child Development and Child Development Perspectives) convened a committee on Open Science Practices, charged with assessing the state of the field and issuing a consensus statement on behalf of the society. Three years later, the society ratified a policy suggesting that authors “note if and where” their materials are shared, and that society publications consider the future and role of pre-registered and replication studies in their flagship journals. The society’s reluctance to mandate open-science practices reflected their sensitivity to variation in attitudes among society members and concerns that open science practices can impede rather than generate and accelerate scientific progress (Gennetian et al., 2020; Gilmore et al., 2020). How has adoption of these practices progressed since? In this section, we review uptake of open science policies in developmental science.  Progress toward replicability  Funders, journals, and professional societies have increasingly encouraged researchers to embrace practices around transparency and open sharing of all forms – including study methods and materials, the use of video for demonstrating study procedures, and the pre-registration of study questions and hypotheses. Such practices promise to increase the replicability of research. Changing expectations, attitudes, and behaviors around pre-registration offers one example. Researchers engaged in NIH funded clinical and behavioral interventions must now pre-register their research questions, hypotheses, analysis plans, etc. in clinicaltrials.org. Other research funders (e.g., the Arnold Foundation) have followed this lead. Still, much basic developmental research – which largely does not entail randomized trials – is not subject to such mandates. Nevertheless, researchers interested in preregistration can use independent registry sites such as AsPredicted.org or the Open Science Framework (OSF).  OSF represents one of the largest pre-registration portals available for developmental ",Gennetian et al. - 2022 - Open Science in developmental science.pdf
" 
13  researchers: It is host to over 10,000 projects identified under search terms “child development,” “cognitive development,” “brain development,” and “socio-emotional health.” Figure 1 shows for example that the number of registered studies through OSF when searching terms based on child development epochs “infant” or “adolescent” has increased seven-fold from 2012 to 2021. Although most preregistration templates focus on hypothesis testing for randomized experiments, guidelines on pre-registration of secondary data analyses (Akker et al., 2021), longitudinal research (Petersen et al., 2022), and descriptive, discovery science (Kosie & Lew-Williams, 2022) are emerging. Norms are slowly shifting with increases to preregistration of articles published in many flagship journals of developmental science. We offer a glance at this progress within a set of 20 peer-reviewed journals in Table 1.  Reproducibility and reuse: Data sharing policies Data sharing is increasing in psychology. More than half of psychology researchers reported sharing their data, although the quality and reusability of shared datasets is often low (Borghi & Gulick, 2021; Hardwicke, 2019; Towse et al., 2021). Moreover, what constitutes sharing and what is shared (e.g., raw data vs. processed data; tabular data vs. video recordings) differs enormously across researchers.  As the primary mechanism for production of scientific scholarship, peer reviewed journals play a substantive role in shaping policies around data sharing and transparency. Yet journals have been reluctant to mandate data sharing or to enforce data-sharing policies (Wicherts et al., 2006). The Transparency and Openness Promotion (TOP) guidelines attempted to create uniform standards around data sharing practices (Nosek et al., 2015), but their adoption has been piecemeal (see Table 1). In contrast, funder mandates for data sharing have moved more quickly. Since 2011, the US National Science Foundation has required its grantees to submit a data ",Gennetian et al. - 2022 - Open Science in developmental science.pdf
" 
14  management plan, and the US National Institutes of Health mandated open data sharing for its grantees starting in 2023. Other major funders of developmental science have similarly adopted data sharing mandates, including the Wellcome Trust (2016) and the Robert Wood Johnson Foundation (2019). The next frontier for such policies will be to ensure that data are shared in a way that maximizes their value so that they are discoverable and can be combined with other data to accelerate scientific discovery. Expanding global reach Growing awareness of the need for diversity, equity, and inclusion in psychological studies has spurred efforts to broaden the representation of underrepresented groups around the world among both the scientists conducting research and the children participating in research. Such efforts include open access to publications; global access and contributions to the scientific database on children’s development; transparency and thoroughness in reporting sample demographics in published works; and mechanisms for scientists to broadly collaborate on study design, implementation, and sharing of research products.  Open access to research products. Despite widespread endorsement of open access to published studies – with many researchers viewing open access as an ethical imperative – the adoption of such practices has been inconsistent and rife with challenges. Progress towards open access in psychology has been considerable, though far from perfect. By one estimate, more than 50% of published psychology articles are available through some form of open access. Within developmental science, “gold” (paid) open access is an option for many journals (Table 1). For open access advocates, the key challenge has been engaging funders and stakeholders to avoid exacerbating disparities by burdening authors with extra publication fees (Brainard, 2021) in what risks becoming a “pay to publish” system.  ",Gennetian et al. - 2022 - Open Science in developmental science.pdf
" 
15  Yet the biggest change in the open access landscape has been the rise of “green” open access. Fueled in part by the need to disseminate research rapidly during the covid-19 pandemic (Watson, 2022), preprint servers like arXiv, bioArXiv, and psyArXiv have grown tremendously. At the time of writing, psyArXiv hosts more than 22,000 psychology papers and is growing by around 20 papers per day, with authors typically posting papers before submitting for publication at a peer-reviewed journal.  Preprints serve several purposes. First, they guarantee access to published work in un-typeset form even if the eventual published article is inaccessible. Second, and perhaps more important to their adoption, they allow authors to share their work when it is completed, addressing the career hurdles that may arise when papers are under review for months or even years. By listing preprints on a CV or in a grant application, early career researchers can gain credit for what they have accomplished to date, rather than only at the end of a long and uncertain publication process. Finally, by making research results available regardless of publication outcomes, preprints can help counter biases in the literature caused by the reluctance of some journals to publish null findings, failed replications, or applications of unconventional theories or methods (Wingen et al., 2022).  Transparency about samples. In 2020, the Society for Research on Child Development enacted policies on the reporting of sample characteristics, a direction spearheaded by former Editor in Chief Cynthia Garcia Coll. Specifically, SRCD’s policy requires authors to clearly specify recruitment procedures and sampling decisions; participant characteristics including race/ethnicity, socioeconomic status, native language; and contextual information to frame the study, including the country, region, city, and so on where data were collected. Full transparency on the characteristics and contexts of developmental studies serves the dual goals of increasing ",Gennetian et al. - 2022 - Open Science in developmental science.pdf
" 
16  diversity in the participants of research and advancing efforts towards the replicability of findings.  Access to and contributing data to the knowledge base. The broad and open sharing of research methods, behavioral coding manuals, analytic code, and raw and processed data of all forms – in repositories such as OSF and Databrary – supports diversity, equity, and inclusion by allowing researchers from around the world to both access and contribute to the knowledge base on child development. Researchers only require an internet connection to download developmental data of all forms to ask new questions on existing data or to leverage research stimuli, methods, etc. to replicate and extend existing studies to new samples. Moreover, the ability of researchers to contribute data openly provides a valuable mechanism for expanding the knowledge base on child development – from the overly narrow samples of children represented in research today to samples drawn from around the world in future studies.  Creating spaces for collaboration. Cross-lab collaboration among researchers from different countries or regions of a country (e.g., rural and urban) allows investigators from different backgrounds, languages, and cultures to share their expertise, samples, and resources to address topics of mutual interest (Frank et al., 2017). Such collaborations in turn help ensure that developmental theories and findings are grounded in the experiences of children from a variety of backgrounds. Collaborative science also allows researchers to pool resources across sites to recruit a larger sample than would be possible by a single lab, an approach that is particularly important when recruiting children from special populations where generating a sample of sufficient size is a challenge (e.g., children with disabilities). Exciting new initiatives around consortia and data repositories, described next, provide platforms to facilitate collaboration and extend the reach of developmental science.  ",Gennetian et al. - 2022 - Open Science in developmental science.pdf
" 
17  Consortia and Repositories: New Drivers of Open Science As reviewed, the adoption of open science practices in developmental science has been uneven. At the same time, the field is steadily expanding. One of the most exciting trends in the open science landscape is the rise of open, collaborative mega-projects. These are sometimes described under the umbrella of “big team science” (Coles et al., 2022), in which difficult research challenges – data collection, behavioral coding, transcriptions, materials creation, and analyses – are distributed across a community, creating a larger and more impactful product by virtue of the collaboration. In developmental science, collaborative projects and big team science have typically taken the form of either data collection consortia or collaborative data repositories. Here we highlight a few select examples to illustrate how such mechanisms can drive progress towards open science goals.  Consortium research Several developmental science consortia have adopted collaborative research initiatives that are grounded in an open-science framework. For example, ManyBabies (MB) is a research consortium for pursuing multi-lab collaborative projects. The first ManyBabies project was a replication of the well-known phenomenon of an infant-directed speech preference. The goal was to measure variability in findings across infant populations and experimental methods (ManyBabies Consortium, 2020); with 67 labs contributing data, this project represents one of the largest experimental investigations of infancy to date. Initially devised as a way of pursuing best-practices replications of important phenomena (Frank et al., 2017), the consortium has increasingly taken on new projects that require coordinated effort to solve a difficult theoretical or empirical challenge. Critically, open science practices are woven throughout ManyBabies projects: All projects are preregistered (typically as Registered Reports); all materials, analytic ",Gennetian et al. - 2022 - Open Science in developmental science.pdf
" 
18  code, and data are released as part of the project; and participation is open to the entire research community. By actively pursuing global collaborations and by lowering the barriers to participation, ManyBabies aims to broaden the set of labs and investigators contributing to cutting edge experimental work in developmental science, with a special focus on training graduate students and providing mentorship on open science practices.  The NIH-funded Play and Learning throughout A Year (PLAY) project describes a complementary but distinct model for pursuing open science goals within consortium research (play-project.org). Through a model of synergistic, collaborative science among 72 developmental researchers, PLAY will gather 1000+ hours of video of 12–24 month-old children and their mothers engaged in everyday activities in the home environment across 30 geographically distributed sites in the United States; transcribe language interactions between infants and mothers; and collaboratively annotate videos for behaviors of locomotion, emotion, object play, and language pragmatics (across 48 behavioral coding labs) following consensus standards created by subgroups of participating domain experts. The goal of PLAY is to create an openly available video corpus with annotations and associated metadata for authorized investigators on Databrary who can address new questions about learning and development during the second postnatal year. As with ManyBabies, PLAY openly shares consensus best-practice methods and guides that can be applied beyond the original study, including behavioral coding manuals, transcription guidelines, and newly created survey instruments for Spanish-English dual-language learners. Most centrally, PLAY uses video to demonstrate all aspects of the study protocol (e.g., recruitment of families; interviews with mothers about children’s vocabulary, temperament, locomotor skills; how to conduct video tours of participants’ homes, and so on).  These two projects – and others in psychology broadly (e.g., the Psychological Science ",Gennetian et al. - 2022 - Open Science in developmental science.pdf
" 
19  Accelerator; Moshontz et al., 2018) – share a commitment to open science practices with goals to broadly disseminate science practices and build capacity for researchers who likewise seek to engage in open science. For example, participants in the first ManyBabies study noted in an informal survey that they were much more likely to adopt open science practices in their own labs having tried them in a supportive context (Byers-Heinlein et al., 2020). In addition, use of the open standards, tools, and materials created by these projects provide illustrative models on the value of transparency for the next generation of student trainees and non-participating labs. Open data repositories Data repositories and platforms both promote open science and allow for generative research beyond the boundaries of original data collection goals or investigators. Large data repositories such as the Inter-University Consortium for Political and Social Research (ICPSR) provide a database of longitudinal and national studies with information about children’s developmental environments and outcomes (e.g., the National Survey of Children’s Health).   Indeed, some of these repositories have arrangements with professional organizations to also serve as co-hosts for data deposits of published research articles (e.g. OpenICPSR collaborates with, for example, the American Education Research and American Economic Associations).   Below, we describe three successful examples specific to developmental research that have overcome the perceived barriers of broad data availability related to working with vulnerable populations and collecting sensitive data.  TalkBank is an open-access repository of naturally occurring spoken language (MacWhinney, 2019). The TalkBank corpora currently contains 122 million words across 44 languages, and it has generated over 8000 publications across thousands of users. Developmental researchers have leveraged the openly shared transcripts of TalkBank to tackle questions about ",Gennetian et al. - 2022 - Open Science in developmental science.pdf
" 
20  language learning and speech production in monolingual and bilingual populations from diverse cultural communities, individuals experiencing language disfluencies (e.g., stuttering), persons with autism spectrum disorder, and so on, and it even contains transcripts from classroom settings. TalkBank’s reliance on an openly shared uniform transcription format (CHAT), facilitates combining language data across studies, and thus allows researchers to analyze various components of language – phonology, lexicon, syntax, and discourse – across larger samples than would be possible in a single lab. Databrary (databrary.org) is a restricted access video data library specialized for sharing, storing, and streaming video-based data in the form of inherently identifiable video and audio recordings of children with and without caregivers or demonstrating research methods. The database allows researchers to ask new questions by accessing video recordings of children of different ages, in different settings, engaged in a rich variety of activities (e.g., bookreading with caregivers, natural play in the home environment). Currently, Databrary has over 1,500 investigators at 700+ institutions across 5 continents, and contains over 90,000+ hours of video recordings. Databrary consent forms offer participants options for different release levels for data stored in the database, including sharing with authorized researchers, learning audiences (e.g., talks and classes), or the broader public.  Wordbank is an open database of children’s vocabulary development, archiving data from a specific parent report instrument for measuring children’s early language, the MacArthur-Bates Communicative Development Inventory (Frank et al., 2016). The repository contains data from more than 78,000 children and 39 languages, allowing for a rich characterization of how language learning varies across cultures and contexts (Frank et al., 2021). Unlike broader repositories such as OSF and Databrary, Wordbank takes advantage of the uniform format of its data to allow for ",Gennetian et al. - 2022 - Open Science in developmental science.pdf
" 
21  access and browsing of the data through both interactive online visualizations and a programmatic interface.  Challenges for Open Science in Developmental Science Although developmental science has made substantial progress in embracing open science during the past ten years, many obstacles remain. In this section, we discuss three perceived challenges for open science in developmental science: tensions between exploration and confirmation, the potential for career risks, and how open science practices intersect with the goal of fostering an inclusive science. Each of these critiques is important to understand in its own right, and also because critiques of specific, narrowly construed policies can unproductively undermine perceptions of the value of open science more broadly.  Does open science limit exploration and discovery?  Initial open science reforms focused on correcting perceived issues in experimental psychology, for example by reducing the risk of p-hacking. Thus, many policies initially assumed that preregistration simply entailed registering a hypothesis about condition differences and depositing a single tabular data file in a repository. Although this narrow conceptualization may be a useful starting point, it does not conform to many developmentalist scientists’ vision of their work, which can involve observational designs, rich behavioral and/or physiological measures, longitudinal observations, and deep appreciation of the limitations of generalizing across samples with different characteristics.  Recent reforms have been increasingly sensitive to the broader set of research goals and activities that define developmental science. For example, preregistrations may productively take many forms that promote transparency about the research process without a strict focus on hypothesis testing, as reflected in the breadth of preregistration templates available on the Open ",Gennetian et al. - 2022 - Open Science in developmental science.pdf
" 
22  Science Framework. Furthermore, initiatives like ManyBabies recognize that the combination of rich data and privacy concerns may lead to the need for more careful data sharing policies (e.g., sharing tabular data via an open channel like OSF and sharing raw video data through a more restricted access channel such as Databrary).  Does open science pose risks to career enhancement and development?  In a casual interaction between one of the authors of this paper and an early career scholar in psychology, a query about pre-registering new exploratory work was met with dismay by the early career scholar who declared, “if I pre-register this pilot work, I’ll get yelled at by everyone for what I did not get right.” Transparency in research is not always comfortable! Sharing data and analysis code can lead researchers to worry that mistakes in their work will be identified. Yet this worry may be misplaced: Researchers tend to be judged by whether they are open to correcting issues or errors in their work, not whether they are correct to begin with (Ebersole et al., 2016). Sharing resources prior to publication can also lead researchers to worry that their work will be “scooped” or their ideas will be co-opted. Both might have more severe consequences for early career researchers. Collision of ideas is a part of science, especially in fast-moving fields (Kim & Corn, 2018), but – when used appropriately – open science practices can be part of the solution, not part of the problem. For example, posting preprints is an important way to establish precedence independent of the vagaries of peer review (Kriegeskorte, 2016). And while scooping is a real issue, its risks can be mitigated by strategic decision-making about which data to release and when (Popkin, 2019). For example, a researcher might decide to make a public release of only enough data from a larger dataset to ensure reproducibility of the key results in a paper, embargoing the full raw dataset for a set period (e.g., two years) to allow for completion of a ",Gennetian et al. - 2022 - Open Science in developmental science.pdf
" 
23  publication(s). This step would signal to observers that the researcher values openness and sharing, while also protecting the researcher’s future interest.  Although it can be challenging for researchers to navigate the changing landscape of open science practices, many of these practices can have positive career value – by allowing greater transparency in understanding a researcher’s contributions and by signaling a commitment to shared scientific values. Senior researchers should encourage doctoral and postdoctoral researchers in their labs – the next generation of developmental scientists – to list open-science practices on their resumes (e.g., publications for which they share video, stimuli, methods, behavioral coding manuals, data, analytic code, etc.) so they can be properly credited for their commitment to scientific transparency and integrity.  Does open science raise barriers for inclusion and diversity in who conducts science? Many open science policies are compatible with – and in some cases, designed to – decrease historical barriers to involvement in science. Open access policies, especially preprint posting, democratize access to the scientific literature, removing the barrier of access to institutional journal subscriptions. Data, code, and materials sharing policies allow researchers without personal access to a scientific network to access experimental materials and analytic techniques to the same extent as those who occupy positions of privilege in the social network of science. Such changes are necessary to decrease inequities in science – but they are not in themselves sufficient. To diversify the research community in developmental science beyond majority groups in a small number of nations, developmental scientists must engage in active outreach, seeking to build networks that broaden participation and construct training pipelines and collaborative ventures that lead scholars from a wide range of backgrounds into the field. Such collaborations must be reciprocal in nature to ensure they are grounded in community input (i.e., ",Gennetian et al. - 2022 - Open Science in developmental science.pdf
" 
24  participatory research) at all stages of the research process – from questions, to design and methods, to implementation of the study, to analyses and dissemination of findings.  When scientists engage with complex social issues, the chance of skepticism and criticism is high. Motivated individuals may seek to undermine findings that do not align with their personal beliefs, subjecting evidence to increased scrutiny based on desired outcomes rather than the quality of research. In the face of such scrutiny, researchers may be tempted to withhold details and to shy away from transparency. But researchers should do precisely the opposite. When work is conducted with full transparency, critics and advocates can assess the strength of the evidence and researchers can easily refute charges of selective or biased reporting. Furthermore, because controversial areas of research are precisely where public trust in science is likely to be lowest, researchers should be as scrupulous as possible in following policies that are designed to maximize credibility.  Suggestions for the Future The 4Rs – replication, reproducibility, reuse, and reach – offer an organizing action-oriented framework for ongoing adoption and adaptation of open science practices in developmental science. As described, funder, journal, and institutional policies are key instruments for encouraging the adoption of open science practices. Yet, implemented in isolation, many policies meet with resistance and avoidance. Policy alone is not enough to produce concrete changes to behavior without educating stakeholders about why policies exist and how to follow them, fostering changes in social norms, creating incentives for compliance, and building capacity. We discuss each of these directions in turn.  Educating stakeholders Policy interventions are most effective when the people they affect understand them ",Gennetian et al. - 2022 - Open Science in developmental science.pdf
" 
25  (Weisman & Markman, 2017), especially when compliance with a policy requires learning new tools or thinking through new issues. Thus, for open science policies to be maximally effective, researchers need to be educated about how and why to carry them out. First, the whys and hows of open-science practices should be included in curricula of research methods classes so that students understand the potential pitfalls of closed science and learn reproducible, transparent workflows from the outset (Frank & Saxe, 2012; Hawkins* et al., 2018). Educating students about open science also helps to create a pipeline of researchers who are passionate about collaborative, open research, and helping to change norms (Tackett et al., 2019). Second, beyond the classroom, researchers need access to educational resources that help them to understand and easily deploy open sciences practices. Professional societies must play a role by offering pre-conferences, webinars, and workshops to facilitate adoption of open science practices. Funders should incorporate budget lines for open sharing practices (e.g., open access to publications that arise from a grant; funds to prepare data so they are easily findable and usable by others). Furthermore, organizations and consortia such as ManyBabies and the PLAY project can take responsibility for educating participants about the values and reasons for adopting open science practices. Changing norms As social scientists know, behavior change is difficult, especially when researchers may perceive costs or risks in adopting new practices – as is precisely the situation for open science reforms. In the face of adoption challenges, one of the most effective tools for behavior change is through a change in social norms (Sunstein, 2019). If researchers perceive that the norm in their field is to post preprints, share data and materials, and to preregister the details of their studies, they will be more likely to adopt such practices, even if they are costly. To change norms – and to ",Gennetian et al. - 2022 - Open Science in developmental science.pdf
" 
26  change researchers’ perceptions of norms – the open science movement needs to make visible the rapid adoption of open science practices over the past ten years. Such visibility can be especially powerful when social referents, scientists, and organizations with high status within their community advocate the practices (Prentice & Paluck, 2020). Thus, individual scientists – especially more prominent or senior ones – have the responsibility to adopt open science practices and endorse and advocate for those practices. Even an act as simple as highlighting policy compliance during peer review (“I commended the authors for sharing their code and data to allow me to reproduce their results”) can signal shifting norms. Indeed, communicating values and norms around open science through modeling, direct encouragement, and explanations about the value of full transparency may be more powerful than signaling style strategies such as awarding badges to researchers who engage in open science. While 4 of the 20 journals that we reviewed give badges, this practice has engendered some mixed feelings in the community and has received limited support as an intervention (Rowhani-Farid et al., 2020).  Creating incentives Crediting open science practices through informal and formal mechanisms can go a long way in incentivizing researchers to engage in such practices. Informally, individual lab policies can endorse open science practices, and encourage and train students to pre-register and openly share their research questions, hypotheses, protocols, behavioral coding, videos, raw data, analytic code, and so on in existing repositories. Course assignments that encourage students to access and use openly available data can highlight the value of open science by making them beneficiaries of the research investments made by others, and to one day pay it forward themselves. Such grassroots efforts will go a long way in producing the next generation of advocates for open science practices in developmental research.  ",Gennetian et al. - 2022 - Open Science in developmental science.pdf
" 
27  Formal incentives for open science practices can be instituted through a variety of mechanisms, including creation of DOIs associated with shared stimuli, procedures, and data to be cited by researchers who access shared resources; consideration of open science practices in personnel and tenure reviews of faculty hires and promotions; required sections of grant applications stating how and when research products will be shared; and funding streams for the preparation of data for open sharing and the reuse of existing datasets to maximize investments. Building capacity One of the most common responses to policy shifts around data sharing is grumbling around “unfunded mandates.” To recognize the importance of open science, institutions and funders must be willing to direct resources towards building and sustaining these practices. For example, the new NIH data sharing policy explicitly lists data sharing costs as allowable budget items; other funders would do well to make this norm explicit. The creation and maintenance of data sharing resources like Talkbank, Databrary, and Wordbank is also extremely resource intensive; although all three were created with US federal funding, few programs exist for funding the continued maintenance of such resources.   Finally, global collaboration requires mechanisms for creating capacity across institutions, especially outside the United States. As an example, the first ManyBabies study received a $50,000 grant from the Association for Psychological Science, which was regranted to participating labs around the world in small amounts from $500 - $2,500. Under most funding models, such small grants are impossible – for example, creating US federal subcontracts for these amounts would require dozens of hours of paperwork and paying overhead to two institutions – yet they create a substantial incentive to participate in an open, global network.  ",Gennetian et al. - 2022 - Open Science in developmental science.pdf
" 
28       Conclusion A narrow conception of open science can lead to a perceived mismatch between overly specific policies and the breadth of research that falls under the umbrella of developmental science. This conception is mistaken. We argue for a broad view of open science that revolves around four goals: increasing replicability, ensuring reproducibility, facilitating reuse, and broadening global reach. Despite impressive progress towards each of these goals in the last ten years, challenges remain. Nonetheless, there is reason to be optimistic: new practices like preprinting are expanding access to the scientific literature; new policy guidelines by funders, journals, and so on are ensuring greater levels of data sharing; and new consortia and repositories are leading the way towards a representative and robust research. These exciting new developments illustrate the ways that open and transparent science practices can be adapted to the rich variety of perspectives, methods, samples, and goals of developmental science.  ",Gennetian et al. - 2022 - Open Science in developmental science.pdf
" 
29  References Adolph, K. E. (2020). Oh, Behave! PRESIDENTIAL ADDRESS, XXth International Conference on Infant Studies New Orleans, LA, US May 2016. Infancy, 25(4), 374–392. Akker, O. R. van den, Weston, S., Campbell, L., Chopik, B., Damian, R., Davis-Kean, P., Hall, A., Kosie, J., Kruse, E., Olsen, J., Ritchie, S., Valentine, K. D., Veer, A. van ’t, & Bakker, M. (2021). Preregistration of secondary data analysis: A template and tutorial. Meta-Psychology, 5. https://doi.org/10.15626/MP.2020.2625 Asarnow, J., Bloch, M. H., Brandeis, D., Alexandra Burt, S., Fearon, P., Fombonne, E., Green, J., Gregory, A., Gunnar, M., & Halperin, J. M. (2018). Special editorial: Open science and the Journal of Child Psychology & Psychiatry–next steps? In Journal of Child Psychology and Psychiatry (Vol. 59, Issue 7, pp. 826–827). Wiley Online Library. Aust, F., & Barth, M. (2018). papaja: Create APA manuscripts with R Markdown. Borghi, J. A., & Gulick, A. E. V. (2021). Data management and sharing: Practices and perceptions of psychology researchers. PLOS ONE, 16(5), e0252047. https://doi.org/10.1371/journal.pone.0252047 Brainard, J. (2021). Open access takes flight. Science, 371(6524), 16–20. https://doi.org/10.1126/science.371.6524.16 Brakewood, B., & Poldrack, R. A. (2013). The ethics of secondary data analysis: Considering the application of Belmont principles to the sharing of neuroimaging data. Neuroimage, 82, 671–676. Branney, P., Reid, K., Frost, N., Coan, S., Mathieson, A., & Woolhouse, M. (2019). A context-consent meta-framework for designing open (qualitative) data studies. Qualitative Research in Psychology. ",Gennetian et al. - 2022 - Open Science in developmental science.pdf
" 
30  Byers-Heinlein, K., Bergmann, C., Davies, C., Frank, M. C., Hamlin, J. K., Kline, M., ..., & Soderstrom, M. (2020). Building a collaborative Psychological Science: Lessons from ManyBabies 1. Canadian Psychology, 61, 349–363. https://doi.org/10.1037/cap0000216 Chang, A. C., & Li, P. (2017). A preanalysis plan to replicate sixty economics research papers that worked half of the time. American Economic Review, 107(5), 60–64. Coles, N. A., Hamlin, J. K., Sullivan, L. L., Parker, T. H., & Altschul, D. (2022). Build up big-team science. Nature, 601(7894), 505–507. https://doi.org/10.1038/d41586-022-00150-2 Davis-Kean, P. E., & Ellis, A. (2019). An overview of issues in infant and developmental research for the creation of robust and replicable science. Infant Behavior and Development, 57, 101339. https://doi.org/10.1016/j.infbeh.2019.101339 Duncan, G. J. (2014). Replication and robustness in developmental research. Developmental Psychology, 50(11), 2417. https://doi.org/10.1037/a0037996 Duncan, G. J., & Brooks-Gunn, J. (1997). Consequences of growing up poor. Russell Sage Foundation. Ebersole, C. R., Axt, J. R., & Nosek, B. A. (2016). Scientists’ Reputations Are Based on Getting It Right, Not Being Right. PLOS Biology, 14(5), e1002460. https://doi.org/10.1371/journal.pbio.1002460 Frank, M. C., Bergelson, E., Bergmann, C., Cristia, A., Floccia, C., Gervain, J., Hamlin, J. K., Hannon, E. E., Kline, M., & Levelt, C. (2017). A collaborative approach to infant research: Promoting reproducibility, best practices, and theory-building. Infancy, 22(4), 421–435. Frank, M. C., Braginsky, M., Yurovsky, D., & Marchman, V. A. (2016). Wordbank: An open repository for developmental vocabulary data. Journal of Child Language. ",Gennetian et al. - 2022 - Open Science in developmental science.pdf
" 
31  Frank, M. C., Braginsky, M., Yurovsky, D., & Marchman, V. A. (2021). Variability and Consistency in Early Language Learning: The Wordbank Project. MIT Press. http://wordbank-book.stanford.edu Frank, M. C., & Saxe, R. (2012). Teaching replication. Perspectives on Psychological Science, 7, 595–599. https://doi.org/10.1177/1745691612460686 Gehlbach, H., & Robinson, C. D. (2021). From old school to open science: The implications of new research norms for educational psychology and beyond. Educational Psychologist, 56(2), 79–89. Gennetian, L. A., Tamis-LeMonda, C. S., & Frank, M. C. (2020). Advancing transparency and openness in child development research: Opportunities. Child Development Perspectives, 14(1), 3–8. https://doi.org/10.1111/cdep.12356 Gilmore, R. O., Cole, P. M., Verma, S., van Aken, M. A. G., & Worthman, C. M. (2020). Advancing Scientific Integrity, Transparency, and Openness in Child Development Research: Challenges and Possible Solutions. Child Development Perspectives, 14(1), 9–14. https://doi.org/10.1111/cdep.12360 Gilmore, R. O., & Qian, Y. (2022). An open developmental science will be more rigorous, robust, and impactful. Infant and Child Development, 31(1), e2254. https://doi.org/10.1002/icd.2254 Hardwicke, T. E. (2019). Data availability, reusability, and analytic reproducibility: Evaluating the impact of a mandatory open data policy at the journal Cognition | Royal Society Open Science. https://royalsocietypublishing.org/doi/full/10.1098/rsos.180448 Hardwicke, T. E., Bohn, M., MacDonald, K., Hembacher, E., Nuijten, M. B., Peloquin, B. N., deMayo5, B. E., Long, B., Yoon, E. J., & Frank, M. C. (2021). Analytic reproducibility in ",Gennetian et al. - 2022 - Open Science in developmental science.pdf
" 
32  articles receiving open data badges at Psychological Science: An observational study. Royal Society Open Science, 8. https://doi.org/10.1098/rsos.201494 Hawkins*, R. X. D., Smith*, E. N., Students, P. 254, & Frank, M. C. (2018). Improving the replicability of psychological science through pedagogy. Advances in Methods and Practices in Psychological Science. https://doi.org/10.1177/2515245917740427 Hill, K. G., & Buckley, P. (2021). Blueprints for Healthy Youth Development. Home Visiting Evidence of Effectiveness Review: Brief - 2021. (n.d.). Retrieved May 19, 2022, from https://www.acf.hhs.gov/opre/report/home-visiting-evidence-effectiveness-review-brief-2021 Ioannidis, J. P. A. (2005). Why Most Published Research Findings Are False. PLOS Medicine, 2(8), e124. https://doi.org/10.1371/journal.pmed.0020124 Kaplan, R. M., & Irvin, V. L. (2015). Likelihood of null effects of large NHLBI clinical trials has increased over time. PloS One, 10(8), e0132382. Kidd, E., & Garcia, R. (2021). How diverse is child language acquisition? Kidwell, M. C., Lazarević, L. B., Baranski, E., Hardwicke, T. E., Piechowski, S., Falkenberg, L.-S., Kennett, C., Slowik, A., Sonnleitner, C., & Hess-Holden, C. (2016). Badges to acknowledge open practices: A simple, low-cost, effective method for increasing transparency. PLoS Biology, 14(5), e1002456. Kim, J.-S., & Corn, J. E. (2018). Sometimes you’re the scooper, and sometimes you get scooped: How to turn both into something good. PLOS Biology, 16(7), e2006843. https://doi.org/10.1371/journal.pbio.2006843 Klein, O., Hardwicke, T. E., Aust, F., Breuer, J., Danielsson, H., Mohr, A. H., IJzerman, H., Nilsonne, G., Vanpaemel, W., & Frank, M. C. (2018). A practical guide for transparency ",Gennetian et al. - 2022 - Open Science in developmental science.pdf
" 
33  in psychological science. Collabra: Psychology, 4, 20. https://doi.org/10.1525/collabra.158 Kluyver, T., Ragan-Kelley, B., Pérez, F., Granger, B. E., Bussonnier, M., Frederic, J., Kelley, K., Hamrick, J. B., Grout, J., & Corlay, S. (2016). Jupyter Notebooks-a publishing format for reproducible computational workflows. (Vol. 2016). Kosie, J., & Lew-Williams, C. (2022). Open Science Considerations for Descriptive Research in Developmental Science. Kriegeskorte, N. (2016). The selfish scientist’s guide to preprint posting. The Winnower. https://doi.org/10.15200/winn.145838.88372 MacWhinney, B. (2000). The CHILDES Project: Tools for Analyzing Talk. Third Edition. Lawrence Erlbaum Associates. MacWhinney, B. (2019). Understanding spoken language through TalkBank. Behavior Research Methods, 51(4), 1919–1927. https://doi.org/10.3758/s13428-018-1174-9 ManyBabies Consortium. (2020). Quantifying sources of variability in infancy research using the infant-directed speech preference. Advances in Methods and Practices in Psychological Science, 3(1), 24–52. https://doi.org/10.1177/2515245919900809 Moody, J. W., Keister, L. A., & Ramos, M. C. (2022). Reproducibility in the Social Sciences. Annual Review of Sociology, 48. Moshontz, H., Campbell, L., Ebersole, C. R., IJzerman, H., Urry, H. L., Forscher, P. S., Grahe, J. E., McCarthy, R. J., Musser, E. D., Antfolk, J., Castille, C. M., Evans, T. R., Fiedler, S., Flake, J. K., Forero, D. A., Janssen, S. M. J., Keene, J. R., Protzko, J., Aczel, B., … Chartier, C. R. (2018). The Psychological Science Accelerator: Advancing Psychology through a Distributed Collaborative Network. Advances in Methods and Practices in ",Gennetian et al. - 2022 - Open Science in developmental science.pdf
" 
34  Psychological Science, 1(4), 501–515. https://doi.org/10.1177/2515245918797607 Munafò, M. R., Nosek, B. A., Bishop, D. V., Button, K. S., Chambers, C. D., Du Sert, N. P., Simonsohn, U., Wagenmakers, E.-J., Ware, J. J., & Ioannidis, J. P. (2017). A manifesto for reproducible science. Nature Human Behaviour, 1(1), 0021. Nielsen, M., Haun, D., Kärtner, J., & Legare, C. H. (2017). The persistent sampling bias in developmental psychology: A call to action. Journal of Experimental Child Psychology, 162, 31–38. Nosek, B. A., Alter, G., Banks, G. C., Borsboom, D., Bowman, S. D., Breckler, S. J., Buck, S., Chambers, C. D., Chin, G., Christensen, G., Contestabile, M., Dafoe, A., Eich, E., Freese, J., Glennerster, R., Goroff, D., Green, D. P., Hesse, B., Humphreys, M., … Yarkoni, T. (2015). Promoting an open research culture. Science, 348(6242), 1422–1425. https://doi.org/10.1126/science.aab2374 Nosek, B. A., Ebersole, C. R., DeHaven, A. C., & Mellor, D. T. (2018). The preregistration revolution. Proceedings of the National Academy of Sciences, 115(11), 2600–2606. https://doi.org/10.1073/pnas.1708274114 Nosek, B. A., & Errington, T. M. (2020). What is replication? PLoS Biology, 18(3), e3000691. Nosek, B. A., Hardwicke, T. E., Moshontz, H., Allard, A., Corker, K. S., Dreber, A., Fidler, F., Hilgard, J., Kline Struhl, M., & Nuijten, M. B. (2022). Replicability, robustness, and reproducibility in psychological science. Annual Review of Psychology, 73, 719–748. Nosek, B. A., & Lakens, D. (2014). Registered reports. In Social Psychology. Hogrefe Publishing. Nuijten, M. B., Hartgerink, C. H., Van Assen, M. A., Epskamp, S., & Wicherts, J. M. (2016). The prevalence of statistical reporting errors in psychology (1985–2013). Behavior Research Methods, 48(4), 1205–1226. ",Gennetian et al. - 2022 - Open Science in developmental science.pdf
" 
35  Open Science Collaboration. (2015). Estimating the reproducibility of psychological science. Science, 349(6251), aac4716. Petersen, I. T., Apfelbaum, K. S., & McMurray, B. (2022). Adapting open science and pre-registration to longitudinal research. Infant and Child Development, n/a(n/a), e2315. https://doi.org/10.1002/icd.2315 Phillips, J., Ong, D. C., Surtees, A. D. R., Xin, Y., Williams, S., Saxe, R., & Frank, M. C. (2015). A second look at automatic theory of mind: Reconsidering Kovács, Téglás, and Endress (2010). Psychological Science, 26(9), 1353–1367. https://doi.org/10.1177/0956797614558717 Poldrack, R. A., Baker, C. I., Durnez, J., Gorgolewski, K. J., Matthews, P. M., Munafò, M. R., Nichols, T. E., Poline, J.-B., Vul, E., & Yarkoni, T. (2017). Scanning the horizon: Towards transparent and reproducible neuroimaging research. Nature Reviews Neuroscience, 18(2), 115–126. Popkin, G. (2019). Data sharing and how it can benefit your scientific career. Nature, 569(7756), 445–447. https://doi.org/10.1038/d41586-019-01506-x Prentice, D., & Paluck, E. L. (2020). Engineering social change using social norms: Lessons from the study of collective action. Current Opinion in Psychology, 35, 138–142. https://doi.org/10.1016/j.copsyc.2020.06.012 Rowhani-Farid, A., Aldcroft, A., & Barnett, A. G. (2020). Did awarding badges increase data sharing in BMJ Open? A randomized controlled trial. Royal Society Open Science, 7(3), 191818. https://doi.org/10.1098/rsos.191818 Simmons, J. P., Nelson, L. D., & Simonsohn, U. (2011). False-positive psychology: Undisclosed flexibility in data collection and analysis allows presenting anything as significant. ",Gennetian et al. - 2022 - Open Science in developmental science.pdf
" 
36  Psychological Science, 22(11), 1359–1366. Simon, D. A., Gordon, A. S., Steiger, L., & Gilmore, R. O. (2015). Databrary: Enabling sharing and reuse of research video. Proceedings of the 15th Acm/Ieee-Cs Joint Conference on Digital Libraries, 279–280. Slobin, D. I. (1985). The crosslinguistic study of language acquisition: Theoretical issues (Vol. 2). Psychology Press. Soska, K., Xu, M., Gonzalez, S., Hertzberg, O., Tamis-LeMonda, C., Gilmore, R. O., & Adolph, K. E. (2021). (Hyper)active Data Curation: A Video Case Study from Behavioral Science. PsyArXiv. https://doi.org/10.31234/osf.io/89rcb Steegen, S., Tuerlinckx, F., Gelman, A., & Vanpaemel, W. (2016). Increasing Transparency Through a Multiverse Analysis. Perspectives on Psychological Science, 11(5), 702–712. https://doi.org/10.1177/1745691616658637 Stodden, V., Seiler, J., & Ma, Z. (2018). An empirical analysis of journal policy effectiveness for computational reproducibility. Proceedings of the National Academy of Sciences, 115(11), 2584–2589. Sunstein, Cass. (2019). Conformity. NYU Press. https://nyupress.org/9781479867837/conformity Tackett, J. L., Brandes, C. M., & Reardon, K. W. (2019). Leveraging the Open Science Framework in clinical psychological assessment research. Psychological Assessment, 31(12), 1386. Towse, J. N., Ellis, D. A., & Towse, A. S. (2021). Opening Pandora’s Box: Peeking inside Psychology’s data sharing practices, and seven recommendations for change. Behavior Research Methods, 53(4), 1455–1468. https://doi.org/10.3758/s13428-020-01486-1 Weisman, K., & Markman, E. M. (2017). Theory-based explanation as intervention. Psychonomic ",Gennetian et al. - 2022 - Open Science in developmental science.pdf
" 
37  Bulletin & Review, 24(5), 1555–1562. https://doi.org/10.3758/s13423-016-1207-2 Wicherts, J. M., Borsboom, D., Kats, J., & Molenaar, D. (2006). The poor availability of psychological research data for reanalysis. American Psychologist, 61(7), 726. Wingen, T., Berkessel, J. B., & Dohle, S. (2022). Caution, Preprint! Brief Explanations Allow Nonscientists to Differentiate Between Preprints and Peer-Reviewed Journal Articles. Advances in Methods and Practices in Psychological Science, 5(1), 25152459211070560. https://doi.org/10.1177/25152459211070559 Zwaan, R. A., Etz, A., Lucas, R. E., & Donnellan, M. B. (2018). Making replication mainstream. Behavioral and Brain Sciences, 41.     ",Gennetian et al. - 2022 - Open Science in developmental science.pdf
" 
38  Table 1. The 4Rs: Values for a Broader Open Science. Value Description Replication Measurement of the same effect in a new sample of participants Reproducibility Calculation of the same numerical measures from the original dataset Reuse Use of shared code, data, or materials for new research   Reach Broadening global access to scientific products     ",Gennetian et al. - 2022 - Open Science in developmental science.pdf
" 
39  Table 2. Open Science Practices of Selected Journals in Developmental Research. 
  
Statement regarding open science Socio-cultural sample description  Accepts registered reports2 Fee-based open access  Badges available Pre-prints acceptable Endorses publishing replications 
Data availability policy Annual Review of Developmental Psychology      X  None Child Development* X X X X  X  Authors must state whether data, code, and materials are available, but information will not be used in review Child Development Perspectives  X  X  X  None Current Directions in Psychological Science*    X  X  None Developmental Review*    X  X  Encouraged to state data availbility, if unaccessible or unsuitable to post, you can indicate why during the submission process Developmental Psychology* X X X   X X Must state whether data and study materials are available and if so where to access them - if not available, must state legal or ethical reasons Developmental Science   X X  X  Must include a data availbility statement to confirm if data is available and if so where it can be accessed Educational Psychologist*    X  X  Must provide info about where the data can be found & must deposit in recognized repository Educational Psychology Review* X   X  X X Encourages authors where possible in applicable to deposit data that supports the findings in a public repository Exceptional Children* X  X  X X X Make the data for the study available for independent review and provide access to procedures and code used in analysis Human Development      X  Strongly encourage authors to make all datasets available without unnecessary restrictions, required to include statement Infancy   X X  X  Encourage data sharing through NYU Databrary, must include data availibility statement and link to repository used Infant Behavior and Development X  X X  X  Encourages data sharing when appropriate and state availability of data in submission J of Abnormal Psychology* X  X  X X  Must state whether data and study materials are available and where to access them - if not available, must state legal or ethical reasons  J of American Acad of Child and Adolescent Psychiatry* X X X X  X  Different requirements based on the type of study  J of Child Psychology and Psychiatry    X X X  Encourages authors to share the data and other artifacts supporting the results in an appropriate public repository  J of Cognition and Development  X  X X X X  Required to provide a data availability statement detailing where associated data can be found and how it can be accessed, if not available must state why ",Gennetian et al. - 2022 - Open Science in developmental science.pdf
" 
40  J of the Learning Sciences*    X  X  Please provide information about where the data supporting the results or analyses presented in the paper can be found Monographs of the SRCD X X  X  X  None SRCD: Social Policy Report  X  X    None    Notes: X = Yes as of May 2022. Each of the journals included in this table mandates a conflict of disclosure form.  Each of the journals also requires an indication from authors whether data used in the study are available and whether there are any restrictions.  No journal listed here requires that data or analysis programs be made available as a condition of publication. * Journal identified as top 10 in 2020 in Development and Educational Psychology. Additional Js in the field of developmental science that accept registered reports include British J of Developmental Psychology; Developmental Cognitive Neuroscience; Infant and Child Development; J of Child Psychology; J of Child Language.     ",Gennetian et al. - 2022 - Open Science in developmental science.pdf
" 
41  Figure 1 Number of studies registered on the Open Science Framework since 2012 for each of three keywords corresponding to epochs of child development.  
  
",Gennetian et al. - 2022 - Open Science in developmental science.pdf
" 
42  BOX 1: Examples of Pioneering Open Science Precedents Reproducibility and Replicability. An influential edited volume about the impact of poverty on children’s development (Duncan & Brooks-Gunn, 1997) also modeled an early vision of reproducibility in developmental science. Authors were invited to contribute to a chapter of the volume with the condition that each chapter would begin with reporting on estimates that adhered to a regression model that followed an agreed-upon set of covariates as independent variables and an agreed-upon dependent variable. This practice resulted in a volume that could speak to the reproducibility and robustness of findings across data sets.  Reuse. Developmental science as a field has long recognized the importance of sharing datasets and technical tools that enable collaboration and reuse. For example, the Child Language Data Exchange System (CHILDES) has been a pioneer in making transcripts of speech to children freely available to the full research community (MacWhinney, 2000) since long before “big data” or “open science” were buzzwords.  Reach. One important goal of research in language acquisition is to characterize the shared learning mechanisms that – in combination with language specific environmental input – lead to the acquisition of a child’s native language(s). Pursuing this goal requires research with global reach, as recognized by a set of pioneering studies of cross-linguistic language acquisition that sought to decrease the English-centric perspective of the psycholinguistics field (Slobin, 1985).  ",Gennetian et al. - 2022 - Open Science in developmental science.pdf
"https://doi.org/10.1177/1523422319886300Advances in Developing Human
Resources
2020, Vol. 22(1) 72 –86
© The Author(s) 2020
Article reuse guidelines:
sagepub.com/journals-permissions 
DOI: 10.1177/1523422319886300
journals.sagepub.com/home/adhArticle
Collaborative Research: 
Techniques for Conducting Collaborative Research  From the Science of Team Science (SciTS)
John R. Turner1 and Rose Baker1
Abstract
The ProblemThe field of human resource development (HRD) is a multidisciplinary field of research and practice requiring collaboration. Unfortunately, the literature on how to conduct collaborative research is incomplete within HRD and other disciplines. Any breakdown in the communication, exchange of ideas, agreed-upon methodologies, or shared credit for dissemination has the potential of preventing research from moving forward. Promotion and tenure policies also hamper collaborative efforts in that these policies often reward individual initiative as opposed to collaborative outcomes. These behavioral patterns provide constraints to the improvement and betterment of efforts to changing of the guard.The SolutionThis article highlights new and improved methods for working in collaborative environments. During an academic’s transition and professional development, these methods will help emerging scholars, new to collaborative research, when facing the team science revolution.The StakeholdersScholars and scholar-practitioners engaged in collaborative research. Emerging scholars who are beginning their journey into collaborative research. Graduate students preparing for a career in academia.
Keywords
collaboration, multidisciplinary, team science
1University of North Texas, Denton, TX, USA
Corresponding Author:
John R. Turner, University of North Texas, 3940 North Elm Street, G150, Denton, TX 76207, USA. Email: john.turner@unt.edu886300 ADH XXX10.1177/1523422319886300Advances in Developing Human ResourcesT urner and Baker
research-article 2020
",1523422319886300.pdf
"Turner and Baker 73
Scientific research today has become more of a collaborative process to the point that 
it is viewed as a “team science revolution” (Bozeman & Youtie, 2017, p. 2). Others have termed this revolution a new collaboration cosmopolitanism (Bozeman & Youtie, 2017) to represent the fact that research is not only conducted by academics. It also involves practitioners from industry and stakeholders from the community. Scientific collaboration provides the benefits of promoting synergy across disciplines, holds the potential of solving larger and more complex problems, and potentially increases the impact of research outcomes due to its nature of being multidisciplinary (DeHart, 2017). Here, scientific collaboration is defined as “a form of interaction among pro-ducers of knowledge, allowing effective communication and exchange; sharing of skills, competencies and resources; working, generating and reporting findings together” (Ynalvez & Shrum, 2011, p. 205). Unfortunately, if any of these processes break down during the research effort (e.g., communication, sharing of skills), the scientific collaborative process could be hampered, or worse, could produce invalid outcomes.
The field of human resource development (HRD) is a multidisciplinary field of 
research and practice. Unfortunately, literature on how to conduct collaborative research is lacking within HRD. Conducting multidisciplinary research is challeng-ing. Each discipline adheres to its unique definitions, theories, processes, procedures, and methodologies. Reconciling any conflict or disagreement between disciplines must be performed by the researchers during the initial stages of the study. Breakdowns in communication, exchange of ideas, agreed upon methodologies, and shared credit for dissemination also have the potential of preventing the research from moving forward.
Acceptance of multidisciplinary research efforts still has its barriers in that promo-
tion and tenure requirements within traditional academic institutions continue to favor independent research over collaborative studies (Zucker, 2012). Multidisciplinary research presents a shift in traditional academic practices. Collaboration trumps indi-vidual achievement: “the scientific community realizes that to truly understand com-plex phenomena, we must transcend disciplinary boundaries” (Fiore, 2008, p. 258). To succeed in the future, researchers will need to transition efforts from individual achievements for promotion and tenure to the new emerging culture of collaboration. This transition can be difficult to achieve, both for the individual researcher and for administrators. Scholars and scholar-practitioners are beginning to realize the need to embrace multidisciplinary research. New requirements from grant funding agencies and the complexity involved in today’s research require “both broad and in–depth expertise” (Zucker, 2012, p. 780).
Preparing the field of HRD for this new team science revolution is essential in pro-
viding scholars and scholar-practitioners with the necessary resources to aid them in this collaborative transition, which is innate to changing of the guard. This change will be essential for the discipline moving forward in today’s increasingly complex land-scape. Complexity requires multiple agents (multidisciplinary) coming together to resolve a complex problem that individual agents (interdisciplinary) could not accom-plish on their own. The advantage of these collaborative methods, when practiced ",1523422319886300.pdf
"74 Advances in Developing Human Resources 22(1)
correctly, is that their results produce unexpected and astonishing outcomes—known 
as the concept of emergence from complexity science. Emergent outcomes result from collaborative practices by autonomous agents that are self-organizing and free of con-straints—the essence of changing of the guard. The current article focuses on the col-laborative aspects that can lead to, or emerge into, successful outcomes when changing of the guard becomes essential.
The remainder of this article is organized by highlighting the burgeoning trends in 
collaborative research, primarily multiauthored research by disparate researchers. This article will also differentiate between the divergent types of cross-disciplinary collabo-ration research models (unidisciplinary, multidisciplinary, interdisciplinary, transdisci-plinary). Teamwork and taskwork will be placed in context as collaborative research demands effective teamwork skills and the coordination and management of taskwork. Techniques on how to manage this teamwork along with evaluation techniques for collaborative research proficiency and accomplishment will be covered. Also pro-vided will be recommendations for necessary changes to existing policies, funding, author contributions, and promotion and tenure guidelines. These recommendations should help support scholars and scholar-practitioners in HRD as they begin to navi-gate this new collaboration cosmopolitanism.
Collaborative Research
As researchers and practitioners are called upon to address increasing levels of com-plexity in their daily practices, collaborations will be necessary rather than being optional. Addressing complex problems requires identifying relevant causal modali-ties from multiple levels of analysis, requiring consideration of multiple environmen-tal dimensions. For example, social ecology considers four environmental scales: natural, built, sociocultural, and virtual environments (Stokols, 2018). Natural envi-ronments involve nature, animals, and plants; built involves physical environments designed and constructed by humans; sociocultural environments involve organiza-tional and institutional entities; and virtual environments involve computing and mobile technologies, the internet, and virtual reality (Stokols, 2018). In considering multiple environments to identify relevant causal mechanisms, collaborative research teams are essential. The expertise of these multidisciplinary teams must also be diverse enough to provide the requisite knowledge, skills, and experiences required to narrow down the causal mechanisms from each environmental scale to only a few that may be relevant to the phenomenon being researched. As problems become more complex, collaborative endeavors require multiple research partners (e.g., academics, lay people or community members, politicians, practitioners).
The Multi-ness of Cross-Disciplinary Research
In most disciplines, multiple-authorship research articles have been increasing (Huang, 2015; Katz & Martin, 1997) and the number of single-authored research articles have been decreasing (Huang, 2015). Earlier appraisals of authorship in the basic sciences ",1523422319886300.pdf
"Turner and Baker 75
(chemistry, mathematics, physics) showed a decline in single-author papers and an 
increase in two-author papers. Following this trend over time, the number of two-author papers began to decline and the number of three-author papers began to increase. Currently, this trend has continued in each of the basic sciences in which four- and five-author papers are becoming the norm, above and beyond single-, two-, and three-author papers (Huang, 2015). Research of authorship has concluded that the increase in the number of co-authors is “ongoing and follows an exponential growth” (Huang, 2015, p. 2146) and can be attributed to “the explosion of knowledge in vari-ous fields” (Stokols, 2018, p. 330). These basic science findings are similar to what has been found in other disciplines. For example, in coronary heart disease (CHD) research, the number of multiauthored papers have increased “from 4.2 in 1981 to 6.4 in 2010” (Yu et al., 2013, p. 632), placing the average number of multiauthors per paper for this field around six. Also, in CHD research, it has been common to find that not only has the number of multiauthored research papers increased, but it also has been documented that the number of multi-institutional collaboration efforts have also increased, from 23% in 1981 to 56% in 2010 (Yu et al., 2013). In addition, the number of multinational (global) collaboration efforts have also increased, from 2% in 1981 to 19% in 2010 (Yu et al., 2013).
Similar trends have been realized within the HRD academic journals (see Figure 1). 
The number of authors per published research article (not including editorials, reviews, reaction articles, etc.) were coded for each of the four HRD journals as shown in Figure 1. The number of single-authored research papers for Human Resource Development Quarterly (HRDQ) decreased from 87 articles between 1990 and 1999 to 57 between 2000 and 2009, and down again to 25 between 2010 and 2018. This journal increased the number of articles with more than three authors substantially after 2000 and first published articles with six and eight authors within the time frame of 2010 to 2018. Human Resource Development Review (HRDR) showed similar trends with a total of 73 single-authored articles between 2002 and 2009 with a decrease to 51 single-authored articles from 2010 to 2018. This journal, HRDR, had published articles with one to five authors during the period of 2002 to 2009 and expanded to include articles with six and seven authors from 2010 to 2018. Human Resource Development International (HRDI) decreased single-authored articles from 122 between 1998 and 2009 to 68 between 2010 and 2018. Authorship in the time frame between 1998 and 2009 included articles with one through five authors and one article with eight authors. Between 2010 and 2018, more articles were published with multiple authors with a range between one and nine authors with one article having 11 authors. As a special issue journal and one that promotes collaboration, Advances in Developing Human Resources (ADHR) showed similar trends. Single authorship reduced from 192 published articles between 1999 and 2008 to 119 published articles between 2009 and 2018. Both time frames represented multiple-authored articles with as many as six authors, and one article included nine authors in the earlier period between 1999 and 2008. The trends for each of the four journals highlight the point that single-authored research is becoming less common and that the trend of multiple-authored research has impacted the field of HRD and is expected to continue to grow.",1523422319886300.pdf
"76 Advances in Developing Human Resources 22(1)
This increasing trend in multiauthor, multi-institutional, and multinational research 
collaborations has been found in other disciplines (Rahman et al., 2017; Yu et al., 
2013). This increasing trend is beginning to raise some concerns as well. For exam-ple, as the number of multiauthor research studies increase, researchers should be trained on how to collaborate more effectively, especially when working across other 
Figure 1. Authorship trends in HRD journals.
Note. HRD = human resource development; HRDQ = Human Resource Development Quarterly;  
HRDI = Human Resource Development International; HRDR = Human Resource Development Review; ADHR = Advances in Developing Human Resources.",1523422319886300.pdf
"Turner and Baker 77
disciplines, across different institutions, and across various countries and language 
barriers. This increasing trend falls under the umbrella of cross-disciplinary research in which a shift in traditional academic practices involving collaboration trumps indi-vidual achievement, creating a culture of collaboration, a collaboration cosmoplitan-ism, or a team science revolution (Bozeman & Youtie, 2017).
Cross-Disciplinary Collaboration
When dealing with cross-disciplinary research, multiple agents from differing disci-plines are involved in exchanging multiple perspectives, philosophies, and theories aimed toward the phenomenon and research problem. Some of the benefits of conduct-ing cross-disciplinary collaboration research follow:
••Complex modern problems such as climate change and resource security are not amenable to single-discipline investigation.
••Discoveries are said to be more likely on the boundaries between fields.
••These encounters with others benefit single disciplines, extending their hori-zons (Rylance, 2015, p. 313).
However, even with the numerous benefits, cross-disciplinary research comes 
with many challenges as well. Addressing these challenges and issues when conduct-ing cross-disciplinary research is the focus of the field of the Science of Team Science (SciTS; Falk-Krzesinski et al., 2010). Building upon knowledge gained from the field 
of Team Science (TSci), SciTS has concentrated efforts primarily on conducting cross-disciplinary research. The field of SciTS is “concerned with understanding and managing circumstances that facilitate or hinder the effectiveness of collaborative cross-disciplinary science, and the evaluation of collaborative science outcomes” (Falk-Krzesinski et al., 2010, p. 263). TSci views cross-disciplinary research as a 
“process of teamwork to be mastered” (Fiore, 2008, p. 256).
Depending on the level of complexity in a research problem, there are generally 
four different types of cross-disciplinary collaboration research models (i.e., uni-disciplinary, multidisciplinary, interdisciplinary, transdisciplinary). Unidisciplinary includes research that is conducted within a single discipline. Multidisciplinary includes research that involves contributions from multiple disciplines (DeHart, 2017), defined as the “coordinated efforts of some set of disciplines designed to achieve some common goal or goals” (Fiore, 2008, p. 254). Interdisciplinary involves integrative contributions from multiple disciplines; this integration is nec-essary as no single discipline can address the issue alone (DeHart, 2017). The out-come of interdisciplinary research often involves new ways of understanding, new lexicon to describe an emerging phenomena, and new techniques in developing new theories (Fiore, 2008).
Transdisciplinary research, in contrast, involves multiple disciplines bringing novel 
contributions to a study (DeHart, 2017). One example of transdisciplinary is when a new method needs to be developed to address a problem because existing methods are ",1523422319886300.pdf
"78 Advances in Developing Human Resources 22(1)
not adequate for the stated problem. Transdisciplinary also involves, at times, stake-
holders and community members as part of the research effort (DeHart, 2017).
The following sections identify what teamwork and taskwork entail along with an 
introduction of the critical components that a researcher should concentrate on when working in a team setting. Then, team contracts are discussed along with collaborative research agreements that need to be decided in advance of beginning research. Last, an introduction of contextual factors for conducting cross-disciplinary collaborations is presented.
Teamwork and Taskwork
For teams, processes are divided into two types, teamwork and taskwork. Teamwork identifies the interactions required among team members to achieve their tasks, whereas taskwork involves the activities that must be completed to complete the team’s task or objective. Teamwork relates to team member interactions, the sharing of information required at any point in time for team members to complete their tasks, the knowledge of who has what skills so that they can easily be identified when required, and basic com-munication among team members throughout the process. Taskwork refers to the actual activities (technical, physical, procedural) involved in completing a task or subtask.
The field of team science has identified nine core processes that make up team-
work. These nine core processes have been categorized as being either an emerging state (coaching, cognition, conflict, cooperation, and coordination) or an influencing condition (context, composition, and culture; Dihn & Salas, 2017; Turner et al., 2018). Emerging states relate to internal team dynamics that can be managed, whereas influ-encing conditions are external dynamics in which the team has little control.
Identifying the conditions that can be managed compared with those that cannot be 
managed would support the efforts of scholars and scholar-practitioners in any col-laborative endeavor. For example, it is important for new scholars to identify what the context is for their new academic home. Comparing a department’s research interest with one’s own research agenda would aid new scholars to identify better with their colleagues and the department. In the short-term, the new scholar would not be able to make changes if their research agenda was not in line with their new academic home. This new scholar would most likely have to alter an existing research agenda to meet the department’s needs. This is an example of having little to no control over the influ-encing conditions. Scholar-practitioners would benefit from identifying with the emerging states that they can contribute to in a positive manner. By better contributing to some of these emerging states will place a scholar-practitioner in the role of a team player and potentially as an appointed team leader. This is just one example of how scholar-practitioners can influence teamwork by identifying with the emerging states.
Techniques for Collaborating
Teamwork activities requires each team member to work both independently and interdependently. Team members are required to work interdependently to “the extent ",1523422319886300.pdf
"Turner and Baker 79
to which a job is contingent on others’ work” (Cordery & Tian, 2017, p. 111). 
Resulting in team members being dependent on other team members completing their tasks. Task interdependence requires decisions at the team level involving “the specialization and differentiation of activities and tasks within teams, where the separation of activity elements generates explicit coordination requirements, or through the application of formalized routines that specify particular task and work flows” (p. 111). Essentially, successful teaming requires monitoring and managing these emergent states (coaching, cognition, conflict, cooperation, and coordination) by all team members.
Planning a team’s activities/goals/tasks takes work. One common mistake that is 
often realized when beginning cross-disciplinary collaborations is in underestimating the level of commitment and personal relationships required in such an effort (Ledford, 2015). Each of the emerging states of teamwork must be managed throughout the pro-cess. One tool for managing these emerging states can be found in Figure 2. Figure 2 provides a brief example of a team agreement contract that each team member contrib-utes to. A team or collaborative agreement should cover a team’s goals and schedule, information pertaining to who will perform which tasks and when, credit for task accomplishment acknowledged by peers (other team members), contingencies around communicating, and any conflicts of interests (Bennett & Gadlin, 2012).
The team agreement contract in Figure 2 provides information on who is on the 
team along with the team’s name. When completing this form, team members will have an opportunity to get to know the other team members as well as their knowl-edge, skills, experiences, and area of expertise (discipline). The action of includ-ing a name for the team helps to begin forming a sense of team identify among the team members. These initial activities will help in developing transactive memory systems for the team members, identifying who knows what and who has which skills. This exercise also begins to develop the team’s cognition, the organized understanding of collective knowledge among team members (Mohammed & Dumville, 2001).
The second section requires group members to identify what the team’s task goals 
and process goals are. During this stage, if this was being used for a multidisciplinary research effort, it would be recommended to include a row on problem identification. Identifying the problem is a critical step prior to assigning tasks and process goals. This step initiates the communication and coordination steps of teamwork by requiring each team member to identify and agree on the listed goals. The following section of the form identifies who will do what along with who is appointed as the team leader. Not all teams operate with a team leader, some utilize shared leadership models or incorporate a blend of coaching and leadership. Either is fine; it is more a matter of what works best for the team. Also, this section identifies who is to complete what assignment or task.
Next in the form is a section on the team’s rules of conduct. The team decides on 
how they wish to conduct business. The section on evaluation includes a debriefing session after a milestone or after a set of subtasks have been completed. This debrief-ing is part of a continuous learning cycle; team members identify what worked and ",1523422319886300.pdf
"80 Advances in Developing Human Resources 22(1)
Figure 2. Team agreement contract.",1523422319886300.pdf
"Turner and Baker 81
what did not work as well as planned, then discuss options to make the teamwork 
processes work better during the next cycle. The last section involves each team mem-ber agreeing to the information on the form and willing to adopt the changes recom-mended by the team for the next cycle.
This process of having the team agree to the team’s goals, coordination, communi-
cation, and leadership begins the process of managing the emerging states of team-work. Scholar-practitioners can utilize this form when working in team settings with their customer, whereas scholars can easily modify this form for use in any type of cross-disciplinary research activity. Furthermore, this form has been used successfully in a number of team tasks in the classroom, which can benefit both scholars and stu-dents. Utilization of this form addresses each of the critical techniques of strengthen-ing teamwork to (Bennett & Gadlin, 2012)
••Foster an environment that is collegial and nonthreatening.
••Openly recognize strengths of all members of the team and discuss how these different strengths contribute to advancing the project.
••Take a few minutes at regularly scheduled group meetings to do a check-in. Ask how everyone is doing.
••Encourage open and honest discussion by establishing trust.
••Jointly develop a process for bringing issues and disagreements forward for early resolution.
••Assure that when decisions are being made, which require everyone’s input, that each person has an opportunity and understands the process for providing comment.
••Schedule periodic assessments and feedback, including opportunities for col-laborators to discuss what is going well, what is not, and what needs to be improved.
Implications for HRD Practitioners
Practitioners will need to be even more aware of how to gain credit when contributing to a collaborative research project, how to gain funding for collaborative research projects. In addition, scholar-practitioners will also need to know how to show impact from their collaborative efforts to better advance one’s self for promotion and tenure. These items are discussed in the following sections.
Author’s Contribution
Additional questions have been raised as to how to account for each author’s contribu-tion to a multiauthor research paper. This collaborative contribution record aids in displaying an individual author’s impact to their field, typically by citation counts, but it also contributes to the author’s evaluation for promotion and tenure. Contribution should be decided upon the following criteria set by the Committee on Publication Ethics (COPE):",1523422319886300.pdf
"82 Advances in Developing Human Resources 22(1)
1. Substantial contributions to conception and design, or acquisition of data, or 
analysis and interpretation of data;
2. Drafting the article or revising it critically for important intellectual content; and
3. Final approval of the version to be published (Albert & Wager, 2003).
Authorship contribution should be based on each author meeting all three conditions throughout the research and writing processes. Discussions relating to the order of authorship should begin during the start of any research project rather than later as well as publication and conference intentions of the final output (Albert & Wager, 2003).
Contribution can be evaluated in two general methods: the full counting approach 
and the fractional counting approach (Rahman et al., 2017). Full counting provides credit to each author for an article’s citation, whereas fractional counting partitions the citation into a part per each author such that each author would receive one (cita-tion) divided by the number of authors (n ; 1/n). Different variations of this fractional 
approach can be found in the literature (see Rahman et al., 2017). The point remains, however, that attribution for an author’s contribution in multiauthored papers is cur -
rently being considered within academia. These discussions have the potential of negatively impacting one’s academic career if, for example, one’s contribution was unfairly measured using a fractional measure that assigned the researcher little to no credit for participation in the multiauthored contribution. This could deeply impact one’s chances of being promoted to a tenured faculty position, further highlighting the need for scholars and students (emerging scholars) to be able to counter such practices.
Funding
Other problems that accompany this trend of cross-disciplinary research is that national and international funding agencies are beginning to require that research be conducted by cross-disciplinary teams, typically of the variety of multidisciplinary or transdisci-plinary research teams due to the complexity involved in today’s environment and problems (Zucker, 2012).
Funding agencies have been encouraged to support cross-disciplinary efforts. 
One example of this is in the Global Research Council’s (GRC) report on interdisci-plinarity. Regarding recommendations to funding agencies, the GRC provided the following:
••Our research and much of the literature suggest that top-down thematic funding programs are one of the most common approaches adopted by funders to encourage interdisciplinarity.
••There is also a consensus that researcher-led “bottom-up” approaches are required and funding agencies should support such approaches despite the potential risks associated with the most innovative ideas.",1523422319886300.pdf
"Turner and Baker 83
••At the same time, interdisciplinary research should be viewed as a means to an 
end and not an end in itself. Several funding agencies emphasized that practices and policies toward interdisciplinarity should be driven by the required out-comes and scientific demand (Gleed & Marchant, 2016).
Promotion and Tenure Practices
Typically, academic institutions’ promotion and tenure policies still favor individual research (Rylance, 2015). Also, current bibliometrics favor individual efforts as opposed to providing collaborative measures. This results, in many cases for early career aca-demics, in advisement against cross-collaborative research in favor of concentrating one’s time and effort on research that is individual, unidisciplinary, or both (Rylance, 2015). Future changes in promotion and tenure practices will need to be considered, especially with current and future changes to the requirements from grant-funding insti-tutions. Scholars and scholar-practitioners will be better served to collaborate, joining academia with practice, to gain future funding for research. Receipt of such funding could potentially be the catalyst to change the requirements for promotion and tenure, at least at one’s current institution.
Evaluation Techniques
There are various evaluation measures for collaboration and transdisciplinary research projects. One such tool is the research orientation scale provided by the National Cancer Institute (NCI; Fload Expert, 2010). This tool is available free and is included as part of the Team Science Toolkit sponsored by NCI (https://www.teamsciencetool-kit.cancer.gov/Public/Home.aspx). Another evaluation tool is the transdisciplinary ori-entation scale provided by Mirsa et al. (2015). The transdisciplinary orientation scale provides “a reliable and preliminarily validated tool to measure scientists’ and schol-ars’ personal disposition toward transdisciplinary research” (Mirsa et al., 2015, p. 6).
These evaluation tools could be tested and validated for collaborative research 
within the HRD community. Future research could entail utilizing either of these eval-uation tools to identify the level of collaboration among collectives along with extend-ing HRD’s knowledge base by introducing new evaluation tools that could be used by scholars and scholar-practitioners within the field of HRD.
Conclusion
Research is a collaborative process. Advances in science rely partially on social inter -
action among other scientists (Katz & Martin, 1997). These interactions have, in some cases, formed into larger, organized, collaborative networks such as the Academy of Human Resource Development (AHRD) and the SciTS. Funding agencies have also begun to require applicants to show collaborative efforts when documenting research compared with single-discipline research. Part of this drive for more cross-disciplinary research is because “it has a key role to play in addressing the grand challenges that ",1523422319886300.pdf
"84 Advances in Developing Human Resources 22(1)
society faces” (Gleed & Marchant, 2016, p. 5). Multiauthorship research is more com-
mon in many disciplines and has been shown to already be acceptable in HRD.
This shift toward collaborative research identifies with the theme of this special 
issue in that it identifies the shifting roles that scholars and scholar-practitioners will face shortly regarding cross-disciplinary research. The potential changes in policies required from the new demands of cross-disciplinary research to gain funding will indeed impact the future of current and emerging scholars.
Managing and adjusting the collaborative processes, current practices, and policies 
will take time. These items are critical and must be realized for scholars and scholar-practitioners to advance into the future of collaborative research. Achieving emer -
gence, through collaborative research practices, is essential for those who practice HRD as it is a multidisciplinary field of study. This emergence addresses the theme of this special issue, Changing of the Guard. Scholars and scholar-practitioners need to be responsive and adaptable to this transformation. They must be capable of participat-ing in managing and evaluating new collaborative efforts. These efforts will benefit HRD scholars and scholar-practitioners and could begin a new field of study within HRD, TSci, and Cross-Disciplinary Research.
Declaration of Conflicting Interests
The author(s) declared no potential conflicts of interest with respect to the research, authorship, 
and/or publication of this article.
Funding
The author(s) received no financial support for the research, authorship, and/or publication of this article.
References
Albert, T., & Wager, E. (2003). How to handle authorship disputes: A guide for new research-
ers: The COPE report. https://doi.org/10.24318/cope.2018.1.1
Bennett, M. L., & Gadlin, H. (2012). Collaboration and team science: From theory to prac-
tice. Journal of Investigative Medicine, 60(5), 768–775. https://doi.org/10.2310/
JIM.0b013e318250871d
Bozeman, B., & Youtie, J. (2017). The strength in numbers: The new science of team science. 
Princeton University Press.
Cordery, J. L., & Tian, A. W. (2017). Team design. In E. Salas, R. Rico & J. Passmore (Eds.), 
The Wiley Blackwell handbook of the psychology of team working and collaborative pro-cesses (pp. 103–128). John Wiley.
DeHart, D. (2017). Team science: A qualitative study of benefits, challenges, and lessons learned. 
The Social Science Journal, 54, 458–467. https://doi.org/10.1016/j.soscij.2017.07.009
Dihn, J. V., & Salas, E. (2017). Factors that influence teamwork. In E. Salas, R. Rico & J. 
Passmore (Eds.), The Wiley Blackwell handbook of the psychology of team working and collaborative processes (pp. 15–41). John Wiley.
Falk-Krzesinski, H. J., Borner, K., Contractor, N. S., Fiore, S. M., Hall, K. L., Keyton, J., 
. . . Uzzi, B. (2010). Advancing the science of team science. Clinical and Translational 
Science, 3, 263–266. https://doi.org/10.1111/j.1752-8062.2010.00223.x",1523422319886300.pdf
"Turner and Baker 85
Fiore, S. M. (2008). Interdisciplinarity as teamwork: How the science of teams can inform team sci-
ence. Small Group Research, 39(3), 251–277. https://doi.org/10.1177/1046496408317797
Fload Expert. (2010). Research Orientation Scale (ROS). https://www.teamsciencetoolkit.can-
cer.gov/public/TSResourceMeasure.aspx?tid=2&rid=35
Gleed, A., & Marchant, D. (2016, May). Interdisciplinarity: Survey report for the Global 
Research Council 2016 annual meeting. https://www.globalresearchcouncil.org/fileadmin/
documents/GRC_Publications/Interdisciplinarity_Report_for_GRC_DJS_Research.pdf
Huang, D.-W. (2015). Temporal evolution of multi-author papers in basic sciences from 1960 to 
2010. Scientrometrics, 105, 2137–2147. https://doi.org/10.1007/s11192-015-1760-x
Katz, S. J., & Martin, B. R. (1997). What is research collaboration? Research Policy, 26, 1–18. 
https://doi.org/10.1016/S0048-7333(96)00917-1
Ledford, H. (2015). How to solve the world’s biggest problems: Interdisciplinarity has become 
all the rage as scientists tackle climate change and other intractable issues. But there is still strong resistance to crossing borders. Nature, 525(7569), 308–311. https://doi.org 
/10.1038/525308a
Mirsa, S., Stokols, D., & Cheng, L. (2015). The transdisciplinary orientation scale: Factor struc-
ture and relation to the integrative quality and scope of scientific publications. Journal of Translational Medicine & Epidemiology, 3(2), 1042.
Mohammed, S., & Dumville, B. C. (2001). Team mental models in a team knowledge frame-
work: Expanding theory and measurement across disciplinary boundaries. Journal of 
Organiztional Behavior, 22, 89–106. https://doi.org/10.1002/job.86
Rahman, M. T., Regenstein, J. M., Kassim, N. L. A., & Haque, N. (2017). The need to quantify 
authors’ relative intellectual contributions in a multi-author paper. Journal of Informatics, 11, 275–281. https://doi.org/10.1016/j.joi.2017.01.002
Rylance, R. (2015). Global funders to focus on interdisciplinarity [Commentary]. Nature, 
525(7569), 313–315. https://doi.org/10.1038/525313a
Stokols, D. (2018). Social ecology in the digital age: Solving complex problems in a globalized 
world. Elsevier.
Turner, J. R., Baker, R., & Morris, M. (2018). Complex adaptive systems: Adapting and man-
aging teams and team conflict. In A. V. Boas (Ed.), Organizational conflict (pp. 65–94). 
INTECH Open Science. https://doi.org/10.5772/intechopen.72344
Ynalvez, M. A., & Shrum, W. M. (2011). Professional networks, scientific collaboration, and 
publication productivity in resource-constrained research institutions in a developing  country. Research Policy, 40, 204–216. https://doi.org/10.1016/j.respol.2010.10.004
Yu, Q., Shao, H., He, P., & Duan, Z. (2013). World scientific collaboration in coronary 
heart disease research. International Journal of Cardiology, 167, 631–639. https://doi.
org/10.1016/j.ijcard.2012.09.134
Zucker, D. (2012). Developing your career in an age of team science. Journal of Investigative 
Medicine, 60, 779–784. https://doi.org/10.231/JIM.0b013e3182508317
Author Biographies
John R. Turner, PhD, is an assistant professor at the University of North Texas for the Department of Learning Technologies in the College of Information. He currently serves as the editor-in-chief for the Performance Improvement Quarterly (PIQ) journal. His research inter-
ests are in team science, team cognition, leadership, performance improvement, knowledge 
management, theory building, complexity theory, multilevel model development, and analysis techniques. He is the co-creator of The Toyota Flow System. He has published a number of book ",1523422319886300.pdf
"86 Advances in Developing Human Resources 22(1)
chapters and research articles in Advances in Developing Human Resources; Human Resource 
Development Review; European Journal of Training & Development; International Journal of Technology, Knowledge, & Society; Journal of Information and Knowledge Management; 
Journal of Manufacturing Technology Management; Journal of Knowledge Management; 
Performance Improvement; Performance Improvement Quarterly; and Systems.
Rose Baker, PhD, is an assistant professor in the Learning Technologies Department in the 
College of Information at the University of North Texas. She researches open learning, manage-ment techniques and statistical applications for operations and performance improvement, the-ory development, survey and evaluation design, and impact and prevention of substance use. 
She holds a PhD in instructional systems from Penn State and is a certified PMP
® by PMI.",1523422319886300.pdf
"Scientific Teams and Scientific Laboratories  
Author(s): Alvin M. Weinberg  
Source: Daedalus , Fall, 1970 , Vol. 99, No. 4, The Making of Modern Science: Biographical 
Studies (Fall, 1970), pp. 1056-1075  
Published by: The MIT Press on behalf of American Academy of Arts & Sciences  
Stable URL: https://www.jstor.org/stable/20023981
JSTOR is a not-for-profit service that helps scholars, researchers, and students discover, use, and build upon a wide 
range of content in a trusted digital archive. We use information technology and tools to increase productivity and 
facilitate new forms of scholarship. For more information about JSTOR, please contact support@jstor.org. 
 
Your use of the JSTOR archive indicates your acceptance of the Terms & Conditions of Use, available at 
https://about.jstor.org/terms
American Academy of Arts & Sciences  and The MIT Press  are collaborating with JSTOR to 
digitize, preserve and extend access to Daedalus
This content downloaded from 
              198.51.92.3 on Mon, 06 Feb 2023 21:21:35 UTC               
All use subject to https://about.jstor.org/terms",20023981.pdf
" ALVIN M. WEINBERG 
 Scientific Teams and Scientific Laboratories1 
 Scientific truths discovered in one age are essential for scientific prog 
 ress in another: the laws of thermodynamics, discovered in the nine 
 teenth century, will remain relevant and necessary for the scientist of 
 the twenty-second century. Similarly scientific truth discovered in one 
 place is required for scientific progress elsewhere: Lord Rutherford's ex 
 periments at Manchester on the scattering of alpha particles led even 
 tually to the prolific investigation of nuclear phenomena throughout the 
 world. To paraphrase Alfred Korzybski, man the scientist is both a time 
 binder and a space-binder. 
 In this sense science has always been a cumulative, team activity, 
 more than, say, the arts or literature.2 To be sure, great individual 
 geniuses, like Newton or Maxwell or Darwin, create the revolutions that 
 punctuate scientific progress. (T. S. Kuhn, in his The Structure of Sci 
 entific Revolutions, calls these turning points in science ""paradigm-break 
 ing.""3 I shall refer to them, along with the more modest ""important dis 
 coveries,"" simply as ""breakthroughs."") Yet the connections of even such 
 individual geniuses with their predecessors and their contemporaries 
 are surely more direct and demonstrable than is the connection between 
 Beethoven and Mozart, or Picasso and Renoir. As Newton wrote to Rob 
 ert Hooke, ""If I have seen further (than you and Descartes) it is by 
 standing upon the shoulders of giants.""4 
 Nineteenth-century science was mainly conducted by geographically 
 isolated, though intellectually interacting, individuals; much of today's 
 science is conducted by large interdisciplinary teams. These teams often 
 center around pieces of expensive equipment and are then said to be part 
 of ""big science."" Team science is characteristically conducted in the 
 large multipurpose scientific laboratory, an institution that is predom 
 inantly a phenomenon of World War II and after. My purpose will be 
 first to trace the origins of big team science and to examine its multipur 
 pose institutions, second to estimate the capacity of this new scientific 
 style to launch and carry off the scientific breakthroughs so necessary 
 1056
This content downloaded from 
              198.51.92.3 on Mon, 06 Feb 2023 21:21:35 UTC               
All use subject to https://about.jstor.org/terms",20023981.pdf
" Scientific Teams and Scientific Laboratories 
 for the progress of science, and finally to speculate on the future of team  research and its institutions. 
 I. The Origins of Big Team Science 
 The emergence of the large interdisciplinary scientific team as the 
 landmark of science can be traced to at least three separate develop 
 ments. First is the extraordinary growth of science and the resulting in 
 crease in the amount of scientific information produced; second is the 
 emergence and institutionalization of applied science; third, and pos 
 sibly most important, is the increasing complexity of scientific machinery. 
 A. The Information Crisis and the Rise of Team Science 
 The scientific information explosion has caused scientists to become 
 more specialized. Some scientists respond to the information crisis by 
 confining their range of scientific undertakings to those over which they  can still retain command of the relevant information sources. Others 
 form interdisciplinary teams in which are represented different though 
 overlapping ranges of expertise or technique. In principle, the problems 
 that can be tackled successfully by such teams ought to be more com 
 plex than those tackled by individuals. 
 This trend in the sociology of science was foreshadowed in an essay, 
 ""The Limits of Science,"" by Eugene Wigner in 1950.5 Wigner argued 
 that, for the reason I have mentioned, team research in which individual 
 scientists are orchestrated into a productive whole by a scientific leader  would become more common. He then asked how this new social struc 
 ture would change the course of science. Could the theory of relativity 
 or the Schrodinger equation have been discovered by an interdisciplinary 
 team? Or, for that matter, could the mysteries of the ""omega-minus"" 
 particle and violation of charge-parity invariance (both discoveries of 
 teams of high-energy physicists ) have been unearthed by the typical in 
 dividual scientist of the nineteenth century? I shall return to these ques 
 tions later. 
 The information explosion has been the subject of many essays and 
 studies. Here I will mention only how the spawning of the scientific in 
 formation specialist has affected the organization of scientific research. 
 In previous generations the scientist gathered his information more or 
 less on his own and rather haphazardly. Today scientists of course con 
 tinue to browse in this manner, but they are now backed by a host of 
 information services, ranging from libraries to abstract services and 
 specialized information centers. 
 Already one can see the considerable influence of the information 
 1057
This content downloaded from 
              198.51.92.3 on Mon, 06 Feb 2023 21:21:35 UTC               
All use subject to https://about.jstor.org/terms",20023981.pdf
" D/EDALUS 
 specialists on those fields of science such as nuclear physics and high 
 energy physics where the spectroscopy0 has become so elaborate as to 
 outrun any single individual's capacity to hold all the relevant data in 
 his mind. As a result, much of the output of the nuclear or high-energy 
 spectroscopist goes to a secondary source?such as K. Way's or A. H. 
 Rosenfeld's centers?where the data are compiled and collated. But in 
 the process the role of the individual scientist who first made the mea 
 surements is weakened; the citation now often tends to be to the sec 
 ondary source rather than to the original experimenter. Could this mean 
 that one of the delicious joys and motivations of science?recognition 
 and approbation by one's peers?will be attenuated? Parts of basic sci 
 ence have already acquired some of the facelessness that characterizes 
 applied science, and this trend, it seems to me, will continue as the in 
 formation crisis deepens.7 
 B. The Emergence of Applied Science and 
 the Large Industrial Laboratory 
 A second source of the trend toward team science is the rise of ap 
 plied science and particularly the growth of the large industrial labora 
 tory. Here the interdisciplinary team has predominated from the first, 
 for reasons that are implicit in the strategy of science?that is, the way 
 that scientists choose what they do. To make this point clearer, I shall 
 digress to consider the strategy of scientific research. 
 Science is the ""art of the soluble"" according to Peter Medawar.8 What 
 a scientist does is largely determined by what he thinks he can do suc 
 cessfully. According to this view, science is a meandering stream that 
 pushes salients out wherever the bank is weak and can be conquered; 
 that such meandering may lengthen and make more tortuous the path 
 to the sea is somewhat irrelevant. The river valley ( to push the metaphor ) 
 is irrigated more heavily, and becomes greener, as a consequence of the 
 meandering. 
 Insofar as Medawar is referring to basic science, his view of science  as the art of the soluble contains much truth. In basic science, the sci 
 entist's criteria for deciding what he ought to try are usually internally 
 generated; that is, they derive from the internal logic of the sp?cific field 
 in which he works and from his assessment of how soluble the problem 
 is. Moreover, in basic science success is achieved if one solves the prob 
 lem he sets out to solve, if he solves a different problem, or even if he 
 can show that a particular approach is unfruitful. For all these reasons, 
 in basic research it is acceptable to tailor problems to one's capacity for 
 solving them. An expert in nuclear magnetic resonance can confine his 
 researches to that segment of the field of nuclear magnetic resonance 
 1058
This content downloaded from 
              198.51.92.3 on Mon, 06 Feb 2023 21:21:35 UTC               
All use subject to https://about.jstor.org/terms",20023981.pdf
" Scientific Teams and Scientific Laboratories 
 over which he can comfortably retain command. Thus basic science ( at 
 least before the advent of the big machine) with its internally generated 
 problems, can be pursued adequately within a narrow discipline. If the 
 problem takes the researcher out of his specialty, he is still observing the 
 canons of pure science if he turns to a different problem that is more 
 easily accommodated by his interest and competence. It is for this rea 
 son that much of basic science can remain disciplinary and little: the 
 interdisciplinary team is not its social characteristic. 
 Of course, even as a description of basic little science this is over 
 simplified; to characterize science as the art of the soluble tells only part 
 of the story. Basic science, at its best, is the art of the soluble and the 
 important (as Medawar himself recognizes). Researchers, even poor 
 ones, usually have more ideas than they have resources with which to 
 pursue them; their research strategy always amounts to choosing, from 
 among a variety of soluble problems, the ones they regard as important. 
 What constitutes importance in science? One, though certainly not the 
 only, criterion is the degree to which a given piece of science relates to 
 neighboring sciences. Indeed, the motivation for much basic scientific 
 activity originates outside that activity. Sometimes the motivation lies 
 in a neighboring basic science. For instance, a nuclear physicist may 
 study light-element reactions because these are needed by an astro 
 physicist who wants to understand the mechanism of stellar evolution. 
 Sometimes the motivation lies in technology: a physicist may investigate 
 the basic properties of plasmas because of their relevance to the con 
 trolled release of thermonuclear energy. But the main point is that as 
 soon as a scientist ventures to deal with a question arising in a field 
 outside his discipline he has less control over where to look for a solu 
 tion. He no longer has the luxury of narrowing the problem to what is 
 soluble with his own expertise. Externally motivated science tends to 
 be interdisciplinary and therefore more of a team activity than internally  motivated science. 
 Applied science is externally motivated par excellence. Its questions 
 are posed from without: from engineering, military, and even social 
 demands. Such questions usually transcend the individual disciplines. 
 The criterion of success in applied science is simply, ""Does it work?"" not 
 ""Does it add to knowledge in a particular discipline?"" Thus applied sci 
 ence is characteristically interdisciplinary; it lends itself to?in fact it 
 almost requires?teams of interacting individuals, none of whom by him 
 self commands all the knowledge necessary to make progress, but all of 
 whom, when taken as a whole, hopefully do. 
 It is therefore no accident that the great institutions of applied 
 science in industry and in government are typically homes for inter 
 disciplinary teams. The jobs of these institutions are set outside the 
 1059
This content downloaded from 
              198.51.92.3 on Mon, 06 Feb 2023 21:21:35 UTC               
All use subject to https://about.jstor.org/terms",20023981.pdf
" D^DALUS 
 disciplines, even outside science; in consequence their style is interdis 
 ciplinary. Though the first of these laboratories, such as the General 
 Electric Research Laboratory, Bell Telephone Laboratories, and the 
 National Bureau of Standards, appeared around the turn of this century, 
 there was an enormous development of them during and after World  War II. The best known of the wartime laboratories were the Radiation 
 Laboratory at the Massachusetts Institute of Technology, which devel 
 oped radar, and the Metallurgical and Los Alamos laboratories of the 
 Manhattan Project, which developed the atomic bomb. My own experi 
 ence has been almost entirely confined to the atomic energy laboratories, 
 and so I shall draw largely on them to illustrate some characteristics of 
 the big applied scientific institutions. 
 From its very beginning in late 1941, the Chicago Metallurgical 
 Laboratory, at which the first fission chain reaction was established, was 
 interdisciplinary. Arthur H. Compton, the director of the Metallurgical 
 Project, realized that the technology of the chain reactor would require 
 physicists, mathematicians, chemists, instrument experts, metallurgists, 
 biologists, and the various engineers who could translate these scientists' 
 findings into practice. The chain reactor was much more than a nuclear 
 physicist's experiment. Uranium to fuel the first reactor had to be purified 
 and reduced to metal. Graphite of unprecedented purity was needed to 
 moderate the neutrons. The chemistry of the new element plutonium 
 was largely unknown. The production of plutonium was very hazardous, 
 and the most sophisticated instruments were needed to keep everything 
 under control. The biological effects of the radiation that would be re 
 leased had to be assessed if not mitigated. 
 The difference between most interdisciplinary engineering enterprises 
 and the engineering at the Metallurgical Laboratory lay in the incredible 
 speed with which the latest scientific findings at the Laboratory were 
 converted into engineered chain reactors. Eugene P. Wigner, who headed 
 the theoretical physics group, began to engineer the water-cooled Han 
 ford reactors in early 1942, almost ten months before the first chain re  action had been established. 
 There was nothing very complicated or obscure about the function 
 and purpose of the Metallurgical Laboratory. Its output was a specific 
 gadget and a specific process: the nuclear chain reactor and the produc 
 tion and extraction of plutonium. If the reactor succeeded, the Labora 
 tory succeeded; if it failed, the Laboratory failed. Because of this single 
 ness of purpose, which at least for the first two years was evident to all, 
 there was remarkably little difficulty in forging the teams necessary to get 
 on with the job. 
 Like most institutions of this sort, the Laboratory was organized into 
 divisions. Enrico Fermi and Eugene Wigner were in charge of the 
 physicists; James Frank and then Sam Allison and Farrington Daniels, 
 1060
This content downloaded from 
              198.51.92.3 on Mon, 06 Feb 2023 21:21:35 UTC               
All use subject to https://about.jstor.org/terms",20023981.pdf
" Scientific Teams and Scientific Laboratories 
 in charge of the chemists; Charles Cooper, the engineers; and so on. But 
 the over-all project overwhelmed the disciplinary divisions. This was 
 relatively easy because everyone knew the stakes; one could readily 
 submerge his personal aspirations for the sake of achieving the whole 
 objective. This is not to say that there was no tension between the proj 
 ect and the divisions ( that is, the disciplines ). Even in the dark days of 
 1943 one could find physicists at the Metallurgical Laboratory working 
 on the spherical harmonic method of solution of the Boltzmann equation 
 ( an activity that at the time seemed like an unjustified luxury ) instead 
 of estimating more routinely the multiplication constant of the latest 
 reactor design. 
 This criss-cross organization?with each scientist having a permanent 
 home in a division but being lent out temporarily to an interdisciplinary 
 project?is the usual organization in applied laboratories. The project 
 leaders generally control the funds; the division leaders, the people. 
 The projects maintain pressure on the division managers t? keep their 
 outlook and activities relevant as judged by the projects; the disciplinary 
 divisions maintain pressure on the project managers to keep their ac 
 tivities up to the standards of sophistication imposed by the divisions. 
 It is hoped that out of this criss-cross tension between project and di 
 vision there will come both relevance and sophistication. 
 The Metallurgical Laboratory was hierarchical. Arthur Compton was 
 boss, but there were many other managers at lower levels, each on top 
 of a pyramid of lesser and usually younger scientists. This pyramidal 
 structure gave very great power to the man on top: he could command 
 information resources; he could order investigations in many directions 
 that would be out of the question had he not had a team at his disposal.  In such hierarchical scientific teams, the members lower down must sub 
 merge their personalities and to some extent their scientific instincts to 
 those of the boss. One therefore finds genius in such organizations less  often than in the universities, where science is conducted more individ 
 ually. On the other hand, a really good man in a position at the top of a 
 pyramid obviously can get much more done than he can if he works in 
 the usual university setting. Glenn Seaborg at the Metallurgical Labora 
 tory had about thirty chemists working for him, and in only two years his 
 group elucidated much of the chemistry of plutonium in addition to 
 developing a process for extracting plutonium that was used successfully 
 at Hanford! 
 Though there were many scintillating talents around?Szilard and 
 Fermi and Wigner and Seaborg?the decisions were finally made by 
 Compton. Yet, as in any organization, those with enough energy, con 
 fidence, and ability could impose their views in the face of official rejec 
 tion. At the Metallurgical Laboratory, a showdown of this sort occurred 
 during 1942. The issue was the coolant?and therefore the whole en 
 1061
This content downloaded from 
              198.51.92.3 on Mon, 06 Feb 2023 21:21:35 UTC               
All use subject to https://about.jstor.org/terms",20023981.pdf
" DAEDALUS 
 gineering design?of the Hanford plutonium-producing reactors. The 
 prevailing view held that since helium absorbed no neutrons, helium 
 should be used to cool the reactors. With this view Wigner disagreed 
 vigorously; he wanted to cool the reactors with water. To him, the 
 handling of hot and somewhat radioactive helium under pressure seemed 
 much more serious than the loss of nuclear performance caused by the 
 tendency of hydrogen to absorb neutrons. In arguing his case, Wigner 
 commanded all the relevant elements of knowledge?the engineering, 
 the chemistry, the metallurgy, and the physics. And, when the Hanford 
 reactors were actually to be built, the DuPont engineers chose the water 
 cooled pile rather than the original helium-cooled version. 
 Writing about these events twenty-seven years after they occurred, 
 I am struck not by their uniqueness but by their generality. The Metal 
 lurgical Laboratory, in Anthony Downs's terminology,9 was a bureaucracy 
 ?that is, a large organization that is not governed, or is only indirectly 
 governed, by the feedback from the marketplace. In this sense almost 
 every large laboratory, even if it is part of a big corporation, is a bureauc 
 racy; its connection with the marketplace is usually tenuous. Many of 
 the organizational features and sources of power in the large laboratory 
 are not characteristic specifically of a scientific establishment but rather 
 of any large nonmarket establishment. The hierarchical structure, the 
 possibility of the energetic individual prevailing against the official 
 position, the great logistic power of a big laboratory,10 above all, the 
 urgent imperative of the various groups to survive and to expand? 
 all these are obvious to students of large organizations, scientific or 
 otherwise. 
 C. The Influence of the Big Scientific Machine 
 The third thread in the development of big team science goes back 
 for some of its spirit to the explorations of the fifteenth and sixteenth 
 centuries. To a degree, we would have to regard the great explorers as 
 geographers, and hence scientists of sorts. Their enterprises were on a 
 grand scale by the standards of their time; they required large teams 
 and much money. And at least Columbus among them politicked with 
 John of Portugal and Queen Isabella in much the same way that a 
 promoter of a large accelerator must now politick with the Atomic En 
 ergy Commission or the National Science Foundation, or even with the 
 President himself, to sell his project. 
 Many of today's explorations in basic science involve such elaborate 
 and expensive pieces of hardware that the whole enterprise requires 
 much the same mobilization of resources as was required by the ex 
 plorers. The most extreme example today of huge mobilization of re 
 1062
This content downloaded from 
              198.51.92.3 on Mon, 06 Feb 2023 21:21:35 UTC               
All use subject to https://about.jstor.org/terms",20023981.pdf
" Scientific Teams and Scientific Laboratories 
 sources for a purpose that is at least partly scientific is the exploration 
 of space. And even before we began to use rockets, earth-based astron 
 omy had some of the attributes of modern big science: the 200-inch Hale 
 telescope at Mount Palomar, completed in 1948, was one of the largest 
 and most expensive pieces of scientific machinery until the advent of the 
 large research reactors and large accelerators during and after the war. 
 The new style of big science based on very large pieces of equipment 
 is generally attributed to Ernest O. Lawrence. His 37-inch cyclotron at 
 Berkeley was a monster for its time; this was followed by the 60-inch, 
 the 184-inch, the synchrocyclotron, the proton synchrotron (Bevatron), 
 in ever-increasing size and complexity. To be sure, there had been 
 earlier scientific teams dominated by great leaders: J. J. Thomson and 
 later Rutherford at the Cavendish Laboratory, Fermi and his neutron 
 group in Rome, and, of course, the German institutes. But Lawrence's 
 laboratory was probably the first in which the central piece of equip 
 ment was so elaborate, and possibly so temperamental, as to require a 
 more or less full-time engineering staff. The logistics of keeping the place 
 going?whether this means the scientific machinery or the elaborate 
 organization that tends the machinery?becomes an essential ingredient 
 of the activity. There are engineers and instrument technicians and 
 financial people and personnel experts, many of whom identify rather 
 little with the purpose of the entire laboratory, but each of whom is 
 valued for his specialized expertise. 
 Thus the modern home of big basic science, especially the big ac 
 celerator or reactor laboratory, acquires much of the flavor of the in 
 dustrial laboratory. The time allotted for use of the machine is rigidly 
 scheduled, and this imposes a regularity on the working habits at least  of the technicians who tend the machine. There is a division of labor 
 between those who are expert in electronics and computing and electrical 
 engineering; and this requires coordination. The necessity for explicit 
 planning is taken for granted, in much the same way as planning by a 
 project manager is the accepted way of doing business in the applied 
 laboratory. 
 The typical home of massive basic science, like CERN in Geneva or 
 the Stanford Linear Accelerator, is however more specialized than is 
 the modern home of applied science; this goes back to the aforemen 
 tioned distinction between basic science, which tends to be internally 
 motivated and disciplinary, and applied science, which tends to be ex 
 ternally motivated and interdisciplinary. The General Electric Research 
 Laboratory covers a wider range of specialties than does the Stanford 
 Linear Accelerator. The Argonne National Laboratory, with its experts 
 ranging from biom?dical researchers and ecologists to high-energy physi 
 cists, covers a wider range of specialties than does the nearby Fermi 
 1063
This content downloaded from 
              198.51.92.3 on Mon, 06 Feb 2023 21:21:35 UTC               
All use subject to https://about.jstor.org/terms",20023981.pdf
" D^SDALUS 
 National Accelerator Laboratory. I imagine that this greater specializa 
 tion will in the long run pose some difficulties if the question of redeploy 
 ing the large basic laboratories, like Fermi or SLAC, should ever arise. 
 II. Individual Science and Team Science: 
 Breakthroughs Versus Spectroscopy 
 A. The Xenon Compounds: A Breakthrough by an Individual 
 In 1962 Neil Bartlett, a young chemist at the University of British 
 Columbia, stumbled onto the fact that oxygen could be oxidized by 
 platinum hexafluoride. About the same time Bartlett had noticed, while 
 browsing through a table of ionization potentials, that the energy re 
 quired to strip an electron off xenon (to form Xe+) was about the same 
 as that required to form the 0./ ion. He therefore concluded it was worth 
 trying to oxidize xenon with PtF6. Almost on his first try he was success 
 ful, and, in 1962, sixty-eight years of chemical dogma came to an end: 
 the first stable compound of a noble gas, Xe(PtF6), was produced. The 
 noble gases were no longer noble.11 
 Immediately after Bartlett's discovery, a group of chemists at the 
 Argonne National Laboratory plunged into the new chemistry of the 
 noble gases. They came to this task well prepared: for many years they 
 had been interested in the chemistry of the fluorine compounds of 
 plutonium and other transuranics. Their laboratories were well equipped 
 for handling treacherous, extremely toxic materials like elemental fluorine 
 and PuF6. Almost immediately H. H. Claassen, J. G. Malm, and H. Selig 
 discovered that xenon could be oxidized by fluorine alone, and within 
 a year of frenzied activity many compounds of xenon and other noble 
 gases were prepared and characterized. A blank page in inorganic chem 
 istry had been expanded into a good-sized, well-filled book.12 
 This incident serves to illustrate, in almost too perfect outline, the 
 usually held stereotypes as to the strengths and the weaknesses of the 
 traditional individual and the newer team styles of research. The brilliant 
 initial stroke?Bartlett's crazy idea that xenon could be oxidized if only 
 one chose a sufficiently strong oxidant?was very much the doing of an 
 individual. Not that this idea was absolutely new: in 1933 Linus Pauling 
 had suggested that stable xenon compounds exist, and D. M. Yost even 
 tried, unsuccessfully, to prepare them at California. And, even closer to 
 home, at Oak Ridge S. S. Kirslis, F. H. Blankenship, and W. R. Grimes, 
 who were developing a reactor that was fueled with molten uranium 
 fluoride, had noticed that the fission product xenon consistently was miss 
 ing, whereas the fission product krypton was always present as expected 
 in the gas phase. The question of whether the xenon could be disappear 
 ing as a chemical compound did arise and was discussed but of course 
 1064
This content downloaded from 
              198.51.92.3 on Mon, 06 Feb 2023 21:21:35 UTC               
All use subject to https://about.jstor.org/terms",20023981.pdf
" Scientific Teams and Scientific Laboratories 
 was rejected, although, as it turned out, XeF4 was being produced. Sci 
 entific dogmas of such strength as the nobility of the rare gases are hard  to dethrone. 
 But, once the brilliant, individual breakthrough had been made, the 
 integrated team and the great logistic power of the National Laboratory 
 moved in, massively and professionally, to fill in the spectroscopic de  tails. A field that in earlier times would have remained fertile and excit 
 ing for a decade or more was largely elucidated in little more than a 
 year's time. 
 In attacking the xenon compounds so massively, the Argonne Na 
 tional Laboratory was working very much in the style of the applied 
 laboratory. There was a group leader with a staff of highly professional 
 people, each of whom was an expert. Bartlett with graduate students 
 probably would have been no match for Argonne with its professionals. 
 And indeed, this extraordinary elaboration of a field of chemistry in 
 just a year has led some to suggest that at least in the field of chemistry 
 the future belongs to the professional team supported with superb 
 equipment and unencumbered by teaching commitments, rather than to 
 the professor whose professionalism in research is diluted by teaching.13 
 This view has been sharply criticized by representatives of the univer 
 sity scientific community who insist that only individuals can achieve 
 breakthroughs. 
 In point of fact, the team can and has achieved breakthroughs, and 
 it is by no means clear that the team will snuff out the fire of scientific 
 revolution. In the table below, as an example, I list the Nobel Prizes 
 in physics during the past twenty years, the time during which team 
 physics has grown so markedly. Of course not every discovery that wins 
 a Nobel Prize breaks a paradigm, but I believe most physicists will agree 
 that these discoveries at the very least represent important breakthroughs. 
 Though the individual winners exceed the team winners, the fact remains 
 that team science has produced several Nobel Prizes in physics. Indeed, 
 examples of teams achieving breakthroughs are not hard to find. I shall 
 describe one that occurred in Oak Ridge in the past few years. 
 B. Anomalous Losses in Channeling: A Breakthrough by a Team 
 Charged particles in traversing crystals often become trapped in chan 
 nels formed by rows of regularly spaced atoms. This phenomenon, called 
 channeling, was predicted theoretically by Mark T. Robinson in 1962, 
 and then was discovered experimentally. In 1964 a team at Oak Ridge, 
 consisting of several nuclear physicists, solid state physicists, and a 
 physical chemist, examined the energy loss of the channeled particles 
 as they emerged from thin crystalline gold foils. They were astonished 
 1065
This content downloaded from 
              198.51.92.3 on Mon, 06 Feb 2023 21:21:35 UTC               
All use subject to https://about.jstor.org/terms",20023981.pdf
" DJEDAIAJS 
 Nobel Prizes in physics, 1948-1968. 
 Year
 Winner 
 Discovery  Team Individual 
 1948 P. M. S. Blackett 
 1949 H. Yukawa 
 1950 C. F. Powell 
 1951 Sir J. D. Cockcroft 
 E. T. S. Walton 
 1952 F. Bloch  E. M. Purcell 
 1953 F. Zernike 
 1954 M. Born  W. Bothe 
 1955 P. Kusch  W. E. Lamb 
 1956 W. Shockley  W. H. Brattain 
 J. Bardeen 
 1957 C. N. Yang  T. D. Lee 
 1958 P. A. Cerenkov 
 I. Y. Tamm  I. M. Frank 
 1959 E. G. Segr?  O. Chamberlain 
 1960 D. A. Glaser 
 1961 R. Hofstadter  R. L. M?ssbauer 
 1962 L. D. Landau 
 1963 E. P. Wigner 
 Maria Goeppert-Mayer  J. H. D. Jensen  1964 C. H. Townes  N. Basov 
 A. Prokhorov 
 1965 R. P. Feynman  Julian S. Schwinger  S. Tomonaga  1966 A. Kastler 
 1967 H. Bethe 
 1968 L. W. Alvarez  Development of the Wilson 
 method and discovery by this  method of the r- and /x-mesons 
 Prediction of mesons 
 Development of the photographic 
 method of the study of nuclear  processes and discovery concern 
 ing mesons  Cockcroft-Wal ton accelerator 
 and first disintegration 
 Nuclear magnetic resonance 
 Phase-contrast microscope 
 Quantum mechanics; coincidence 
 method 
 Lamb shift; anomalous magnetic  moment of electron 
 Transistor 
 Nonconservation of parity 
 Cerenkov effect 
 Antiproton 
 Bubble chamber 
 Electron nucle?n and nuclear 
 interaction; M?ssbauer effect 
 Liquid helium, etc. 
 Shell theory; symmetry in physics 
 Maser and laser 
 Quantum electrodynamics 
 Optical pumping 
 Nuclear (astro) physics  Giant bubble chamber and  resonances obtained with it 
 * Strongly influenced by the wartime teamwork on radar.  ** Small team of students. 
 1066
This content downloaded from 
              198.51.92.3 on Mon, 06 Feb 2023 21:21:35 UTC               
All use subject to https://about.jstor.org/terms",20023981.pdf
" Scientific Teams and Scientific Laboratories 
 to find that the particles lost their energy in discrete jumps: the amount 
 of energy a particle lost depended very sensitively on the angle with 
 the channel axis at which the particle entered the channel. From this 
 quite accidental discovery came a completely new and possibly quite 
 powerful method of probing the details of the interatomic potential in 
 certain crystals. 
 I tell this story because it illustrates the unique power of an interdis 
 ciplinary attack. Here is an instance in which the whole team is much 
 more than the sum of its separate components, where a team as opposed 
 to an individual (as in the case of Bartlett and xenon compounds) 
 achieves a breakthrough. First, the experiments required very thin, per 
 fect gold crystalline foils; these happened to be the specialty of T. S. 
 Noggle, a metallurgist and electron microscopist. Next the 50-MeV 
 (million electron volts) iodine ions had to be accelerated, and their 
 energies after degradation had to be measured with precision; this re 
 quired experts on Van de Graaff accelerators and particularly on so 
 phisticated time-of-flight techniques. Once the phenomenon was dis 
 covered, its full significance required the insight of a young solid state 
 theorist, H. Lutz, as well as a variety of additional experiments that 
 served to corroborate the theoretical predictions. And the team required 
 orchestration: this was supplied by S. Datz, a chemist who had been 
 concerned with the related phenomenon of sputtering. 
 To be sure, elaborate equipment was needed?a time-of-flight Van 
 de Graaff machine. Nowadays this is not so unusual; there are perhaps 
 two dozen laboratories which possess such instruments. But the num 
 ber having at the same time an electron microscopist who can make 
 perfect gold crystals a few hundred angstroms thick, an expert on 
 sputtering, and a solid state theorist capable of interpreting the experi 
 ments is much smaller. It was very much more the style of research? 
 the willingness of all parties to collaborate fully?that led to the break 
 through. This willingness among professionals to collaborate is actually 
 not to be taken for granted, especially in the academic world. In reading 
 James Watsons The Double Helix14 one is constantly aware of the 
 barriers that were placed between Watson and Francis Crick (who had 
 the major idea about the structure of RNA) and Rosalind Franklin 
 and Maurice Wilkins ( who had the means for making the measurements 
 needed). I suppose it is for reasons such as this that I am convinced the 
 success of team science depends on the institution. There must be a 
 tradition of interdisciplinary collaboration between professionals. This 
 is more likely to exist in an institute with hierarchical organization? 
 such as one finds in the applied or project laboratories?than in the 
 typical university. 
 The example I have given would only marginally qualify as big 
 1067
This content downloaded from 
              198.51.92.3 on Mon, 06 Feb 2023 21:21:35 UTC               
All use subject to https://about.jstor.org/terms",20023981.pdf
" D^DALUS 
 science: the machines, though large, are not all that large. If one ex 
 amines the more typical endeavors of big science, particularly those that 
 require unique accelerators or unique reactors, one finds many examples 
 of breakthroughs by teams. One of the most recent is the finding by 
 Val Fitch and his collaborators (many of whom were students) of the 
 nonconservation of charge parity in the decay of the K-meson or, per 
 haps even more uniquely tied to the capacity of a single machine, the 
 discovery by Segr? of the antiproton, at the Lawrence Radiation Lab 
 oratory in Berkeley. In these cases I would argue that it was the ma 
 chine properly used and good leadership more than the team. The 
 team was needed mainly because the experiment was so complex. In a 
 certain sense, the team tends to be incidental to the machine in very 
 big science; by contrast, the team is central in those cases?such as the 
 work on DNA and channeling?in which the means are more modest. 
 Here what is important is a delicate balancing and interweaving of 
 individual expertise. 
 So we see that teams can achieve breakthroughs, operating either 
 in the interdisciplinary mode or in the big science mode. Yet the power 
 of the team seems to me to lie primarily in its ability to do spectroscopy; 
 as team science becomes more and more common, so might the emphasis 
 on the spectroscopic style of science. This trend may be accentuated by 
 the weightiness and inertia of modern big science. Where scientific 
 teams have mobilized around very big pieces of machinery there is an 
 understandable incentive to exploit that machine. The path of de 
 velopment, instead of following the logical demands of the discipline, 
 tends to be constrained to directions that are made accessible by the 
 machinery at hand. 
 Something like this has always happened in science: one exploits 
 whatever tools one has available. But scientists are naturally much less 
 ready to scrap a 400-MeV proton-synchrocyclotron that costs several mil 
 lion dollars, but which no longer can cut at the main edge of high 
 energy physics, than they are to scrap, say, an optical microscope. The 
 somewhat bureaucratic imperative to exploit expensive machinery cir 
 cumscribes the direction of scientific growth. The spectroscopic filling 
 in of details tends to crowd out the breakthroughs, simply because the 
 number of breakthroughs possible with a particular machine is very 
 small compared with the practically infinite spectroscopic detail the 
 machine can generate. 
 It would be foolish to underestimate the importance of spectroscopy 
 in setting the groundwork for important discoveries and conceptual 
 breakthroughs. Quantum mechanics would have been impossible with 
 out its underlying detailed optical spectroscopy. Or, more recently, low 
 energy physics has a strongly spectroscopic flavor. Most experiments 
 1068
This content downloaded from 
              198.51.92.3 on Mon, 06 Feb 2023 21:21:35 UTC               
All use subject to https://about.jstor.org/terms",20023981.pdf
" Scientific Teams and Scientific Laboratories 
 seek to measure, in various nuclides, specific properties that already 
 fit into a general theoretical framework. Yet out of this elaborate spec 
 troscopy (conducted, incidentally, by teams) has come a seemingly 
 endless succession of breakthroughs: either in experimental techniques, 
 as in the discovery of the lithium-drifted germanium-detector, or in 
 new insights into nuclear structure, as in the discovery of isobaric 
 analogue states and short-lived isomers. 
 There is another side to the story which deserves mention. The team, 
 especially around the big machine, is a powerful scientific device. More 
 difficult experiments can be tried with a large team equipped with a 
 unique facility than with a smaller outfit not so equipped. Thus, insofar 
 as breakthroughs flow from difficult experiments, one might expect 
 teams working with powerful and unique apparatus to continue to con 
 tribute their share of important discoveries. For example, as soon as the 
 high-flux isotope reactor became available, questions in the phonon dis 
 tribution in solids that had plagued solid state physicists became an 
 swerable. 
 To make important breakthroughs in science will always require 
 competent, imaginative leadership. But it seems reasonable to expect 
 that the degree of insight required to make such discoveries may be 
 somewhat less than it was in the day of individual science: the team, 
 or the big machine, may offer elements of uniqueness that were formerly 
 supplied by sheer intellectual power. And, since competence is so much 
 more common than genius, the team may be spreading the possibility 
 of significant scientific discovery to many more scientists than in former 
 days. Perhaps this democratization will prove to be one of the main 
 by-products of big team science. 
 III. The Future of Team Research 
 A. The Institutional Setting 
 It seems clear to me that team science in the modern style is done 
 better in the hierarchical, logistically strong institute than it is in the 
 university. This, coupled with the unrest that wracks the university, 
 suggests that we might see a gradual movement of modern science 
 away from the university and toward the national institute?possibly 
 even a growing separation between education and research. To most 
 writers on this subject, especially since the Seaborg Report,15 the notion 
 that research and education are inseparable and indissoluble, that 
 the one cannot be done without the other, has acquired the ring of holy 
 dogma. But the facts do not really bear this out: certainly insofar as 
 one is elaborating a certain area, such as the chemistry of xenon, pro 
 fessionals are better than students. For many years applied chemistry 
 1069
This content downloaded from 
              198.51.92.3 on Mon, 06 Feb 2023 21:21:35 UTC               
All use subject to https://about.jstor.org/terms",20023981.pdf
" D^DALUS 
 has been conducted to great advantage in the industrial laboratory 
 without benefit of students. I know that at Oak Ridge some (though 
 not all) of our division directors are convinced that they achieve results 
 more quickly and more reliably with professionals than with students. 
 The universities have responded to the trend toward team research 
 by setting up institutes, interdisciplinary and logistically strong, where 
 team research can be performed effectively, but largely by students. 
 But on the average these institutes suffer from a mismatch between the 
 social ethos of the university and the social ethos of the institute: the 
 one is individual and democratic, the other collective and hierarchical. 
 When the institute acquires a collective and hierarchical character, 
 which I believe is necessary for its success, its tie with the university 
 department becomes more tenuous. 
 So we may be going full circle. Science in the seventeenth and 
 eighteenth centuries was practiced predominantly in the academies, 
 not the universities. It moved into the German and English universities 
 in the nineteenth century. And perhaps, with the growth of the large 
 team, it may gradually be moving out again, or at least it may not retain 
 as intimate a connection with the university as it has had in the im 
 mediate past. 
 B. New Fields for Team Research: The Rise of Big Biology 
 The big interdisciplinary team has generally been confined to the 
 physical sciences and to engineering. The biological sciences have re 
 mained the bastion of little, individualistic science, probably because 
 the experimental tools needed to conduct biological experiments have 
 typically been small and relatively inexpensive. Yet there are now im 
 portant trends toward large interdisciplinary teams in the biom?dical 
 sciences and, very recently, teams that include engineers as well as 
 physical scientists. 
 Part of this trend comes from the rise of molecular biology. In one 
 sense the most important parts of molecular biology are really a branch 
 of crystallography; the double helix model for DNA, for example, is 
 based on a crystal structure deduced from X-ray diffraction data. Wat 
 son in his book bemoans his lack of expertise in crystallography, a lack 
 which was made up for him by Crick and by Wiltons' group. It is 
 highly significant that of the two men who made the most important 
 discovery of modern biology, one (Crick) was originally a physicist. 
 Again, the extraordinary elucidation of the working of peripheral nerve, 
 for which A. L. Hodgkin and A. F. Huxley received the Nobel Prize, 
 would have been impossible had it not been for the underlying work on 
 electrical properties of nerves by the physicist K. S. Cole. Many biolo 
 1070
This content downloaded from 
              198.51.92.3 on Mon, 06 Feb 2023 21:21:35 UTC               
All use subject to https://about.jstor.org/terms",20023981.pdf
" Scientific Teams and Scientific Laboratories 
 gists, especially in the most active fields of biochemistry, feel it neces 
 sary to rub shoulders with physicists and with physical chemists.  The second trend discernible in biom?dical science is the rise of the 
 very large-scale experiment. With our present concern with low-level 
 insults to the biosphere (radiation, pesticides, smog), it becomes neces 
 sary to conduct animal experiments on a scale far greater than had 
 hitherto been customary in biology. The husband and wife team of 
 William B. and Liane Russell at Oak Ridge maintains more than 
 100,000 mice in order to study the mutagenic effect of moderate levels 
 of radiation. Such biological experimentation, immediately becomes a 
 team activity: geneticists to manage the entire experiment; statisticians 
 to scan the data for significance; veterinarians to manage the animals; 
 pathologists to look for somatic effects; and, of course, the whole array 
 of animal attendants, janitors, and cage-washers who are needed to keep 
 100,000 animals alive and thriving. 
 I would expect this trend toward very large-scale animal experimenta 
 tion to become increasingly prevalent as we become more sensitive to 
 the widespread influence of seemingly subtle factors in our environment. 
 A call for such experimentation has been made by Ren? Dubos, for 
 example. Should this call be answered by the funding agencies, we may 
 expect to see an increasing fraction of biology being conducted in the 
 style of big science. 
 Biologists, particularly biochemists, are beginning to learn how to 
 employ engineers and other supporting scientists, notably analytical 
 chemists. This represents a new trend since biological institutes tradi 
 tionally have not crossed deeply into the physical sciences, even less 
 into engineering. True, the National Institutes of Health is an enor 
 mously large complex, but NIH has not had within it a strong tradition 
 in the physical sciences or in process engineering. By contrast, the 
 atomic energy laboratories have from the beginning spanned the bio 
 logical sciences, the physical sciences, and the engineering sciences. 
 This unusual juxtaposition has now begun to pay off?for example, in 
 the brilliant development of zonal centrifuges under the leadership of 
 Norman Anderson at the Oak Ridge complex. These centrifuges, which 
 were first developed to separate uranium isotopes, have been modified 
 by Anderson, together with a large team of engineers, to handle biolog 
 ical materials. The centrifuges are now being used very widely to 
 separate, on a large scale, various cell moieties; for instance, they have 
 been used to purify flu vaccine of its antigenic protein impurities. 
 Anderson is now exploiting in his Molecular Anatomy Program ( MAN ) 
 whatever relevant engineering and analytical expertise he can find in 
 the Oak Ridge complex to systematically separate, and then prepare on 
 large scale, the many cellular particles which now can only be seen in 
 1071
This content downloaded from 
              198.51.92.3 on Mon, 06 Feb 2023 21:21:35 UTC               
All use subject to https://about.jstor.org/terms",20023981.pdf
" DJEDALVS 
 the electron microscope. Anderson's success I believe is only the fore 
 runner of future successes that biology will enjoy as it enlists the co 
 operation of the engineering sciences. 
 C. Redeployment of the Big Institutions 
 The modern scientific team arose as an integral part of the great 
 laboratories; whether in basic research or in applied research, the team 
 style is the dominant mode in the modern big laboratory. It seems in 
 evitable then that the future of team research will depend on the fate 
 of the big laboratories. I shall therefore close with a few speculations  on these institutions. 
 Though it is evidently impossible to generalize, one can see limits 
 to the prospects of the big laboratories. For example, in those institu 
 tions devoted to high-energy physics, the future is limited by the sheer 
 increase in expense of the necessary gadgets. The Alternating Gradient 
 Synchrotron facility which was completed in 1960 cost $30,650,000; the 
 Stanford Linear Accelerator, completed in 1966, cost $114,000,000; and 
 the National Accelerator, a 200-GeV proton synchrotron, expected to be 
 operating in 1972, is estimated to cost some $240,000,000. Presumably 
 each of these devices will become obsolete, not because there is not 
 always more spectroscopy to be done, but rather because people will 
 eventually get bored with spectroscopy. Unless there are occasional 
 stirring breakthroughs?perhaps a breakdown of quantum electrody 
 namics?I cannot visualize these institutions forever sustaining them 
 selves, or remaining immortal, by simply amassing spectroscopic details 
 about elementary particles. 
 The atomic energy laboratories must also face questions of redeploy 
 ment, though for a different reason. True, the two central problems of 
 nuclear energy?breeding and controlled fusion?have yet toi be solved. 
 But even these are questions of finite dimensions; the first because it is 
 not all that difficult, the second because it may prove so difficult that 
 interest in it will wane. There is already evidence that the world's 
 atomic energy laboratories are beginning to adjust to these facts, mainly 
 by expanding their areas of concern beyond nuclear energy. 
 By contrast, the future of the great biological laboratories seems 
 clear enough: the questions which biom?dical science seeks to answer 
 are urgent, massive, timeless until they are solved. It seems likely 
 therefore that the need for redeployment will hardly arise for, say, the 
 instrumentalities of the NIH. My guess is that these institutions will ac 
 quire a more interdisciplinary flavor, especially by developing engineer 
 ing skills, simply because the cross between the physical sciences and  biom?dical research has been so fruitful. 
 1072
This content downloaded from 
              198.51.92.3 on Mon, 06 Feb 2023 21:21:35 UTC               
All use subject to https://about.jstor.org/terms",20023981.pdf
" Scientific Teams and Scientific Laboratories 
 Thus redeployment of at least some of the big laboratories is in the  cards. This realization comes at a time when we hear much about the 
 many social and socio-technological conflicts that plague our modern 
 society?racial unrest, the decay of the city, pollution, overpopulation. 
 John Platt sees modern society on the verge of crises so profound as to 
 warrant launching wartime-like projects to resolve them.16 The Com 
 mittee on Government Operations of the United States Senate has been 
 holding hearings during the past couple of years under the chairmanship 
 of Senator Edmund S. Muskie aimed at establishing a Select Committee 
 on Technology and the Human Environment. Everywhere there is a 
 restlessness and concern: the priorities our society has lived with in the 
 postwar world need reassessment; we must abjure our preoccupation  with hard science and address ourselves to these subtler, more difficult, 
 and more important human problems. 
 Whether science can help very much with these social questions is a 
 moot point. Many of us scientists believe that science can help: that 
 almost every one of the conflicts and problems that we face has some 
 technological, as well as social, components, and that therefore science 
 directed specifically at their resolution may be helpful. In this we may 
 be displaying an uninformed na?vet?; perhaps racial conflict, urban 
 decay, and overpopulation are beyond help even from science. 
 Yet this much can be said: if science has something to offer toward 
 resolving these questions, it surely will have to be a broadly inter 
 disciplinary, team type of science. The social components of these prob 
 lems are more obvious than are the technological ones, but there is 
 always an interaction between the two aspects; it is quite natural to 
 visualize interdisciplinary teams, ranging over social science as well as 
 natural science and engineering, being mobilized to attack some of these 
 desperately troublesome questions. As of now, however, such teams 
 have no natural home: the university is unsuitable because of its prej 
 udice against teams, the national laboratory because of its inexperience 
 in social science. I have therefore suggested the creation of new entities, 
 national socio-technological institutes. Some such institutes might be 
 formed ab initio; others by co-opting experts in the social sciences to 
 work in existing hardware-oriented laboratories. 
 National socio-technological institutes at which one would apply 
 the methods of science to our difficult socio-technological problems 
 might also serve an entirely different purpose: a means of focusing the 
 socially relevant energies of our young people. Many young students 
 seem to be disillusioned with natural science: those who in the previous 
 decades went into physics or chemistry now go into the more ""relevant"" 
 social sciences, and even the students of the natural sciences are acquiring 
 a taste for socially relevant issues. Yet I can foresee this socially motivated 
 1073
This content downloaded from 
              198.51.92.3 on Mon, 06 Feb 2023 21:21:35 UTC               
All use subject to https://about.jstor.org/terms",20023981.pdf
" DAEDALUS 
 cohort of students being frustrated all over again if, once they are trained, 
 once they are readied to do battle on behalf of society, they find no in 
 strumentalities to which they can attach themselves to carry on their com 
 mendable crusade. I should think that just as the institutions of big sci 
 ence for several decades provided a home for the aspiring scientists of the 
 1950's and 1960's, so the socio-technological institutions might provide a 
 home for the aspiring social engineers of the 1970's. 
 There have been several suggestions by now for national socio 
 technological institutes, most recently, in Senate Bill 3410 sponsored 
 by Senators Howard Baker and Edmund Muskie to establish national 
 environmental laboratories. It is premature to really assess such proposals. 
 It could be that the difficulties we face go beyond resolution by the 
 methods of science?hard analysis, empirical observation, engineering 
 design. Yet, before taking so pessimistic a view of man's capacity for 
 self-betterment, I would urge trying the interdisciplinary team attack? 
 an approach that was so notably successful in the generation immediately 
 following World War II and that just may help guide us during the 
 coming generation. 
 References 
 1. For alternative approaches to this topic see L. Kowarski, ""Team Work and In  dividual Work in Research,"" in N. Kaplan, Science and Society (Chicago: Rand  McNally, 1965), pp. 247-255; and Cecil F. Powell, ""Promise and Problems of  Modern Science,"" Concluding Address, Maria Sklodowska-Curie: Centenary  Lectures, Proceedings of a Symposium, Warsaw, October 17-20, 1967 (Vienna:  International Atomic Energy Agency, 1968 ). 
 2. Dr. Saul Benison pointed out at the Bellagio conference that I may be over  doing this distinction between art and science: Leonardo trained in the atelier  of Verrochio, who influenced much of his early style; Melville was much in  fluenced by the Bible and by Shakespeare. Yet the connection between, say, the  physicist Hertz and his predecessor Maxwell is, to my mind, far more explicit  and continuous than the connection between two artists. Hertz used Maxwell's 
 equations precisely as Maxwell formulated them; his work flows from Maxwell  with an inevitability and logic that can never be matched in the work of an  artist who follows an illustrious predecessor. As Professor Edward Shils puts it,  ""There is a coercive element in the tradition of the sciences that is absent in 
 the arts."" 
 3. University of Chicago Press, 1962. 
 4. ""Letter to Robert Hooke, February 5, 1675/6,"" in John Bartlett, Familiar 
 Quotations (Boston: Little, Brown and Company, 1968). 
 5. Proceedings of the American Philosophical Society, 94 (October 1950), pp. 422 
 427.
 6. As will be apparent later in the discussion, I often extend and generalize the 
 1074
This content downloaded from 
              198.51.92.3 on Mon, 06 Feb 2023 21:21:35 UTC               
All use subject to https://about.jstor.org/terms",20023981.pdf
" Scientific Teams and Scientific Laboratories 
 word ""spectroscopy"" to mean both the activity and the results of filling in the  scientific details after a major discovery has broken new ground. 
 7. The facelessness of big team research has been commented on by others?for 
 example, Gerald Holton, ""Scientific Research and Scholarship, Notes Toward  the Design of Proper Scales,"" Dsedalus (Spring 1962), pp. 362-399. 
 8. P. B. Medawar, The Art of the Soluble (London: Methuen & Company Ltd., 
 1967).
 9. inside Bureaucracy (Boston: Little, Brown and Company, 1967). 
 10. To anyone who has spent some time in a large laboratory, it must be perfectly  clear what I mean by its great logistic power; but to'those who are unfamiliar 
 with such institutions, perhaps I can illustrate with the experience of a distin 
 guished demographer who spent a summer at Oak Ridge National Laboratory  studying urban decentralization. At the end of his stay I asked him what he 
 thought of ORNL as a possible locale for demographic research. He replied:  ""Demography would be revolutionized if it were conducted there. It would be 
 converted from a small, rather individualistic enterprise into a big-scale, massive  business. There would be huge computers with programmers and mathematicians  to help one use them, experts of every sort available at the other end of the 
 hall, as well as editorial assistants, draftsmen, travel agents; above all, they 
 would be ready and willing to help you get on with the job."" If the large 
 laboratory possesses so much logistic strength in the eyes of a demographer,  one can imagine how much greater is its strength in the fields of science it  was originally set up to exploit! 
 11. Neil Bartlett and N. K. Jha, ""The Xenon-Platinum Hexafluoride Reaction and 
 Related Reactions,"" in H. H. Hyman, ed., Noble-Gas Compounds (Chicago:  University of Chicago Press, 1963), pp. 23-30. 
 12. Hyman, ed., Noble-Gas Compounds; cf. J. H. Holloway, Noble-Gas Chemistry  (London: Methuen & Company Ltd., 1968). 
 13. ""Basic Chemical Research in Government Laboratories,"" Report of the Panel  on Basic Chemical Research in Government Laboratories of the Committee for 
 the Survey of Chemistry, Division of Chemistry and Chemical Technology,  National Academy of Sciences Report 1292-A (Washington, D.C., 1966). 
 14. James D. Watson, The Double Helix (New York: Atheneum, 1968). 
 15. ""Scientific Progress, the Universities, and the Federal Government,"" Statement 
 by the President's Science Advisory Committee (Washington, D.C.: U.S. Gov  ernment Printing Office, November 15, 1960). 
 16. John Platt, ""What We Must Do,"" Science, 166 (November 28, 1969), 1115 
 1121.
 1075
This content downloaded from 
              198.51.92.3 on Mon, 06 Feb 2023 21:21:35 UTC               
All use subject to https://about.jstor.org/terms",20023981.pdf
"The ENIGMA Brain Injury Working Group: Approach, Challenges, 
and Potential Benefits
Elisabeth A. Wilde1,2,3, Emily L. Dennis1,2,4,5, David F. Tate1,2,6
1Department of Neurology, University of Utah School of Medicine, Salt Lake City, UT
2George E. Wahlen VA Medical Center, Salt Lake City, UT
3H. Ben Taub Department of Physical Medicine and Rehabilitation, Baylor College of Medicine, 
Houston, TX
4Psychiatry Neuroimaging Laboratory, Brigham & Women’s Hospital, Harvard Medical School, 
Boston, MA
5Imaging Genetics Center, Stevens Neuroimaging & Informatics Institute, Keck School of 
Medicine of USC, Marina del Rey, CA
6Missouri Institute of Mental Health, University of Missouri, St. Louis, MO
Abstract
The Enhancing Neuro Imaging Genetics through Meta- Analysis (ENIGMA) consortium brings 
together researchers from around the world to try to identify the genetic underpinnings of brain 
structure and function, along with robust, generalizable effects of neurological and psychiatric 
disorders. The recently-formed ENIGMA Brain Injury working group includes 10 subgroups, 
based largely on injury mechanism and patient population. This introduction to the special issue 
summarizes the history, organization, and objectives of ENIGMA Brain Injury, and includes a 
discussion of strategies, challenges, opportunities and goals common across 6 of the subgroups 
under the umbrella of ENIGMA Brain Injury. The following articles in this special issue, including 
6 articles from different subgroups, will detail the challenges and opportunities specific to each 
subgroup.
Introduction to ENIGMA
The Enhancing Neuro Imaging Genetics through Meta- Analysis (ENIGMA; enigma.usc.edu ) 
consortium was formed in 2009 in an effort to increase power to detect associations between 
Terms of use and reuse: academic research for non-commercial purposes, see here for full terms. http://www.springer.com/gb/open-
access/authors-rights/aam-terms-v1
Please address correspondence to : Dr. Emily L Dennis, TBICC, Dept of Neurology, University of Utah School of Medicine, 
emily.dennis@hsc.utah.edu. 
Conflicts of interest
The authors report no potential conflicts of interest related to this project.
Publisher's Disclaimer: This Author Accepted Manuscript is a PDF file of a an unedited peer-reviewed manuscript that has been 
accepted for publication but has not been copyedited or corrected. The official version of record that is published in the journal is kept 
up to date and so may therefore differ from this version.
HHS Public Access
Author manuscript
Brain Imaging Behav . Author manuscript; available in PMC 2022 April 01.
Published in final edited form as:
Brain Imaging Behav . 2021 April ; 15(2): 465–474. doi:10.1007/s11682-021-00450-7.
Author Manuscript Author Manuscript Author Manuscript Author Manuscript",nihms-1672220.pdf
"genetic variation and brain structure and function. ENIGMA has since expanded to examine 
alterations in brain structure and function across a number of disorders, with or without also 
including genetic data. The name ENIGMA, which Webster defines as “mysterious, 
puzzling, or difficult to understand or explain”, also invokes the endeavors of the British 
team at Bletchley Park to decode highly sophisticated war-time communications during 
World War II; similarly, ENIGMA brings investigators together to decode the complex and 
multifaceted factors that influence brain structure and function. At the time of its inception, 
the focus on candidate genes in imaging genetics led to a crisis of reproducibility, but less 
biased genome-wide association studies (GWAS) required tens or hundreds of thousands of 
participants to achieve significance. In 2014, ENIGMA was funded as an NIH Big Data to 
Knowledge (BD2K) Center of Excellence. ENIGMA has resulted in the largest-ever 
neuroimaging datasets of numerous disorders to date, including Major Depression, 
Schizophrenia, and Epilepsy. ENIGMA currently includes 30 disease working groups, 4 
groups on healthy variation over the lifespan, and 9 groups focused on methods 
development. There are currently over 1400 investigators from 40 countries participating in 
ENIGMA activities (see Figure 1). ENIGMA has received funding through over 20 grants 
across the United States, the European Union, and Australia. For a recent review of broader 
ENIGMA activities, see ( Thompson et al. 2020 ).
Formation of ENIGMA Brain Injury
In the fall of 2016, the ENIGMA Brain Injury group was formed. From its inception, it was 
clear that the complexity of brain injury would necessitate specialized groups that could 
more readily address unique features of the varying cohorts. Within the first year, multiple 
subgroups were identified, including Pediatric Moderate/Severe Traumatic Brain Injury 
(TBI) (msTBI) ( Dennis et al. 2020 ), Military-Relevant TBI ( Tate et al. 2020 ), and Sport-
Related Head Injury ( Koerte et al. 2020 ). Soon after, groups for Adult msTBI ( Olsen et al. 
2020 ) and Acute Emergency Department (Civilian) Mild TBI were formed, followed by a 
group focusing on Intimate Partner Violence ( Esopenko et al. 2020 ). The newest groups to 
be formed are focused on emerging imaging methods that may have particular relevance in 
TBI, namely Magnetic Resonance Spectroscopy (MRS)( Bartnik-Olson et al. 2020 ), Arterial 
Spin Labeling (ASL), resting state fMRI (rsfMRI), and Cognitive Endpoints (see Figure 1). 
Additional groups will likely be added in the future to address other aspects of 
methodological and imaging development as well as other TBI-relevant patient populations. 
The focus of this special issue is on the ENIGMA-Brain Injury working group’s efforts to 
facilitate research in TBI and concussion.
Goals and Benefits
The overarching goal of the ENIGMA effort is to create a collaborative framework where 
investigators can work together to address questions and objectives that require large 
amounts of data and to promote replication of preliminary findings through the use of 
multiple and independent samples. Collaboration enables investigators to overcome common 
obstacles which often limit sample sizes in this area of research, including the expense of 
acquiring neuroimaging data and limited sample sizes. The intent of ENIGMA is to 
accelerate the pace of investigation through harnessing the enormous intellectual resources Wilde et al. Page 2
Brain Imaging Behav . Author manuscript; available in PMC 2022 April 01.
Author Manuscript Author Manuscript Author Manuscript Author Manuscript",nihms-1672220.pdf
"and computational power that exists across the globe, not only with regard to a particular 
disease entity, but also through interfacing with others possessing technical expertise in 
imaging, genetics, computational science, or with expertise in conditions that may be 
comorbid (e.g., TBI and PTSD) or may modify disease outcome (e.g., developmental 
issues). Given many unique clinical and functional features of TBI (i.e., spatial and 
functional heterogeneity of injury, common comorbidities, etc), the ENIGMA model poses 
many attractive solutions to addressing important clinical questions.
Immediate goals of the ENIGMA Brain Injury working group are to conduct analyses using 
multiple datasets to find robust effects of brain injury across samples, using mega-analysis 
(direct pooling of data points from different sources) when possible to answer questions that 
require larger samples. Meta-analysis (use of effect sizes from the existing studies or data to 
obtain an overall effect) can function as replication analyses, as effects that are only present 
in a minority of cohorts, or small cohorts that are not likely to survive multiple comparisons 
corrections in the overall analysis. With an increase in statistical power, we can more 
definitively address major questions in the field, such as the existence and nature of sex-
related differences after TBI, how different comparison group impact results (such as contact 
vs. non-contact controls in sports), how comorbid disorders interact with TBI to affect the 
brain (such as PTSD or depression), and how differences in injury mechanisms may 
manifest in the brain. Additionally, large sample sizes allow us to employ machine learning 
approaches to identify patient subgroups based on demographic, clinical, and imaging 
variables, potentially with implications for prognostication and tailored treatment. ENIGMA 
working groups are committed to publishing both positive and negative results, as 
transparency is critical for advancing science and avoiding the “file-drawer” problem 
(Duncan et al. 2018 ). In addition to an increase in statistical power, this collaboration leads 
to an increase in intellectual power by leveraging the collective expertise of a large network 
of scientists. Each researcher brings their own perspective, training background, experience, 
and interests, leading to a rich array of possible projects.
Beyond the immediate goals, ENIGMA Brain Injury is meant to be hypothesis-generating 
for future studies. Although large meta-analyses have increased power, this approach is not 
appropriate for all questions, so we consider it to be complementary to more in-depth 
individual cohort studies. We hope that the findings that result from our efforts raise 
hypotheses that individual sites can interrogate in more depth within their cohorts. Our 
results will hopefully serve as preliminary data to support future individual grant 
submissions by members of ENIGMA Brain Injury. Although the current ENIGMA Brain 
Injury activities will center largely on retrospective data analysis (with the exception of the 
Intimate Partner Violence group, Esopenko et al. 2020 ), this framework will lay the 
foundation for future collaboration between teams. We hope that the exchange of ideas, 
methodology, protocols, data, and analytic tools will lead to further attempts to harmonize 
prospective data collection and will synergize the development of new analytic pipelines and 
techniques.Wilde et al. Page 3
Brain Imaging Behav . Author manuscript; available in PMC 2022 April 01.
Author Manuscript Author Manuscript Author Manuscript Author Manuscript",nihms-1672220.pdf
"Approach
ENIGMA approaches team science in a unique way that differs both conceptually and 
practically from other consortium efforts. We recognize the hesitation that researchers may 
feel in joining group science efforts as well as the logistical hurdles that can accompany data 
sharing, and we make every effort to “meet groups where they are” so that researchers both 
feel comfortable and invested. First, although there are advantages of an approach that favors 
centralized data storage and analysis from the standpoint of quality control, this approach 
has several notable disadvantages, particularly in a global forum. Regulatory mandates may 
prohibit or limit the transfer or sharing of some forms of health information and data, 
including neuroimaging and samples with genetic information. Additionally, centralized 
models may create logistical challenges for the institution where data reside, including 
issues related to the recurring personnel and infrastructure costs of storing and transferring 
large amounts of data. Centralized models also often create a situation where some 
investigators have more access to the data and resources than others, which may limit 
enthusiasm for contributing data. Centralized models may also lack incentives for sharing 
since funding is often awarded to a primary site and publication credit may favor 
investigators at the primary sites. The ENIGMA approach circumvents each of these issues 
in an innovative approach.
First, to accommodate data sharing issues, contribution of raw data is not required for 
participation; though processing support (at several levels) is available for groups that 
request it. Basic requirements for participation are the contribution of raw or summary 
magnetic resonance imaging (MRI) data (e.g., T1-weighted imaging) and simple clinical and 
demographic information. Additional imaging sequences, such as diffusion MRI (dMRI), 
task-based and resting-state fMRI (tbfMRI and rsfMRI), magnetic resonance spectroscopy 
(MRS), and arterial spin labeling (ASL), and more detailed clinical and cognitive 
information allow for broader participation, but are not required. Sites will vary in both 
imaging acquisition parameters, but follow common, validated processing and analytic steps 
as part of the ENIGMA framework. Processing guidelines and scripts for subcortical 
volume, cortical measures, and diffusion MRI measures can be found on the ENIGMA 
website ( http://enigma.ini.usc.edu/protocols/ ), and ENIGMA working groups are engaged in 
developing and adapting pipelines for additional imaging modalities. There are numerous 
challenges in combining and harmonizing distinct datasets, discussed in more detail in the 
Limitations and Challenges section below. Big data analytics is a rapidly advancing field, 
enabling more sophisticated modeling, but these approaches are suboptimal if the input data 
are not equivalent across sites.
The ENIGMA platform is intended to be flexible and expandable (see Figure 2). ENIGMA 
has been successful in other areas of research in part because of the flexibility with data 
sharing: sharing raw data is never required, but a central site is available to provide 
computing support or analysis training, if necessary. Investigators may choose to process 
and analyze their data locally, sending only summary statistics to the lead investigator for an 
analysis that has been proposed, thus maintaining the maximum amount of control over their 
data. Sites that have a stronger emphasis on clinical expertise may opt to send raw imaging 
data to a central site for a given investigation if they do not have the computational or Wilde et al. Page 4
Brain Imaging Behav . Author manuscript; available in PMC 2022 April 01.
Author Manuscript Author Manuscript Author Manuscript Author Manuscript",nihms-1672220.pdf
"personnel resources to process their own data locally. In all cases, analyses are opt in, 
meaning that participation in one project does not assume participation in all projects. The 
result is massively distributed computing, as processing is spread across many sites and 
investigators with different expertise. All working group members are encouraged to submit 
secondary proposals to the group for projects they wish to lead. In this respect, ENIGMA is 
a framework for collaboration, flexible to the interests and restrictions of each participating 
member. While the initial analyses can be completed on existing datasets, the infrastructure 
is established for coordinating aspects of study protocols in new projects which have not yet 
collected data. We encourage the participation of researchers at all levels, and aim to support 
early career researchers through an expanded network of collaborators and access to larger 
amounts of data than is commonly available.
Of further note, there have been a number of recent consortia efforts in TBI and concussion 
neuroimaging. Most of these are multi-site studies with varying degrees of harmonization in 
study protocol. A number have been focused on brain injury in Military Service Members, 
including the Chronic Effects of Neurotrauma Consortium (CENC, Walker et al. 2016 ) and 
the Long-term Impact of Military-relevant brain Injury Consortium (LIMBIC-CENC), the 
Study of Brain Aging in Vietnam War Veterans (DoD ADNI, Weiner et al. 2014 ), the 
Vietnam-Era Twin Study (VETSA, Kremen et al. 2013 ), and the Injury and Traumatic Stress 
(INTRuST, Lepage et al. 2018 ) study. Others have been focused on sports-related head 
impacts, including the NCAA-DoD Grand Alliance Concussion Assessment, Research, and 
Education (CARE, Broglio et al. 2017 ) consortium and the Big Ten-Ivy League Traumatic 
Brain Injury Research Collaboration ( Putukian et al. 2019 ). Lastly, Translating Research and 
Clinical Knowledge in TBI (TRACK-TBI, Yue et al. 2013 ) and Collaborative European 
NeuroTrauma Effectiveness Research in TBI (CENTER-TBI, Maas et al. 2015 ) are multi-
site studies recruiting from emergency departments (EDs), covering a wide range of injury 
types and severities. These large studies will significantly advance our understanding of 
factors that influence outcome after TBI, but the large cost of collecting such large samples 
limits participation of all interested investigators and requires dedicated funding 
opportunities. Some ENIGMA working groups are examining ways to work together to 
converge data collection methods in studies that are just being designed or launched. 
However, historically, the main differences between these consortia and ENIGMA Brain 
Injury is the use of prospective vs. existing retrospective data harmonization and the degree 
of data centralization at a specific site. Each approach has benefits and drawbacks, and we 
believe there is a place for both in the field of TBI research. Studies that are prospectively 
harmonized obviously generate data that are more equivalent and simplify the harmonization 
steps, but they require large amounts of funding, planning, and coordination across sites. 
While the ENIGMA model requires more effort to produce comparable data, using legacy 
datasets represents a cost-effective way to gain further insight from completed projects. 
Harmonization can occur at multiple points during data processing, allowing multiple 
datasets to be used in a unified approach. With the flexibility in data sharing, the ENIGMA 
model engages a larger group of researchers – data sharing regulations differ tremendously 
across sites and across countries and clearly it is not possible to join a prospectively 
harmonized multi-site study after it has begun.Wilde et al. Page 5
Brain Imaging Behav . Author manuscript; available in PMC 2022 April 01.
Author Manuscript Author Manuscript Author Manuscript Author Manuscript",nihms-1672220.pdf
"Successes in Other ENIGMA Groups
Among ENIGMA working groups, the Brain Injury group is relatively young, allowing it to 
benefit from the experiences of more established working groups. We highlight three 
working groups here that have some comorbidity with TBI. The Major Depressive Disorder 
(MDD) was one of the first disease groups to be formed in 2014. To date, they have 
published a large number of papers across a variety of modalities examining both broad 
disease effects and more specific symptoms ( Frodl et al. 2017 ; Kelly et al. 2017 ; Rentería et 
al. 2017 ; Schmaal et al. 2016 , 2017 ; Tozzi et al. 2019 ). Additionally, this group has led the 
creation of related focus groups, such as the Suicidal Thoughts and Behaviors (STB) 
working group. While the MDD group was supported by the initial NIH BD2K Center of 
Excellence grant along with 6 other psychiatric working groups, this initial phase of funding 
has been completed. One group that has been successful in receiving grant support is the 
Post-Traumatic Stress Disorder (PTSD) working group. The PTSD working group has 
considerable overlap in membership with the Military Brain Injury subgroup and has also 
published papers on subcortical volume ( Logue et al. 2017 ) and white matter microstructure 
(Dennis et al. 2019 ). The ENIGMA Addiction working group has similarly received grant 
support, and recently published a paper of 3,240 individuals examining multiple substances. 
They found that alcohol abuse was associated with the most substantial alterations in cortical 
measures ( Mackey et al. 2019 ). Depression, PTSD, and substance use disorders (SUDs) are 
all potentially comorbid with TBI, as either pre-injury and/or outcome so interfacing with 
these groups will support important cross-disorder analyses.
Potential
ENIGMA has the potential to address many of the challenges listed above. There is 
tremendous heterogeneity in TBI and outcome is likely influenced by a large range of 
demographic and clinical variables. When variability is high, large samples are necessary to 
detect reliable effects. Through the increased sample size ENIGMA facilitates, there is 
greater power to detect abnormalities that are consistent across patients, and also to perhaps 
identify subgroups with distinct clinical prognoses. As discussed in more detail in the 
following papers of this issue written by the leaders of each subgroup, there are a large 
number of potentially confounding variables when researching TBI. For example, with 
regard to the complex intersection of TBI and psychiatric disorders, TBI has been cited as 
both a risk factor for subsequent development of post-injury psychopathology or 
developmental disorder (e.g., ADHD), but a history of pre-existing psychopathology may 
also increase the risk of sustaining a head injury. Therefore, comorbid disorders must be 
carefully considered, as mentioned above. Moreover, some comorbid disorders are more 
prevalent in certain subgroups (e.g., ADHD in children, PTSD in Military Service Members 
and Intimate Partner Violence), while others are generally comorbid with TBI of any 
population or severity (e.g., MDD). Larger sample sizes made possible by ENIGMA allow 
consideration of these confounds, and investigators will collaborate with existing ENIGMA 
working groups dedicated to these potentially comorbid disorders. Through collaboration 
with these groups, we endeavor to identify neural phenotypes that are distinct and also 
identify common features that exist across disorders.Wilde et al. Page 6
Brain Imaging Behav . Author manuscript; available in PMC 2022 April 01.
Author Manuscript Author Manuscript Author Manuscript Author Manuscript",nihms-1672220.pdf
"A central aim of the ENIGMA Brain Injury group is identifying factors that affect outcome. 
Some of these may be variables that cannot be modified, such as gender/sex, age, or genetic 
variability, but could warrant more targeted treatment. Others might be modifiable, and 
amenable to treatment or intervention. There may be subgroups of individuals within the 
larger patient group that manifest different patterns of dysfunction. The use of “big data” 
may thus allow us to more accurately predict future recovery or neurodegeneration. Another 
central aim of the ENIGMA Brain Injury group is to develop new image processing 
workflows that are appropriate for brain-injured populations or specifically aimed at 
characterizing injury-related pathology. With individual lesion maps, we can optimize 
existing image processing pipelines and directly examine associations between lesion 
location and functional disruption. We aim to work with others to develop pipelines for 
automated detection of white matter hyperintensities. Additionally, there are a number of 
approaches that have been well studied in individual cohorts, including multimodal 
approaches like connectomics, which we plan to extend for use across multiple cohorts.
Limitations and Challenges
One of the key challenges of multi-site efforts is adequate data harmonization. Large sample 
sizes will not overcome uncharacterized heterogeneity between datasets, and there is a risk 
of a “garbage in, garbage out” outcome if appropriate harmonization and quality control 
steps are not taken. TBI manifests in different forms across severity, acuity, and age at injury, 
highlighting the importance of defining the patient population. Harmonization crosses 
multiple domains, including imaging, neuropsychological assessment, clinical outcomes, 
and blood biomarkers. Combining imaging data is first challenged by different naming 
conventions and data organization, which can be helped by using BIDS (Brain Imaging Data 
Structure) standards ( Gorgolewski et al. 2016 ). As ENIGMA mainly works with data that 
have already been collected, harmonization is completed post hoc  as a data processing step. 
For new data collection, we have the opportunity to harmonize aspects of different of 
protocols. Of note, for structural imaging, T1-weighted MRI is more straightforward. While 
protocols do differ across manufacturers, a voxel-size of 1 mm3 is standard, making volume 
calculations less variable. For diffusion MRI (dMRI), there is considerable variability in 
angular resolution, diffusion weighting, and voxel size. Even two scanners from the same 
manufacturer running the same protocols will yield slightly different average diffusivity 
measures, making it critical that dMRI analyses are meta-analyses, not mega-analyses, 
unless harmonization like ComBat or similar algorithms are applied ( Cetin-Karayumak et al. 
2019 ; Cetin Karayumak et al. 2019 ; Johnson et al. 2007 ). One benefit of this variability, 
however, is that it increases the generalizability of results. We can have more confidence in 
effects that are detected at both 12 direction dMRI and 128 direction dMRI. Additionally, 
there are current efforts in the ENIGMA consortium to develop harmonized methods for 
functional MRI and resting-state fMRI ( Adhikari et al. 2018 ; Adhikari et al. 2018 ; Veer et al. 
2019 ). To address this challenge, the ENIGMA Brain Injury group will experiment with 
various harmonization approaches mentioned above, taking advantage of the multi-site 
projects already included that have more formal harmonization procedures as part of the 
study protocol. This work will yield further insight into factors that impact the within 
sequence imaging heterogeneity.Wilde et al. Page 7
Brain Imaging Behav . Author manuscript; available in PMC 2022 April 01.
Author Manuscript Author Manuscript Author Manuscript Author Manuscript",nihms-1672220.pdf
"An additional challenge lies in harmonizing cognitive and clinical measures. Although the 
introduction of International Common Data Elements for TBI has facilitated use of 
recommended measures within a variety of outcome domains, considerable variability still 
exists as appropriate measures differ between populations of interest (e.g., athletes vs 
military), age range (measures developed and normed for young children differ from those 
used in older children or adults), acuity (symptoms and outcome for acute vary from those in 
chronic phases of recovery) and severity (outcome domains and measures most relevant for 
concussion differ from those used in more severe TBI). Although neuropsychological testing 
or other outcome assessment is common in many studies, the specific assessments used 
necessarily vary widely. Several options exist for harmonizing these data or developing 
common comparable cognitive endpoints that could be used to further the research. The 
most conservative approach involves identification of the most commonly used scales across 
studies and focus analyses around domains and cohorts where common data was collected. 
Another approach would be to convert scores within a given outcome domain to 
standardized T-scores based on population means and standard deviations. This allows for 
more variability in the specific measures that can be included, but care must be applied in 
ensuring that the cognitive constructs are indeed consistent. A third approach involves 
calculating a cognitive composition score created by assigning weighted scores based on the 
degree to which an individual test score deviates from normative expectations (increased 
deviation, increased weight). This method is expected to improve sensitivity by creating 
finer gradations across patients and increase the ceiling for improved detection even in 
mTBI ( Silverberg et al. 2017 ). Finally, in addition to working together to harmonize existing 
outcome data and to address novel imaging analytic pipelines, the development, testing, and 
optimization of innovative cognitive and neurobehavioral outcome measures that are specific 
to the assessment of mTBI, concussion, and repetitive head hits may be a goal of the 
working groups, where appropriate.
Though not discussed directly in this special issue, the recently established Cognitive 
Endpoints group has begun piloting additional statistical methods that are intended to 
quantify the disparity in cognitive measures administered between cohorts. Once the 
disparity is known, the goal would be to apply a set of flexible statistical methods designed 
to minimize the disparity (i.e., item response theory, machine learning) in measures 
administered to produce a set of co-calibrated and validated measures that can be used in 
analyses with the harmonized imaging data. Following these steps (see Figure 3), seemingly 
disparate cognitive data acquired independently across cohorts can move from a low value 
state with regards to big data approaches toward a more useful high value set of variables 
that can examined in an aggregated manner. Data curated and processed in this manner will 
be especially critical when defining the relevant brain-behavior relationships or identifying 
any unique behavioral phenotypes that exist across or within TBI cohorts. In addition, this 
promising effort could be applied more broadly to other ENIGMA working data to allow for 
the exploration of behavioral/functional relationships between imaging findings and these 
common cognitive endpoints.
The use of fluid biomarkers and genetics also brings challenges. As with imaging, important 
considerations surround differences in the collection, processing, and analysis of biofluid 
samples that will necessarily vary by site. An optimal approach includes the use of Wilde et al. Page 8
Brain Imaging Behav . Author manuscript; available in PMC 2022 April 01.
Author Manuscript Author Manuscript Author Manuscript Author Manuscript",nihms-1672220.pdf
"standardized procedures on the actual samples using a limited number of “batches”, but 
meta-analyses are also possible with these data ( Berger et al. 2012 ; Manley et al. 2010 ).
While advances in technology have supported the growing movement towards data sharing 
and open science, there are important legal, ethical, and regulatory issues with global data 
sharing ( Palk et al. 2019 ). All of these considerations are aimed at protecting participant 
privacy and controlling data use. Anonymization is often required, although what is 
considered “anonymized” differs ( Sariyar et al. 2015 ), and this can include both meta-data in 
a file header and physical features from MR images ( Milchenko and Marcus 2013 ). While 
some institutions and countries allow for data sharing to be considered under a “broad 
consent”, others require explicit statements regarding future potential uses of individual data. 
The recently enacted General Data Protection Regulation (GDPR) imposed restrictions on 
sharing personal data within and outside the European Union and requires explicit consent 
from an individual to share data outside of a few lawful purposes ( V oigt and V on dem 
Bussche 2017 ). Material Transfer Agreements (MTAs) and Data Use Agreements (DUAs) 
are required by some institutions, and may be applied more stringently in certain populations 
(e.g., U.S. Veterans and Active Duty Service Members, children, etc.). A Uniform 
Biological Material Transfer Agreement (UBMTA) can expedite transfer between 
participating universities ( Carr et al. 2017 ). Sharing genomic data brings additional 
concerns, as release of this information could have broad legal, medical, and other 
consequences for the individual. The NIH mandates that genomic data be submitted to 
dbGaP (database of Genotypes and Phenotypes) after identifying information is removed 
(names, dates, locations)( Paltoo et al. 2014 ). Controlled access to genomic data is then 
granted to researchers for a specific project. Sharing summary level data within ENIGMA, 
as opposed to raw imaging data, addresses many of these concerns, although some 
institutions restrict this level of sharing as well. In these cases, scripts for site-level statistical 
analysis can be shared with summary statistics returned to the primary site. COINSTAC 
(Collaborative Informatics and Neuroimaging Suite Toolkit for Anonymous Computation) is 
web-based framework for executing harmonized processing and analysis across multiple 
sites that allows the data to stay local, negating the need for explicit sharing consent or 
MTA/DUAs ( Plis et al. 2016 ). Encouragingly, other consortia have reported that 
harmonization was more of a challenge than permissions ( Budin-Ljøsne et al. 2014 ). The 
activities of ENIGMA over the last decade have shown that while there are numerous 
hurdles to data-sharing and international collaboration, in most cases there are solutions that 
satisfy the ethical and legal requirements while facilitating group science ( Thompson et al. 
2020 ). ENIGMA is active in efforts to enhance understanding of regulations in several 
regions of the world around data sharing, and to formulate solutions to facilitate global 
collaboration.
Progress and Deliverables
We will continuously assess our progress to ensure that ENIGMA Brain Injury is moving the 
field forward and supporting the researchers involved. Our immediate goals are to establish 
and expand our network of interested researchers along with potential new datasets to 
include, and identify funding mechanisms to support the effort. Intermediate goals include 
developing and testing new pipelines for processing neuroimaging data that consider Wilde et al. Page 9
Brain Imaging Behav . Author manuscript; available in PMC 2022 April 01.
Author Manuscript Author Manuscript Author Manuscript Author Manuscript",nihms-1672220.pdf
"structural deformations, improve classification of neuropathology such as white matter 
hyperintensities, allow for longitudinal modeling of change after injury, and are stable and 
useful across sites. Continuous goals include supporting the advancement of junior 
investigators through the opportunity to propose and lead new analyses and to provide a 
forum for researchers to discuss controversies and open questions in the field. In the long-
term, we hope that this effort will yield hypotheses that researchers can interrogate in greater 
depth in their individual cohorts and lead to new collaborations among participants as they 
plan future studies with a goal of improving comparability in a mutually beneficial manner. 
The deliverables for the ENIGMA Brain Injury group include publications, grants, new 
pipeline development, establishment of best practices for combining data and for processing 
TBI neuroimaging data (particularly with regard to lesions), and datasets that have been 
curated and harmonized. The ENIGMA Brain Injury group has quarterly conference calls 
along with regular in-person meetings coinciding with relevant conferences to inform 
collaborators of progress and make plans moving forward. Each subgroup has monthly or bi-
monthly conference calls to discuss specific analyses.
Conclusions
The ENIGMA Brain Injury group aims to bring together researchers from around the world 
with the shared goal of furthering our understanding of the impact of brain injury and factors 
that may influence outcome. Building off of the framework of the extremely productive 
broader ENIGMA consortium, we are optimistic that this effort will yield new information 
and will help answer open questions in the field. We also expect our research to introduce 
new questions and hypotheses that individual cohorts can investigate in greater detail and 
will hopefully inspire new data collection. In contrast to other efforts, raw data are not 
centralized, and contributing sites maintain ownership and control of their data, with all 
analyses being opt-in. All members are welcome to submit secondary proposals, benefitting 
from an expanded collaborative network and larger sample size. Researchers interested in 
joining or learning more about the ENIGMA Brain Injury group are encouraged to read the 
companion articles in this special issue and contact the authors.
Acknowledgements
We acknowledge funding sources including K99 NS096116 to Dr. Dennis and PT12051 and I01 RX002174 
(CENC) to Drs. Wilde, Tate, and Dennis. The authors wish to acknowledge the leadership of Dr. Paul Thompson 
(ENIGMA PI) as well as the leadership of Drs. Frank Hillary, Alexander Olsen, Inga Koerte, David Baron, 
Alexander Lin, Brenda Bartnik-Olson, Karen Caeyenberghs, Carrie Esopenko, and Neda Jahanshad, as well as 
ENIGMA support personnel and all working group members and contributors. We wish to acknowledge the 
contribution of Eamonn Kennedy in the conceptualization and creation of the figures. We also wish to thank all 
participants who have contributed to this research. Finally, we extend our gratitude to Drs. Erin D. Bigler and 
Martha E. Shenton for their thoughtful review of this manuscript and their enduring mentorship and support.
References
Adhikari BM, Jahanshad N, Shukla D, Glahn DC, Blangero J, Reynolds RC, et al. (2018). Heritability 
estimates on resting state fMRI data using ENIGMA analysis pipeline. Pacific Symposium on 
Biocomputing. Pacific Symposium on Biocomputing, 23, 307–318. [PubMed: 29218892] 
Adhikari BM, Jahanshad N, Shukla D, Turner J, Grotegerd D, Dannlowski U, et al. (2018). A resting 
state fMRI analysis pipeline for pooling inference across diverse cohorts: an ENIGMA rs-fMRI 
protocol. Brain imaging and behavior. doi:10.1007/s11682-018-9941-xWilde et al. Page 10
Brain Imaging Behav . Author manuscript; available in PMC 2022 April 01.
Author Manuscript Author Manuscript Author Manuscript Author Manuscript",nihms-1672220.pdf
"Bartnik-Olson B, Alger J, Babikian T, Harris AD, Holshouser B, Kirov II, et al. (2020). The Clinical 
Utility of Magnetic Resonance Spectroscopy in Traumatic Brain Injury: Recommendations from the 
ENIGMA MRS Working Group. Brain imaging and behavior, In Press (Special Issue).
Berger RP, Beers SR, Papa L, Bell M, & Pediatric TBI CDE Biospecimens and Biomarkers 
Workgroup. (2012). Common data elements for pediatric traumatic brain injury: recommendations 
from the biospecimens and biomarkers workgroup. Journal of neurotrauma, 29(4), 672–677. 
[PubMed: 22106839] 
Broglio SP, McCrea M, McAllister T, Harezlak J, Katz B, Hack D, et al. (2017). A National study on 
the effects of concussion in collegiate athletes and US military service academy members: the 
NCAA--DoD concussion assessment, research and education (CARE) consortium structure and 
methods. Sports medicine, 47(7), 1437–1451. [PubMed: 28281095] 
Budin-Ljøsne I, Isaeva J, Knoppers BM, Tassé AM, Shen H-Y , McCarthy MI, et al. (2014). Data 
sharing in large research consortia: experiences and recommendations from ENGAGE. European 
journal of human genetics: EJHG, 22(3), 317–321. [PubMed: 23778872] 
Carr N, Shin I, & Maier S (2017, 3 1). Material Transfer and Data Use Agreements. Journal of Clinical 
Research Best Practices. https://papers.ssrn.com/abstract=3195015 . Accessed 11 September 2019
Cetin Karayumak S, Bouix S, Ning L, James A, Crow T, Shenton M, et al. (2019). Retrospective 
harmonization of multi-site diffusion MRI data acquired with different acquisition parameters. 
NeuroImage, 184, 180–200. [PubMed: 30205206] 
Cetin-Karayumak S, Di Biase MA, Chunga N, Reid B, Somes N, Lyall AE, et al. (2019). White matter 
abnormalities across the lifespan of schizophrenia: a harmonized multi-site diffusion MRI study. 
Molecular psychiatry. doi:10.1038/s41380-019-0509-y
Dennis EL, Caeyenberghs K, Asarnow RF, Babikian T, Bartnik-Olson B, Bigler ED, et al. (2020). 
Brain Imaging in Young Brain-Injured Patients: A Coordinated Effort Towards Individualized 
Predictors from the ENIGMA Pediatric msTBI Group. Brain imaging and behavior, In Press 
(Special Issue).
Dennis EL, Disner SG, Fani N, Salminen LE, Logue M, Clarke EK, et al. (2019, 6 20). Altered White 
Matter Microstructural Organization in Post-Traumatic Stress Disorder across 3,049 Adults: 
Results from the PGC-ENIGMA PTSD Consortium. bioRxiv. doi:10.1101/677153
Duncan D, Barisano G, Cabeen R, Sepehrband F, Garner R, Braimah A, et al. (2018). Analytic Tools 
for Post-traumatic Epileptogenesis Biomarker Search in Multimodal Dataset of an Animal Model 
and Human Patients. Frontiers in neuroinformatics, 12, 86. [PubMed: 30618695] 
Esopenko C, Meyer J, Wilde EA, Marshall A, Tate DF, Lin A, et al. (2020). Harmonization of 
Measures to Assess IPV-Related Head Trauma: Recommendations from the ENIGMA IPV 
Working Group. Brain imaging and behavior, In Press (Special Issue).
Frodl T, Janowitz D, Schmaal L, Tozzi L, Dobrowolny H, Stein DJ, et al. (2017). Childhood adversity 
impacts on brain subcortical structures relevant to depression. Journal of psychiatric research, 86, 
58–65. [PubMed: 27918926] 
Gorgolewski KJ, Auer T, Calhoun VD, Craddock RC, Das S, Duff EP, et al. (2016). The brain imaging 
data structure, a format for organizing and describing outputs of neuroimaging experiments. 
Scientific data, 3, 160044. [PubMed: 27326542] 
Johnson WE, Li C, & Rabinovic A (2007). Adjusting batch effects in microarray expression data using 
empirical Bayes methods. Biostatistics, 8(1), 118–127. [PubMed: 16632515] 
Kelly S, van Velzen L, Veltman D, Thompson P, Jahanshad N, Schmaal L, & Group, Group. (2017). 
941. White Matter Microstructural Differences in Major Depression: Meta-Analytic Findings from 
Enigma-MDD DTI. Biological psychiatry, 81(10), S381.
Koerte IK, Dennis EL, Bazarian JJ, Bigler ED, Buckley T, Choe M, et al. (2020). Neuroimaging of 
Sport-Related Brain Injury: Challenges and Recommendations from the ENIGMA Sports-Related 
Brain Injury group. Brain imaging and behavior, In Press (Special Issue).
Kremen WS, Franz CE, & Lyons MJ (2013). VETSA: the Vietnam Era Twin Study of Aging. Twin 
research and human genetics: the official journal of the International Society for Twin Studies, 
16(1), 399–402. [PubMed: 23110957] 
Lepage C, de Pierrefeu A, Koerte IK, Coleman MJ, Pasternak O, Grant G, et al. (2018). White matter 
abnormalities in mild traumatic brain injury with and without post-traumatic stress disorder: a Wilde et al. Page 11
Brain Imaging Behav . Author manuscript; available in PMC 2022 April 01.
Author Manuscript Author Manuscript Author Manuscript Author Manuscript",nihms-1672220.pdf
"subject-specific diffusion tensor imaging study. Brain imaging and behavior, 12(3), 870–881. 
[PubMed: 28676987] 
Logue MW, van Rooij SJH, Dennis EL, Davis SL, Hayes JP, Stevens JS, et al. (2017). Smaller 
hippocampal volume in posttraumatic stress disorder: a multi-site ENIGMA-PGC study. 
Biological psychiatry.
Maas AIR, Menon DK, Steyerberg EW, Citerio G, Lecky F, Manley GT, et al. (2015). Collaborative 
European NeuroTrauma Effectiveness Research in Traumatic Brain Injury (CENTER-TBI): a 
prospective longitudinal observational study. Neurosurgery, 76(1), 67–80. [PubMed: 25525693] 
Mackey S, Allgaier N, Chaarani B, Spechler P, Orr C, Bunn J, et al. (2019). Mega-Analysis of Gray 
Matter V olume in Substance Dependence: General and Substance-Specific Regional Effects. The 
American journal of psychiatry, 176(2), 119–128. [PubMed: 30336705] 
Manley GT, Diaz-Arrastia R, Brophy M, Engel D, Goodman C, Gwinn K, et al. (2010). Common data 
elements for traumatic brain injury: recommendations from the biospecimens and biomarkers 
working group. Archives of physical medicine and rehabilitation, 91(11), 1667–1672. [PubMed: 
21044710] 
Milchenko M, & Marcus D (2013). Obscuring surface anatomy in volumetric imaging data. 
Neuroinformatics, 11(1), 65–75. [PubMed: 22968671] 
Olsen A, Babikian T, Bigler E, Caeyenberghs K, Conde V , Dams-O’Connor K, et al. (2020). Toward a 
Global and Open Science for Imaging Brain Trauma: the ENIGMA Adult msTBI Working Group. 
Brain imaging and behavior, In Press (Special Issue).
Palk A, Illes J, Thompson PM, & Stein DJ (2019). Ethical Issues in Global Imaging Genetics 
Collaborations. NeuroImage, In Review.
Paltoo DN, Rodriguez LL, Feolo M, Gillanders E, Ramos EM, Rutter JL, et al. (2014). Data use under 
the NIH GWAS data sharing policy and future directions. Nature genetics, 46(9), 934–938. 
[PubMed: 25162809] 
Plis SM, Sarwate AD, Wood D, Dieringer C, Landis D, Reed C, et al. (2016). COINSTAC: A Privacy 
Enabled Model and Prototype for Leveraging and Processing Decentralized Brain Imaging Data. 
Frontiers in Neuroscience. doi:10.3389/fnins.2016.00365
Putukian M, D’Alonzo BA, Campbell-McGovern CS, & Wiebe DJ (2019). The Ivy League-Big Ten 
Epidemiology of Concussion Study: A Report on Methods and First Findings. The American 
journal of sports medicine, 47(5), 1236–1247. [PubMed: 30943078] 
Rentería ME, Schmaal L, Hibar DP, Couvy-Duchesne B, Strike LT, Mills NT, et al. (2017). Subcortical 
brain structure and suicidal behaviour in major depressive disorder: a meta-analysis from the 
ENIGMA-MDD working group. Translational psychiatry, 7(5), e1116. [PubMed: 28463239] 
Sariyar M, Schluender I, Smee C, & Suhr S (2015). Sharing and Reuse of Sensitive Data and Samples: 
Supporting Researchers in Identifying Ethical and Legal Requirements. Biopreservation and 
biobanking, 13(4), 263–270. [PubMed: 26186169] 
Schmaal L, Hibar DP, Sämann PG, Hall GB, Baune BT, Jahanshad N, et al. (2017). Cortical 
abnormalities in adults and adolescents with major depression based on brain scans from 20 
cohorts worldwide in the ENIGMA Major Depressive Disorder Working Group. Molecular 
psychiatry, 22(6), 900–909. [PubMed: 27137745] 
Schmaal L, Veltman DJ, van Erp TGM, Sämann PG, Frodl T, Jahanshad N, et al. (2016). Subcortical 
brain alterations in major depressive disorder: findings from the ENIGMA Major Depressive 
Disorder working group. Molecular psychiatry, 21(6), 806–812. [PubMed: 26122586] 
Silverberg ND, Crane PK, Dams-O’Connor K, Holdnack J, Ivins BJ, Lange RT, et al. (2017). 
Developing a Cognition Endpoint for Traumatic Brain Injury Clinical Trials. Journal of 
neurotrauma, 34(2), 363–371. [PubMed: 27188248] 
Tate DF, Dennis EL, Adams JT, Adamson MM, Belanger HG, Bigler ED, et al. (2020). Coordinating 
Global Multi-Site Studies of Military TBI: Potential, Challenges, and Harmonization Guidelines 
from the ENIGMA Military Brain Injury Group. Brain imaging and behavior, In Press (Special 
Issue).
Thompson P, Jahanshad N, Ching CRK, Salminen L, Thomopoulos SI, Bright J, et al. (2020). 
ENIGMA and Global Neuroscience: A Decade of Large-Scale Studies of the Brain in Health and Wilde et al. Page 12
Brain Imaging Behav . Author manuscript; available in PMC 2022 April 01.
Author Manuscript Author Manuscript Author Manuscript Author Manuscript",nihms-1672220.pdf
"Disease across more than 40 Countries. Translational Psychiatry, 10(1), 1–28 [PubMed: 
32066695] 
Tozzi L, Garczarek L, Janowitz D, Stein DJ, Wittfeld K, Dobrowolny H, et al. (2019). Interactive 
impact of childhood maltreatment, depression, and age on cortical brain structure: mega-analytic 
findings from a large multi-site cohort. Psychological medicine, 1–12.
Veer I, Waller L, Lett T, Erk S, & Walter H (2019). ENIGMA task-based fMRI: A workgroup studying 
the genetic basis of task-evoked brain activity. Presented at the Organization for Human Brain 
Mapping.
V oigt P, & V on dem Bussche A (2017). The eu general data protection regulation (gdpr). A Practical 
Guide, 1st Ed., Cham: Springer International Publishing. https://link.springer.com/content/pdf/
10.1007/978-3-319-57959-7.pdf
Walker WC, Carne W, Franke LM, Nolen T, Dikmen SD, Cifu DX, et al. (2016). The Chronic Effects 
of Neurotrauma Consortium (CENC) multi-centre observational study: Description of study and 
characteristics of early participants. Brain injury: [BI], 30(12), 1469–1480.
Weiner MW, Veitch DP, Hayes J, Neylan T, Grafman J, Aisen PS, et al. (2014). Effects of traumatic 
brain injury and posttraumatic stress disorder on Alzheimer’s disease in veterans, using the 
Alzheimer's Disease Neuroimaging Initiative. Alzheimer’s & dementia: the journal of the 
Alzheimer's Association, 10(3 Suppl), S226–35.
Yue JK, Vassar MJ, Lingsma HF, Cooper SR, Okonkwo DO, Valadka AB, et al. (2013). Transforming 
research and clinical knowledge in traumatic brain injury pilot: multicenter implementation of the 
common data elements for traumatic brain injury. Journal of neurotrauma, 30(22), 1831–1844. 
[PubMed: 23815563] Wilde et al. Page 13
Brain Imaging Behav . Author manuscript; available in PMC 2022 April 01.
Author Manuscript Author Manuscript Author Manuscript Author Manuscript",nihms-1672220.pdf
"Figure 1. 
Organization of the ENIGMA consortium and the ENIGMA Brain Injury working group. 
Map in top right shows current sites across ENIGMA, map in bottom right shows current 
sites in the ENIGMA Brain Injury working group. TBI=traumatic brain injury, 
dMRI=diffusion magnetic resonance spectroscopy, EEG=electroencephalography, 
rsfMRI=resting state functional MRI, tbfMRI=task based fMRI, GWAS=genome-wide 
association study, CNV=copy number variation, MDD=major depressive disorder, 
AD=anxiety disorder, PTSD=post-traumatic stress disorder, FTD=frontotemporal dementia, 
HIV=human immunodeficiency virus, OCD=obsessive compulsive disorder, 
ADHD=attention-deficit/hyperactivity disorder, 22q DS=22q11.2 deletion syndrome. 
Adapted from Thompson et al., 2020 .Wilde et al. Page 14
Brain Imaging Behav . Author manuscript; available in PMC 2022 April 01.
Author Manuscript Author Manuscript Author Manuscript Author Manuscript",nihms-1672220.pdf
"Figure 2. 
Schematic showing the approach and goals of the ENIGMA Brain Injury working group.Wilde et al. Page 15
Brain Imaging Behav . Author manuscript; available in PMC 2022 April 01.
Author Manuscript Author Manuscript Author Manuscript Author Manuscript",nihms-1672220.pdf
"Figure 3. 
This illustration shows several steps in producing common cognitive endpoints using data 
from different sources/cohorts. The first column shows a theoretical set of data from a 
variety of sources with unknown disparity, for which the value of the data in aggregated 
form is unknown. When data is collated from these various sources, the disparity can be 
quantified and constructs emerge. Based on the degree of disparity, different statistical 
methods for harmonizing the constructs can be applied to minimize disparity. Finally, these 
constructs can be refactored into data that would allow investigators to perform comparisons 
across these datasets, thereby improving the value and extending the usability of the data in 
big data analyses.Wilde et al. Page 16
Brain Imaging Behav . Author manuscript; available in PMC 2022 April 01.
Author Manuscript Author Manuscript Author Manuscript Author Manuscript",nihms-1672220.pdf
"Neuron
NeuroView
Big Science, Team Science,
and Open Science for Neuroscience
Christof Koch1,*and Allan Jones1
1Allen Institute for Brain Science, Seattle, WA 98109, USA
*Correspondence: christofk@alleninstitute.org
http://dx.doi.org/10.1016/j.neuron.2016.10.019
The Allen Institute for Brain Science is a non-proﬁt private institution dedicated to basic brain science with an
internal organization more commonly found in large physics projects—large teams generating complete,accurate and permanent resources for the mouse and human brain. It can also be viewed as an experimentin the sociology of neuroscience. We here describe some of the singular differences to more academic,PI-focused institutions.
But the greatest paradox of the
sport has to do with the psycholog-ical makeup of the people who
pull the oars .Great crews may
have men or women of exceptionaltalent and strength; they may
have outstanding coxswains or
stroke oars or bowmen; but theyhave no stars. The team effort—
the perfectly synchronized ﬂow of
muscle, oars, boat and water; thesingle, whole, uniﬁed and beautifulsymphony that a crew in motion
becomes—is all that matters. Not
the individual, not the self.—DanielJames Brown ( The Boys in the
Boat )
The ﬁeld of biomedical science enjoys
worldwide prestige, notable triumphs,
and signiﬁcant funding. It has set itselfthe goal of understanding biological
organisms small and large, in health and
in disease. But the ﬁeld also faces twoacute and severe challenges. The ﬁrst—the vast complexity of organisms that it
seeks to understand—is a fundamental
feature of evolved organisms. The secondchallenge is the growing recognition
that many of the conclusions drawn
from biomedical research are unreli-able, and cannot be reproduced. This
replication crisis is pernicious and could
endanger the long-term public sup-port, particularly given the prevailinganti-intellectual and anti-expert political
environment.
Successfully tackling both challenges is
of the essence, in particular if the promise
underpinning the various international
brain projects to diagnose, ameliorate,and ultimately cure mental diseases and
disorders is to be realized in our lifetime.This will require complementing the
traditional, small laboratory culture that
has hitherto driven almost all biologicaldiscovery and that reward individual
investigators and their fecundity in pub-
lishing papers, with large, multi-disci-plinary teams working under highly repro-
ducible standards and making all of their
methods, data, and metadata publiclyavailable. This latter model is what theAllen Institute for Brain Science seeks to
achieve with its focus on Big Science,
Team Science, and Open Science. Wehere describe some of our experiences
and lessons learned.
Biomedical science seeks to answer
an array of diverse questions, such as de-
ciphering the fates of individuals from
their genomes, linking the microbiome tolifestyle and disease, diagnosing and
developing rational therapies for autism
spectrum disorders, and understandinghow the conscious mind emerges fromthe ﬂickering activity of a dizzying number
of heterogeneous nerve cells. Addressing
these questions requires the design,construction, and operation of an arma-
mentarium of sophisticated tools and
methods from a variety of scientiﬁc andtechnological disciplines, in addition to
software engineers and computer scien-
tists to grapple with the massive datastreams to extract and analyze key pa-rameters, and data scientists and theore-
ticians to ﬁt these data to digital simulacra
and theories. Biology has undergone adramatic transformation since Gregor
Mendel, the founder of genetics, used
gardening tools to discover the laws ofMendelian inheritance. Just 150 years
later, the Human Genome Project in-volved a far-ﬂung coalition of technicians
and scientists with their sophisticated
machinery. This singular project broughtthe era of Big Science to biology.
It was the construction of large particle
accelerators at the Radiation Laboratoryin Berkeley under Ernest Lawrence in the
1930s that gave birth to Big Science.
Growing up in the shadow of the Manhat-tan project, Big Science came of agepost-Sputnik, during the Cold War, fueled
by massive government programs. Its
rationalization was the discovery of un-known, exotic high energy particles
(‘‘new’’ physics) and the testing of speciﬁc
theories, such as the Standard Model atthe LHC or General Relativity at LIGO. In
astronomy, large telescopes and plane-
tary probes opened up new windowsinto the cosmos or investigated planets,
moons, or other denizens of the solar
system. This motto of Big Science as ac-cessing new frontiers and surveying thelandscape of possibilities carried into the
Human Genome Project and its follow-
up ENCODE project. This is also the spiritin which the Allen Institute views its contri-
bution to neuroscience.
Big Science is imposed by the com-
plexity of the phenomena investigated
and by the proliﬁcacy and intricacies of
modern instruments. Thus, developingtests to detect and therapies to stop andperhaps even reverse the ravages visited
upon an adolescent brain by schizo-
phrenia, one gifted professor workingwith her graduate student and post-
doctoral fellow in isolation will not tame
the vast beast that is the genome and
612 Neuron 92, November 2, 2016 ª2016 Elsevier Inc.",1-s2.0-S0896627316307206-main.pdf
"the brain (see, for instance, the long
author list in Sekar et al. 2016 ). Under-
standing how brains work in health, and
how they break down in disease, de-mands Big Science to complement the
traditional discovery process. Big Sci-
ence, though, is a signiﬁcant departurefrom the investigator-focused culture of
small and autonomous laboratories and
requires fresh thinking on how individualscientists are rewarded, teams are moti-
vated, and research is planned, paid for,
and executed.
Big Science
At the beginning of the millennium, Paul
Allen, co-founder of Microsoft, assem-bled a group of biologists to discuss the
future of neuroscience and what could
be done to accelerate its course. Fromthese meetings emerged the idea of
generating a complete atlas detailing
gene expression throughout the brain ofthe most popular mammalian model sys-
tem, the laboratory mouse. Funded by
an initial gift of $50 million by Mr. Allen, ateam of about 50 people accomplishedthis feat in 2006, under budget and on
time, using in situ hybridization for map-
ping the expression of 20,000 genes insectioned brain tissue. The annotated
data were, and continue to be, freely
available to anybody with an internetconnection at http://brain-map.org . This
effort was powered by a ‘‘structured
science’’ approach adopted from the
biotechnology industry, including roboticinstrumentation, standardized operating
procedures to reduce variability, high
quality control and quality assurancestandards and extensive program man-
agement, supplemented by external sci-
entiﬁc advisory boards.
The Allen Mouse Brain Atlas is now the
de facto standard for visualizing gene
expression patterns in the mouse brain,with more than 2,000 citations to theassociated publication ( Lein et al. 2007 )
and more than 15,000 unique monthly vis-
itors to the online resource. Indeed, itcould be argued that the Allen Mouse
Brain Atlas satisﬁes Sydney Brenner’s
famous CAP criteria pertaining to large-scale surveys: complete coverage of all
genes throughout the brain, accurate
probes (i.e., with high sensitivity andspeciﬁcity), and maintained as a curatedandpermanent resource.
Building on the success of the Allen
Mouse Brain Atlas, the Allen Institutereﬁned and adapted this high-throughput,
scalable, and robust platform to create
additional cartographies, comprehen-sively mapping transcriptional expression
patterns within a three-dimensional coor-
dinate system in the developing mousebrain ( Thompson et al., 2014 ), the adult
mouse spinal cord, the adult human brain
(Hawrylycz et al., 2012 ), the developing
human brain ( Miller et al., 2014 ), the adult
macaque brain ( Bernard et al., 2012 ), and
the developing macaque brain ( Bakken
et al., 2016 ). These massive online re-
sources are complemented by a meso-scopic mouse connectivity atlas ( Oh
et al., 2014 ) and a cellular-resolution
(1mm/pixel) annotated atlas of an adult
human brain ( Ding et al., 2016 ).
These large-scale and comprehensive
projects could not have thrived withinthe academic ecosystem, with its need
for rapid results and speciﬁc hypotheses
to sustain funding and multiple ﬁrst-auth-ored publications for participating grad-uate students to obtain their PhDs. The
constant reﬁnement and adjustment of
tools, instruments, and analysis methodsby multiple actors without documentation
is also a hindrance for big science pro-
jects. Of course, technology developmentpropels scientiﬁc discovery, but if the dy-
namics at which such improvements
occur are faster than the project timeline,the goal of obtaining a canonical and
reproducible product that can serve as a
gold standard becomes elusive.
We have learned several valuable les-
sons over the years. Critical to success
is testing competing technologies and in-
struments under realistic conditions toassess their robustness and reliability
and insisting on ﬁrm deadlines. Bringing
a technology platform from proof-of-
concept to maturity requires planning for
an extended time course. Finally, it is
important to resist the siren song ofconstantly switching to the latest ‘‘hot’’method in favor of staying the course
with a well-proven, older technology until
large-scale data production is completed(Figure 1 ).
Team Science
Large science projects require a highlyspecialized workforce that works together,
day after day, hand-in-glove, across the
organization. Consider the team thatoperates our latest online offering, theAllen Brain Observatory. Its goal is to
record the cellular level activity of thou-
sands and ultimately millions of neuronsin functionally identiﬁed regions of the
mouse brain as the animal is engaged in
a stereotyped behavior. Our ﬁrst release
Figure 1. Life Cycle of a Generic Technology in Basic Science
An important lesson we have learned many times is the need to pick state-of-the-art instrumentation
technologies that are sensitive and reliable, with well-understood operational procedures that can operate
stably over years for our large neuroscience data products. This may preclude adopting the latest cutting-edge technique that has just been published as a proof of principle. This is schematically illustrated with
the time line for some of the canonical events in the life cycle of some relevant technology. The ordinate
plots penetration (usage and adoption by the science community) in red and quality and reproducibility of
the technology in blue.Neuron
NeuroView
Neuron 92, November 2, 2016 613",1-s2.0-S0896627316307206-main.pdf
"(http://observatory.brain-map.org/ ) in the
open-source Neurodata Without Borders
data format ( Teeters et al., 2015 ) features
the calcium activity of more than 30,000individual neurons responding to a battery
of visual stimuli, including static and
moving gratings, sparse noise, naturalscenes, and the movie Touch of Evil . Activ-
ity is recorded via two-photon microscopy
in primary visual cortex and nearby regions
in different transgenic mice and mappedto a high-resolution neuroanatomical coor-
dinate framework. Together with associ-
ated metadata, including eye movements,video of the running mice and so on, this
ﬁrst dataset exceeds 30TB.
The team that built and operates the
Allen Brain Observatory numbers onehundred specialists and technicians (not
all of whom work full-time on this
project)—technicians to care for and trainthe animals, neurobiologists to plan and
execute the behavioral experiment, elec-
trical and optical engineers to constructand maintain the microscopes to identify
functional regions and image cellular-
level activity, neurosurgeons to preciselyplace transparent windows into murine
skulls, mechanical engineers to build the
gimbals, reticules, and other widgets toreproducibly return to the same set ofneurons over multiple trials, anatomists
and annotators to localize responses
within the 3D brain, software engineersto harness and massage the massive
data stream from all instruments into a
common laboratory information manage-ment system, data scientists to extract
the tiny fraction of relevant information,
and modelers and theoreticians to carryout statistical analyses and build neuralnetwork and other models to analyze
and replicate the data. Four years in the
making, the project has been fully fundedby the generosity of one individual, Paul
Allen.
The difﬁculties in operating such an
observatory are not just those asso-
ciated with setting up any advanced
instrumentation suite, but also includesigniﬁcant sociological and organizational
challenges.
We all treasure a sense of autonomy.
Yet inherent to any team, whether oper-ating on the battle ﬁeld, on the water rac-
ing a scull, in a start-up, or at an institute
with production deadlines, is the impera-tive to align on the agreed-upon goalsand the attendant need to submerge the
ego, the self, to the group as a whole.
These conﬂicting demands have impor-
tant implications for internal decisionmaking and highlight the absolute need
for the development and maintenance of
trust across the entire team. Achievingthese goals requires a sophisticatedmeeting culture (e.g., taking minutes, spe-
ciﬁc agenda, follow-up, starting and
ending on time, no distracting smart-phone or laptop usage, and so on). Regu-
lar team meetings are the most visible
difference to academic culture.
These challenges grow as the size of
the team grows. Our anecdotal evidence
suggests that above a hundred members,group cohesion appears to becomeweaker with the appearance of semi-
autonomous cliques and sub-groups.
This may relate to the postulated limit onthe number of meaningful social interac-
tions humans can sustain given the size
of their brain ( Dunbar, 1992 ).
Teams are built on the strengths and
abilities of their members. While alpha-
type personalities pursuing their ownidea thrive in an academic setting, we
are more dependent on team players
with a highly cooperative style of giveand take in an environment that unleashestheir energy and creativity.
Within this team-oriented context, how
should individual contributors, on whosecreativity, brilliance, dedication, disci-
pline, and hard work the entire enterprise
rests, be rewarded? As a non-proﬁt insti-tution, we do not offer stock options or
outsized salaries to reward strong per-
formers. Likewise, the promise of ﬁrstor senior authorship only acts as aweak draw given the large number of
contributors.
This means that internal motivators are
key—team members value the knowl-
edge that they are participating in an his-
toric mission at the frontier of sciencethat will unearth new knowledge to the
beneﬁt of all of humankind.
Such considerations require a judicious
recruitment process, ample opportunities
for growth, and promotion within the or-
ganization to reward performance, thenurturing of a sense of being part of some-thing larger than oneself, and distinct
tracks for employees interested in careers
in basic research (scientists I, II, seniorscientist, and investigators), structuredscience (managers and directors of
distinct rankings), or science manage-
ment (project and program managers).
Fortunately, the physics community
has shown the way by assembling colos-
sal teams. The two independent collabo-
rations at the Large Hadron Collider(LHC) at CERN in Geneva that success-fully hunted for the Higgs Boson—CMS
and ATLAS—each has about 3,000 par-
ticipants with elaborate, point-basedrules for authorship and who can speak
for the group. The authors list of the
LIGO consortium publication announcingthe ﬁrst detection of gravity waves
emitted by two merging black holes was
strictly alphabetical and included about
a thousand individuals from 133 institu-tions ( Abbott et al., 2016 ). Note that these
large organizations happily and produc-
tively coexist with traditional academia.Thus, while there can be upward of
13,000 people on site at CERN in Geneva,
only about 2,200 of these are CERN em-ployees; the rest are subcontractors, stu-
dents, fellows, and visiting faculty.
We have learned from these and other
organizations that to build a well-func-
tioning team with a strong esprit de corps,
it is critical to jointly and cooperativelyalign on speciﬁc common goals, buildtrust by open and transparent decision-
making, a maximal sharing of both re-
sponsibilities and credit, and nourishmorale by a dense web of formal and
informal means of communication. We
have also learned the importance ofcompromise among conﬂicting demands
and experts from different scientiﬁc
traditions. This is really nothing but theart of the achievable under time andbudgetary constraints—succinctly ex-
pressed by Otto von Bismarck’s dictum,
‘‘Politics is the art of the possible.’’
Open Science
From the early days of the Institute, we
have made the fruits of our investigationsavailable to anybody with an internet
connection through our repository at
http://brain-map.org . These massive re-
sources totaling more than 3,000 Tera-bytes of data are actively maintained
and curated. No login or registration
step is needed to browse, search, view,or download any of this wealth of informa-
tion via our dedicated web tools. Data re-
leases occur well ahead of the publication
614 Neuron 92, November 2, 2016Neuron
NeuroView",1-s2.0-S0896627316307206-main.pdf
"of associated platform papers. Our cur-
rent 10-year plan calls for continual thrice
yearly releases of data. We also share
white papers describing in minute detailour operating procedures and methods
and many of our tools, such as transgenic
animals and viruses (e.g., Madisen et al.,
2010 ). Indeed, The Jackson Laboratory
has shipped more than 20,000 of our
transgenic mice to customers worldwide.
Whydo we pursue such an ‘‘information-
wants-to-be-free’’ policy? The simplest
answer is that this is the vision of Paul
Allen. His intent is that Institute resourcesshould accelerate neuroscience discov-
ery. He also believes that the Institute
should act as an example of what everylaboratory should be doing.
For millennials, communicating via texts
and images is part of their social onlineexperience growing up. Thus, it comesnatural to young scientists to freely and
openly share data, computer code, and
manuscripts. Indeed, open science hasbeen advocated for and practiced by a
small number of neuroscientists for more
than a decade (e.g., Van Horn and Gazza-
niga, 2013; Glasser et al., 2016 ). Regret-
tably, however, the vast majority of data
and metadata in the neurosciences con-tinues to remain inaccessible.
It is urgent that the ﬁeld as a whole
move toward an open science policy, as
it will alleviate some of the root causesof the replication crisis so pervasive in
biomedical research. A recent review on
this topic ( Button et al., 2013 ) opens with
an eye-catching, ‘‘It has been claimed
and demonstrated that many (and
possibly most) of the conclusions drawnfrom biomedical research are probably
false.’’ Estimates for the fraction of false
ﬁndings range from a simple majority to
80% and higher ( Open Science Collabo-
ration, 2015 ). Given that replication is
one of the key steps in the scientiﬁc pro-
cess, its systematic failure for so manypublished ﬁndings constitute a strikingdeparture from good scientiﬁc practice
that could come back to haunt the ﬁeld.
This cannot be eluded by blithely ignoringit as most scientists are wont to do.
There are two broad categories of
causes for the replication crisis.
First, even simple organisms have
vastly more degrees of freedom than the
natural, non-evolved systems physiciststypically deal with, such as elementary
particles, gravity waves, or exo-planets.
Thus, while a Higgs Boson has the samesignature no matter where on Earth it isdetected, a C57BL/6J mouse in Seattle
will not be the same as a laboratory
mouse in Boston ( Crabbe et al., 1999 ).
What is true for standard breeds of mice
kept under controlled conditions is vastly
more so for ‘‘neuro-typical’’ volunteersor patients. Mitigating this problem will
not be easy but must include enforcing,
to the maximal extent possible, standard-ization of model systems, procedures,tools, and instruments, as well as meticu-
lously publicizing all logistical and meth-
odological details. Unbiased surveyseliminate some sources of these statisti-
cal infelicities and have found widespread
adoptions in some ﬁelds. For example,within astronomy, generations of Sloan
Digital Sky surveys have resulted in more
than 5,000 refereed publications and 200million SQL queries to the relevant data-
base and revolutionized the ﬁeld in the
process ( Burns et al., 2014 ).
A second set of causes relate to lack of
statistical ‘‘hygiene,’’ including low statis-
tical power, incorrect noise models, pub-
lication bias (or ‘‘the ﬁle drawer problem’’),hypothesizing after the results are known(HARKing), and p-hacking. These phe-
nomena have been much commented
upon ( Ioannidis 2005; Button et al. 2013 ).
The latest scandal follows the re-analysis
of resting-state fMRI data from the public
database Functional Connectomes Proj-
ect(Eklund et al., 2016 ), which revealed
that widely used analysis software pack-
ages yield false-positive rates of up to70%. This may affect more than 3,000
published fMRI studies, funded at great
expense by the public purse.
There is no question that making all
data and metadata, together with all
computational and statistical procedures
of every publication freely and publicly
available would address some of these
shortcomings (see Box 1 ). Such a policy
allows researchers to easily test whetherthe published conclusions are valid,
particularly when they conﬂict with other
studies. Given the potential threat to thereputation of the authors, Open Scienceis likely to lead to statistically more valid
results than otherwise. Open access
also allows for analysis of published datausing different algorithms and assump-
tions to gain additional or alternative in-
sights, or to ask questions that were notconceived by the original authors. Finally,
open data sharing enables the aggrega-
tion and contrasting of data acrossBox 1. Open Science Recommendation
To accelerate the rate of discovery and to ameliorate the replication crisis, it is imperative that the neuroscience community movestoward an open science ethic. After all, almost all basic science is funded, directly or indirectly, by citizens via their taxes. There-fore, the fruits of these labors ought to be publicly, freely, and widely shared. We offer some speciﬁc suggestions.
dThe detailed code for the complete analysis of data and statistical procedures should accompany every publication. The
most convenient form is a jupyter notebook ( https://jupyter.org/ ), a Python-based web application for the creation and
sharing of documents that contains live code, equations, ﬁgures, and explanatory text. This allows researchers to easilyreplicate—and vary—the conclusion of the study. This imposes a low burden on authors.
dWe do know from our own experience that making data freely available imposes a signiﬁcant burden (for a thoughtful dis-
cussion of the costs and the beneﬁts, see Choudhury et al. 2014 ). Data and relevant metadata need to be formatted into
a common data format and placed online. This repository needs to be curated. Increasingly, funding agencies are receptiveto such initiatives and the attendant costs. With rare exceptions for singular ﬁndings, the era of illustrating discoveries via
nothing but a ﬂat PDF ﬁle, with ‘‘representative results’’ that are often the most expected or cleanest responses, ought to
be coming to a close.
Neuron 92, November 2, 2016 615Neuron
NeuroView",1-s2.0-S0896627316307206-main.pdf
"multiple studies for meta-analyses to
extract broader insights. Quite simply,
Open Science is the right thing to do.
The Allen Institute for Brain Science has
proven that Big Science, Team Science,
and Open Science can be harnessed to
create extraordinary neuroscientiﬁc re-sources that beneﬁt all. With bright eyes,we look toward a future in which we,
together with the world-wide community
of researchers from individual labora-tories at universities and independent
research institutes, will decipher the
most highly organized piece of excitablematter in the known universe: the human
brain.
ABOUT THE AUTHORS
Christof Koch is the President and Chief Scientiﬁc
Ofﬁcer of the Allen Institute for Brain Science. Afterspending 27 years as a faculty member at the Cal-
ifornia Institute of Technology, he joined the Insti-
tute in 2011. He seeks to understand the workingsof the mammalian cerebral cortex and how it gen-erates conscious experience. Allan Jones is thePresident and Chief Executive Ofﬁcer of the AllenInstitute. He joined the Institute from biotech/pharma at the beginning of the Allen Brain Atlasproject in 2003 and is passionate about under-standing complexity in biology.
ACKNOWLEDGMENTS
We thank Anton Arkhipov, Mike Hawrylycz, Han-
nah Krakauer, Ed Lein, John Phillips, Erica Sessle,
and Hongkui Zeng for thoughtful comments andfeedback. None of this work could have been
accomplished without the vision and the enduringsupport of our founders, Paul G. Allen and Jody
Allen. A.J. is the head of the overall Allen Institute,
and C.K. is the head of its daughter, the Allen Insti-
tute for Brain Science.
REFERENCES
Abbott, B.P., Abbott, R., Abbott, T.D., Abernathy,
M.R., Acernese, F., Ackley, K., Adams, C., Adams,
T., Addesso, P., Adhikari, R.X., et al.; LIGO Scien-
tiﬁc Collaboration and Virgo Collaboration (2016).Phys. Rev. Lett. 116, 061102 .
Bakken, T.E., Miller, J.A., Ding, S.L., Sunkin, S.M.,
Smith, K.A., Ng, L., Szafer, A., Dalley, R.A., Royall,J.J., Lemon, T., et al. (2016). Nature 535, 367–375 .
Bernard, A., Lubbers, L.S., Tanis, K.Q., Luo, R.,
Podtelezhnikov, A.A., Finney, E.M., McWhorter,
M.M., Serikawa, K., Lemon, T., Morgan, R., et al.
(2012). Neuron 73, 1083–1099 .
Burns, R., Vogelstein, J.T., and Szalay, A.S. (2014).
Neuron 83, 1249–1252 .
Button, K.S., Ioannidis, J.P., Mokrysz, C., Nosek,
B.A., Flint, J., Robinson, E.S., and Munafo `, M.R.
(2013). Nat. Rev. Neurosci. 14, 365–376 .
Choudhury, S., Fishman, J.R., McGowan, M.L.,
and Juengst, E.T. (2014). Front. Hum. Neurosci.
8, 239 .
Crabbe, J.C., Wahlsten, D., and Dudek, B.C.
(1999). Science 284, 1670–1672 .
Ding, S.-L., Royall, J.J., Sunkin, S.M., Ng, L.,
Facer, B.A., Lesnar, P., Guillozet-Bongaarts, A.,McMurray, B., Szafer, A., Dolbeare, T.A., et al.
(2016). J. Comp. Neurol. 524, 3127–3481 .
Dunbar, R.I.M. (1992). J. Hum. Evol. 22, 469–493 .
Eklund, A., Nichols, T.E., and Knutsson, H. (2016).
Proc. Natl. Acad. Sci. USA 113, 7900–7905 .
Glasser, M.F., Smith, S.M., Marcus, D.S., Ander-
sson, J.L., Auerbach, E.J., Behrens, T.E., Coalson,T.S., Harms, M.P., Jenkinson, M., Moeller, S., et al.
(2016). Nat. Neurosci. 19, 1175–1187 .
Hawrylycz, M.J., Lein, E.S., Guillozet-Bongaarts,
A.L., Shen, E.H., Ng, L., Miller, J.A., van de Lage-
maat, L.N., Smith, K.A., Ebbert, A., Riley, Z.L.,et al. (2012). Nature 489, 391–399 .
Ioannidis, J.P.A. (2005). PLoS Med. 2, e124 .
Lein, E.S., Hawrylycz, M.J., Ao, N., Ayres, M., Ben-
singer, A., Bernard, A., Boe, A.F., Boguski, M.S.,
Brockway, K.S., Byrnes, E.J., et al. (2007). Nature
445, 168–176 .
Madisen, L., Zwingman, T.A., Sunkin, S.M., Oh,
S.W., Zariwala, H.A., Gu, H., Ng, L.L., Palmiter,
R.D., Hawrylycz, M.J., Jones, A.R., et al. (2010).
Nat. Neurosci. 13, 133–140
.
Miller, J.A., Ding, S.L., Sunkin, S.M., Smith, K.A.,
Ng, L., Szafer, A., Ebbert, A., Riley, Z.L., Royall,
J.J., Aiona, K., et al. (2014). Nature 508, 199–206 .
Oh, S.W., Harris, J.A., Ng, L., Winslow, B., Cain, N.,
Mihalas, S., Wang, Q., Lau, C., Kuan, L., Henry,
A.M., et al. (2014). Nature 508, 207–214 .
Open Science Collaboration (2015). Science 349,
http://dx.doi.org/10.1126/science.aac4716 .
Sekar, A., Bialas, A.R., de Rivera, H., Davis, A.,
Hammond, T.R., Kamitaki, N., Tooley, K., Presu-
mey, J., Baum, M., Van Doren, V., et al.; Schizo-
phrenia Working Group of the Psychiatric Geno-
mics Consortium (2016). Nature 530, 177–183 .
Teeters, J.L., Godfrey, K., Young, R., Dang, C.,
Friedsam, C., Wark, B., Asari, H., Peron, S., Li,
N., Peyrache, A., et al. (2015). Neuron 88, 629–634 .
Thompson, C.L., Ng, L., Menon, V., Martinez, S.,
Lee, C.K., Glattfelder, K., Sunkin, S.M., Henry, A.,
Lau, C., Dang, C., et al. (2014). Neuron 83,
309–323 .
Van Horn, J.D., and Gazzaniga, M.S. (2013). Neu-
roimage 82, 677–682 .
616 Neuron 92, November 2, 2016Neuron
NeuroView",1-s2.0-S0896627316307206-main.pdf
"Methodology and Research Practice 
A Roadmap to Large-Scale Multi-Country Replications in        
Psychology  
Hannes Jarke 1,2
  a
 , Shaakya Anand-Vembar 2,3
  , Shilaan Alzahawi 4
  , Thomas Lind Andersen 2,5
  , Lana Bojanić 6
  ,
Alexandra Carstensen 7
  , Gilad Feldman 8
  , Eduardo Garcia-Garzon 2,9
  , Hansika Kapoor 10,11
  ,
Savannah Lewis 12,13
  , Anna Louise Todsen 2,14
 , Bojana Većkalov 2,15
  , Janis H. Zickfeld 16,17
  , Sandra J. Geiger 2,18
 
1 Centre for Business Research, Judge Business School, University of Cambridge, Cambridge, UK, 2 Junior Researcher Programme, Cambridge, UK, 
3 Department of Psychiatry, School of Medicine, Trinity College Dublin, Ireland, 4 Graduate School of Business, Stanford University, Stanford, CA, US, 
5 Child and Adolescent Mental Health Center, Copenhagen University Hospital, Mental Health Services Copenhagen, Copenhagen, Denmark, 
6 Department of Psychology and Mental Health, University of Manchester, Manchester, UK, 7 Psychology Department, University of California, San 
Diego, La Jolla, CA, US, 8 Psychology Department, University of Hong Kong, 9 School of Health, Camilo José Cela University, Villafranca del Castillo, 
Madrid, Spain, 10 University of Connecticut, Storrs, CT, USA, 11 Department of Psychology, Monk Prayogshala, Mumbai, India, 12 Psychological Science 
Accelerator, 13 International Collaboration Research Center, Ashland University, Ashland, OH, US, 14 School of Psychology and Neuroscience, Faculty 
of Arts, University of St Andrews, St Andrews, UK, 15 Department of Psychology, University of Amsterdam, Amsterdam, The Netherlands, 16 Centre for 
Integrative Business Psychology, Aarhus University, Aarhus, Denmark, 17 Department of Management, Aarhus University, Aarhus, Denmark, 
18 Environmental Psychology, Department of Cognition, Emotion, and Methods, Faculty of Psychology, University of Vienna, Vienna, Austria 
Keywords: Replication, Multi-country, Psychology, Big Team Science, Underrepresented Populations, Research Methodology, Reproducibility, 
International Research 
https://doi.org/10.1525/collabra.57538 
Collabra: Psychology 
Vol. 8, Issue 1, 2022 
Classic findings from psychology and the behavioural scienc es are increasingly being 
revisited. Methodological and technological advanc es provide opportunities to replicate 
studies across a wide range of countries and settings to investigate whether these 
findings are universally applicable, limited to specific countries, or vary in magnitude 
depending on settings. Researchers from around the world connec t to revisit such 
findings collaborativ ely, adapt the original design to the Zeitgeist, integrate new 
knowledge to impro ve statistical analyses, and broaden the scope by testing effects 
globally – or at least in as many countries, as budget and feasibilit y allow. We currently 
observe multiple international consortia conduc ting large-scale multi-c ountry 
replications. How do such collaborations form and how do they approach these comple x 
investigations? This paper brings together researchers from different initiativ es that 
conduc t replications on an international scale to outline approaches and summarises 
what we have learned in applying them: Junior Researcher Programme (JRP), 
Psychological Scienc e Accelerator (PSA), ManyBabies, Collaborativ e Open-scienc e 
REsearch (CORE), and International Study of Metanorms (ISMN). We describe different 
ways for study selection, methodological approaches, statistical analyses, ethical issues, 
and most importantly , how the different collaborations formed and how team 
communication worked. We look in detail at challenges of including typically 
underrepresented countries in psychological scienc e, not only in terms of data collection 
but also in making it possible for local researchers to contribute. This paper provides a 
structured insight into how different collaborations work and issues to consider for 
anyone who seeks to conduc t a multi-c ountry replication in psychology , or looking for 
additional perspec tives to their existing plan. We close the article with a checklist built as 
a helpful tool for colleagues putting together their study protocols for such efforts – and 
invite them to collaborativ ely expand it in the future. 
Introduction  
The replication and reproducibilit y crisis (Pashler & Wa-
genmak ers, 2012; Wiggins & Christopherson, 2019) has 
eroded trust in classic findings from psychology and be-havioural scienc es (Anvari & Lakens, 2018; Wingen et al., 
2020). Revisiting such findings and investigating whether 
widely cited and applied concepts can be replicated and 
generalised across time, context, and countries is more and 
more visible and encouraged by researchers and journals. 
Correspondence should be addressed to Hannes Jarke, hj318@cam.ac.uk a 
Jarke, H., Anand-Vembar, S., Alzahawi, S., Andersen, T. L., Bojanić, L., Carstensen, A.,
Feldman, G., Garcia-Garzon, E., Kapoor, H., Lewis, S., Todsen, A. L., Većkalov, B.,
Zickfeld, J. H., & Geiger, S. J. (2022). A Roadmap to Large-Scale Multi-Country
Replications in Psychology. Collabra: Psychology ,8(1).
https://doi.org/10.1525/collabra.57538Downloaded from http://online.ucpress.edu/collabra/article-pdf/8/1/57538/767427/collabra_2022_8_1_57538.pdf by guest on 04 January 2023
",A_Roadmap_to_Large-Scale_Multi-Country_Replication 3.pdf
"Methodological and technological advanc es, as well as im-
provements in data collection and management practices 
can now be utilised to update our knowledge about theory 
and findings and expand further to reach populations 
around the globe. These advanc es provide new opportu -
nities for initiativ es that are sometimes labelled Big Team 
Science: collaborations “involving a relativ ely large number 
of collaborators who may be dispersed across labs, institu -
tions, disciplines, cultures, and continents” (Forscher et al., 
2020, p. 2). For example, to test their 1979 Nobel prize-win-
ning Prospect Theory, Daniel Kahneman and Amos Tversky 
relied on a small random sample of students from the US, 
Sweden, and Israel. Using collaborativ e online tools, Rug-
geri and colleagues (2019)  recently replicated patterns of 
prospec t theory across 19 countries and in 13 languages 
with over 4,000 participants. This projec t was facilitated 
with the help of the Junior Researcher Programme (JRP), 
an initiativ e that provides opportunities for early-career 
researchers in psychology (jrp.pscholars.org ; Jarke, 2021; 
Ruggeri, 2020). Multi-c ountry replication initiativ es that 
started earlier include the Open Science Collaboration 
(2012), ManyLabs (e.g., Ebersole et al., 2014; Klein et al., 
2014), or the Collaborative Replications and Education Pro-
ject (CREP; Grahe et al., 2013). Another approach to such 
multi-c ountry replications is facilitated by the Psychological 
Science Accelerator (PSA; Beshears et al., 2020; Moshont z 
et al., 2018), an organisation focused on producing psycho-
logical scienc e that is generalizable and reliable by utilising 
their distributed network of labs. 
In this paper, we aim to synthesise insights gained from 
several multi-c ountry collaborations. In two online panel 
discussions (Anand-V embar et al., 2021; Jarke et al., 2021), 
we brought together researchers who participated in and 
organised multi-c ountry replication efforts to discuss key 
aspects of such undertakings (see Table 1 for an overview). 
Based on these discussions, we summarise structured in-
sights and experienc es with different approaches on key 
topics, including study selection, replication criteria, trans-
lation, selection of potential collaborators, team commu -
nication, data collection methods and privacy issues, gain-
ing ethical approval for multiple countries, and statistical 
approaches. We also address the inclusion of underrep -
resented countries and outline hurdles in including col-
leagues from these areas. We conceptualised this paper as 
a roadmap to support researchers planning a multi-c ountry 
replication so that they can incorporate what has been 
learned in previous attempts and tailor successful ap-
proaches to their lab’s situation. Some consortia already 
have compiled information on their approaches in detail. 
These are included in the accompanying reading list to this 
paper (Appendix 1). We discuss our insights chronologi -
cally, starting at the beginning of a study until publication 
and beyond. In addition, we compiled a checklist of lessons 
learned (Appendix 2). This checklist is not a comprehensiv e “must-have” but rather a tool that summarises considera -
tions for preparing a multi-c ountry replication. Researchers 
already involved in such efforts may also find suggestions 
and insights that can strengthen their existing approaches 
or add a perspec tive they had not previously considered. 
While this paper primarily focuses on replications including 
large numbers of labs and researchers, many insights are 
also applicable to replications within fewer countries or 
settings. 
Building a Consortium: Identifying Collaborators      
and Team Communication    
Perhaps the easiest way to build a consortium1
 of col-
laborators is to draw on existing networks. Uhlmann et al. 
(2019)  describe multi-lab collaborations along two axes, 
one being inclusiv eness versus selectivity (i.e., who can 
participate and make decisions), the other being communi -
cation (i.e., is there constant collaboration, or does the PI 
“collect” and synthesise the work of otherwise mostly inde-
pendent units). We outline our own experienc es below. 
The JRP draws on its network of programme alumni and 
current team members, with current programme interns 
usually leading a country team in collaboration with stu-
dents participating in GLOBES - a programme at Columbia 
University, which connec ts undergraduate students in be-
havioural scienc es to researchers. The majorit y of collab-
orators are early-career researchers (ECRs; here: students 
and postdocs who make up small projec t teams each year 
but often go on to participate in larger collaborations), 
whereas the projec t itself is led by an established academic. 
JRP interns and alumni can be integrated into projec ts 
without further screening, as a working relationship and 
trust are already established through them having partici -
pated in a summer school and a one-year research projec t 
supported by the programme at that point. Participating re-
searchers are briefed usually five to six months before activ-
ities commenc e and learn about their responsibilities, the 
projec t timeline, and how communication will be facilitated 
(Ruggeri, 2020). Shortly before and during data collection, 
communication is facilitated via daily briefing emails with 
updates, key developments, and reminders. These are sup-
ported by group calls and a slack channel for discussion be-
tween country teams as well as an FAQ page for providing 
important information across country teams. While the 
first JRP-organised multi-c ountry replication (Ruggeri et 
al., 2020) was conduc ted with most contributors on-site in 
one place, the two following projec ts have been conduc ted 
online (Ruggeri et al., 2021, 2022) due to the COVID-19 
pandemic. Although online work clearly allowed for more 
contributors, it also constitutes a much higher workload for 
the projec t leaders and administrators. Further , with the 
JRP’s contributors being mostly ECRs, the in-person ap-
proach also provided a unique and motivating experienc e 
We refer to a consortium in this context as any formal or informal collaboration between multiple labs 1 A Roadmap to Large-Scale Multi-C ountry Replications in Psychology
Collabra: Psychology 2Downloaded from http://online.ucpress.edu/collabra/article-pdf/8/1/57538/767427/collabra_2022_8_1_57538.pdf by guest on 04 January 2023
",A_Roadmap_to_Large-Scale_Multi-Country_Replication 3.pdf
"Table 1. Contributors who Participated in the Panel on Multi-Country Replication Studies.           
Consortium Authors Rele vant e xamples of work related to this topic 
Psy chological 
Science 
Acceler ator 
(PSA) Shilaan Alzaha wi, Sa vannah 
Lewis, Janis Zickfeld Moshontz et al. (2018) , Beshears et al. (2020) 
Junior 
Researcher 
Progr amme 
(JRP) Hannes Jark e, Lana Bojanić, 
Eduardo Garcia-Garzon, Bojana 
Većkalo v, Sandr a J. Geiger Ruggeri et al. (2020) , Ruggeri et al. (2021) , Ruggeri et al. (2022) 
Man yBabies Alexandr a Carstensen Man yBabies Consortium (2020) , By ers-Heinlein et al. (2020) , By ers-
Heinlein et al. (2021) , Tsui et al. (2021) ; see also 
https:/ /man ybabies.github.io/ 
Collabor ativ e 
Open-science 
REsearch 
(CORE) Gilad F eldman Efendić et al. (2022) , Ziano, et al. (2021) , Chandr ashekar , et al. (2021) , 
Chen et al. (2021) , Brick et al. (2021) , An vari et al. (2021) , see also 
Collabor ativ e Open-science REsearch (2022) 
International 
Study of 
Metanorms 
(ISMN) Hansika Kapoor Eriksson et al. (2021) 
and ECRs benefit more by forming stronger relationships 
with international colleagues. 
CORE works with a different model in which the Princi-
pal Investigator (PI) conduc ts all replications with students 
as part of the coursework in a one-semester course, publicly 
sharing each stage of replication, and inviting international 
involvement and collaborations throughout. Once projec ts 
are finalised at the end of the semester , ECRs are publicly 
invited to take the lead over the completed projec ts, veri-
fying and extending the available outputs, and then help-
ing bring these to publication in journals. The PI works with 
each ECR on their led projec ts, and over time, with gained 
experienc e, the ECRs support each other on specific issues 
or initiativ es, and join forces in tackling new projec ts. 
The PSA represents a globally distributed network of 
labs located on all populated continents (see https://psysci -
acc.org/map/  for an updated map of all labs) organising 
data collection of democratically chosen projec ts 
(Moshont z et al., 2018). The network includes one director 
and four associate directors that are elected by all members. 
In addition, the network is coordinated by several commit -
tees focusing on different aspects of the study process such 
as study selection, ethics, translation and cultural diversity, 
communit y building, projec t monitoring, data and meth-
ods, training, and funding. The network regularly issues 
calls for projec t submissions from both inside and outside 
the network, which are first screened by the selection com-
mittee, then sent out for review by at least three experts, 
and finally voted on by all PSA members. Once a projec t has 
been successfully selected, PSA members are able to sign 
up for contributing to the projec t by collecting data, trans-
lating questionnaires or coordinating aspects of the pro-
ject. All researchers can sign up to become a PSA mem-
ber. Theref ore, the PSA focuses both on inclusiv eness and a 
mode of constant collaboration. 
ManyBabies is a network of developmental psychologists 
distributed across a range of large-scale replication pro-jects. New members opt in to the network and join individ -
ual projec ts through the organization’ s website. The gov-
erning board reviews new projec t proposals and advertises 
approved projec ts to the general listser v, where collabora -
tors can volunteer to contribute to any and all parts of a 
projec t, including study design, data collection, analysis, 
and writing. The organization also has several dedicated 
positions, including an executiv e director, office assistant, 
and postdocs who support the range of ongoing studies. 
ManyBabies is commit ted to open scienc e and uses a con-
sensus-based approach for all projec t decisions to promo te 
transparency , inclusivit y, diversity, and collective gover-
nance. 
Authorship and credit    
No matter how a consortium is established, it is advan -
tageous if everyone’s responsibilities and the consequenc es 
of inaction are clear from the beginning. These responsi -
bilities should ideally be listed in the final output (e.g., 
as a Contributor Role Taxonomy , or short CRediT; Allen 
et al., 2019; Holcombe et al., 2020, adopted also by the 
PSA) to transparently report what each author has con-
tributed. To set expectations, it is also advisable to decide 
on the order of authors (or principles for determining au-
thor order) as early as possible and be clear on who will 
be responsible for article processing fees. For instanc e, the 
PSA has specific roles for each projec t (e.g., data man-
agement, translation coordinator , ethics coordinator) and 
these roles typically qualify for different authorship tiers; 
but also adapts criteria on a case-b y-case basis beforehand. 
ManyBabies has a guiding example for determining au-
thorship (https://manybabies.github.io/authorship ) but the 
leadership team for each new projec t is responsible for 
defining and communicating authorship guidelines. Simi-
larly, CORE emplo ys a detailed guide for determining au-
thorship (https://mgto.org/joinmassreplication ). A Roadmap to Large-Scale Multi-C ountry Replications in Psychology
Collabra: Psychology 3Downloaded from http://online.ucpress.edu/collabra/article-pdf/8/1/57538/767427/collabra_2022_8_1_57538.pdf by guest on 04 January 2023
",A_Roadmap_to_Large-Scale_Multi-Country_Replication 3.pdf
"We suggest that authorship be granted to all collabora -
tors who contribute substantially to any crucial parts of the 
study. A collaboration agreement that transparently out-
lines the criteria for co-authorship as well as how author -
ship order is determined can ensure expectations on this 
topic are aligned. Authorship may act as a strong indi-
vidual, and career incentive to academic collaborators, in-
creases the public representation of researchers from mar-
ginalised backgrounds, and shows that contributions are 
credited and valued. In terms of administration, it should 
also be kept in mind that typing in all author names and 
data for submission can take many hours of work. There-
fore, we urge journals to implement systems that allow for 
easier ways of providing such data, such as the option of 
filling in template spreadsheets that can be read and copied 
by the submission system. Big Team Scienc e projec ts may 
run into other unexpected issues regarding authorship: For 
example, in a recent submission, the PSA Self-Determina -
tion Theor y Collaboration (2022)  could not list all 500 au-
thors on the byline because this would have exceeded the 
journal’ s maximum page count. Instead, consortium au-
thorship was adapted, which especially disadvantaged re-
searchers from low-and middle-inc ome countries and early 
career researchers: Institutional policies often do not rec-
ognize this type of authorship (for an extensiv e discussion 
about disadvantages resulting from group authorship and a 
call for change, see PSA’s Open Letter2
). 
Study Selection   
The process of deciding which study to replicate can vary 
substantially from lab to lab and consortium to consortium. 
Approaches can take various forms from where the PI or 
their lab decide which study to replicate, how to do it, and 
whom to include (top-down) to consortia forming through 
loose ideas and developing goals and structure from the 
bottom-up. 
Within the PSA, several labs or PIs submit a study pro-
posal, the study selection commit tee checks its feasibilit y, 
the network votes on the blinded proposal, then the lead 
team of the accepted studies pre-registers the experiment, 
and publicly opens it up for potential collaborators. There 
are also more bottom-up approaches to study selection. For 
instanc e, the ManyBabies Consortium accepts study pro-
posals from any researcher , which are then evaluated by the 
ManyBabies Governing Board. Accepted proposals are ad-
vertised to ManyBabies Consortium members and through 
related organisations, enabling interested researchers and 
labs to join new projec ts. Similarly , CREP starts projec ts 
through collaborators expressing interest first. The study 
Director looks for top-cited studies in a range of sub-dis -
ciplines of psychology . After being checked for feasibilit y 
by established researchers, feasible studies are sent to stu-
dents and collaborators to rate on different aspects, such as 
feasibilit y and interest. The CREP administrator then takes those ratings and identifies the next studies for collabo -
ration. After a study-specific admin team is put together 
for each study, labs are allowed to sign on to the projec t. 
On the other hand, replications conduc ted by the JRP have 
started with a small team around the PI deciding on the 
topic and detailed study plan first, before recruiting collab-
orators. 
Feasibilit y and durability: Is this topic suitable        
for replication?   
The most important criterion to decide on a topic is fea-
sibility: whether a projec t is doable for the PI’s team (plan-
ning and managing) and the partnered labs (execution). Key 
considerations include how easily the research can be ex-
ecuted across environments by all participating labs and 
whether all equipment is available across sites and whether 
necessary training has already been attained or can be pro-
vided. All parties need to carefully consider whether there 
are sufficient resourc es to coordinate such a large-scale ef-
fort, which can require substantial amounts of time-c on-
suming administrativ e work. For example, the ManyBabies 
consortium has multiple staff members dedicated exclu-
sively to non-research tasks. Basic considerations must in-
clude budget (for ethics review, study materials, personnel, 
etc.), whether research goals are attainable (e.g., study ma-
terials are clear and available), and whether the expected 
impac t is worth the large-scale investment. 
Replication targets can also be selected based on feasi-
bility in terms of the replicators’ qualifications, and the in-
tended methods and target sample. In CORE, replication 
studies are designed by undergraduate students as part of 
a one-semester course with data collection designed using 
simplified survey platforms such as Qualtrics to be collected 
online using labour markets such as Amazon Mechanical 
Turk (with CloudR esearch) and Prolific, mostly with US 
American and British participants. Theref ore, in CORE the 
selected target articles are typically the highest impac t 
classics that can be easily adjusted to an online design and 
with methods and statistics that are doable for undergrad -
uate student level. 
Isager et al. (2020)  propose another approach, based on 
a formalised model that can help guide the decision on 
choosing a topic, aiming to calculate an expected utility 
gain, which they label replication value. For this model, the 
authors consider the costs of the study, uncertaint y about 
the claim before replication, the value of the scientific 
claim, and the expected utility of the claim before the repli-
cation as determinants for the expected utility gain. How-
ever, they also state that these variables are not necessarily 
exclusive and further , unexpected variables can influenc e 
the utility gain. 
https://docs.google.c om/document/d/1OLb1VSkHLBo4z8X TEsqPUOK5PEYltbjAkpP2L yxNDSM/ 2 A Roadmap to Large-Scale Multi-C ountry Replications in Psychology
Collabra: Psychology 4Downloaded from http://online.ucpress.edu/collabra/article-pdf/8/1/57538/767427/collabra_2022_8_1_57538.pdf by guest on 04 January 2023
",A_Roadmap_to_Large-Scale_Multi-Country_Replication 3.pdf
"Different types of replications     
Direct replications3
 are usually defined as the repetition 
of a procedure, whereas conceptual replications test hy-
potheses or results of existing research using different 
methods or participant populations (Open Scienc e Collab-
oration, 2015; Schmidt, 2009). In principle, both forms of 
replications can add valuable knowledge to the literature. A 
third possibilit y combining the two types is to run a direct 
replication with a conceptual replication extension added 
in a way that would have minimal impac t on the direct 
replication yet allow for a comparison of the two types of 
replication. For example, Korbmacher et al. (2022)  ran a di-
rect replication of Kruger’ s (1999)  above and below average 
effects which manipulated difficult y in a within-subjec t de-
sign, with an added extension on top of that introducing 
a manipulation in a between-subjec t design, resulting in a 
mixed design demonstrating the strength, robustness, and 
generalizabilit y of the phenomenon. Chen et al. (2021)  re-
port a combination of two direct replications (Studies 1 and 
2) and one additional conceptual replication (Study 3 build-
ing on the same design as in Study 1) within the same arti-
cle. 
A replication and extension design is especially helpful 
in disentangling the cause for failure in finding support for 
a novel contribution, whether the failure has to do with 
the novelty added or the replicabilit y of the underlying 
phenomenon. Extensions can also add nuanc es to what is 
known, address potential weaknesses in the original design, 
or explore new directions that would advanc e the litera-
ture beyond the original’ s. For example, when Jakob et al. 
(2019)  replicated an investigation of the relationship be-
tween membership in the fictional, fraternit y-like Hogwarts 
houses from the Harry Potter franchise and psychometric 
measures of personalit y, the authors identified the concept 
of basic human values to be more in line with the descrip -
tion of these houses found in the books, and added this 
measurement to their questionnaire. 
In addition to planning these technical aspects, it is also 
helpful to take a step back and consider the implications 
of the study design and its potential outcomes. Let’s as-
sume an effect replicates in the country where the origi-
nal study was conduc ted, in a neighbouring country as well, 
but not in a third country, or across a specific set of states. 
The study would then look into not just whether an ef-
fect replicates, but also its generalisabilit y. The two compet -
ing hypotheses here would be consistency in the effect vs. 
variation. This variabilit y in specific countries can add nu-
ance and allow for comparison of populations, even with-
out adding extensions. For example, Hornse y et al. (2018) 
provide country-specific insights regarding political ideol-
ogy and scepticism about climate change, highlighting how 
these relationships were stronger in the USA compared to 
the rest of the world. In a similar fashion, other studies can reveal how country or cultural differences might mediate 
the effect of interventions, as shown for social discounting 
(Tiokhin et al., 2019). As such, multi-c ountry replications 
can be both replications and investigations into general -
isabilit y, showing either variation or robustness of a phe-
nomenon across countries, or even means of data collec-
tion (e.g. near-identical results across platforms Mturk and 
Prolific, see Brick et al., 2021; Chandrashekar et al., 2022; 
Efendić et al., 2022). 
Security and safety of collaborators and       
participants  
Sometimes, a research topic may be incredibly valuable 
and relevant to both researchers and participants, but also 
too taboo in a country to study safely. This could involve 
research eliciting people ’s opinions on sensitiv e political, 
religious, health, social/moral issues, or sexuality, and can 
reach from these topics being frowned upon to being out-
right illegal to discuss in public. Even in situations where 
governing bodies or the laws of a country are not of con-
cern, the communit y of a participant or collaborator may 
exact punishment if sensitiv e information is someho w 
made accessible. Consulting local collaborators in an infor-
mal manner (if possible) before deciding to go ahead with 
the replication is of utmost importanc e here, and if safety 
concerns are found, the best approach is to not conduc t the 
study in that particular area. The securit y of participants 
and local researchers always comes before anything else. 
Including Underrepresented Populations    
Psychological research often lacks the inclusion of his-
torically underrepresented populations (Thalmay er et al., 
2021). However, their inclusion is key when trying to in-
vestigate phenomena globally . While researchers appear to 
be generally motivated to recruit collaborators and partic-
ipants from outside more traditionally represented popu-
lations (e.g. those often summarised as western, educated, 
industrialised, rich and democratic [WEIRD], see also Ar-
nett, 2008), they face a number of obstacles that limit the 
feasibilit y and perceived benefits of doing so. Below are 
some prevalent examples of such obstacles, along with sug-
gestions for addressing them. 
Funding  
Reaching out to potential collaborators from different 
geographic areas often incurs costs that researchers are un-
prepared to bear. These may include costs for hiring and 
training local research assistants, high-qualit y translation 
of materials, travel associated with training and meeting 
collaborators, and incentivising participants. When multi-
country efforts are not funded with an explicitly cross-cul -
Note that even a “direc t” replication is not exactly identical to the original investigation. They are usually as close as possible, with ide-
ally only inevitable differences present (e.g. a new sample). Brandt et al. (2014)  argue that “close” replications would be a more fitting 
term. 3 A Roadmap to Large-Scale Multi-C ountry Replications in Psychology
Collabra: Psychology 5Downloaded from http://online.ucpress.edu/collabra/article-pdf/8/1/57538/767427/collabra_2022_8_1_57538.pdf by guest on 04 January 2023
",A_Roadmap_to_Large-Scale_Multi-Country_Replication 3.pdf
"tural approach in mind, it can be difficult for researchers 
to globalise their samples at later stages. In some cases, 
even well-funded studies designed to document cross-cul -
tural phenomena can be difficult to implement because of 
restric tions from funding agencies requiring that purchases 
be made from companies based in the home country of the 
funder . Based on discussions in our panels and some lit-
erature (Azouaghe et al., 2020; Silan et al., 2021), funding 
appears to be the largest obstacle to achieving more glob-
ally representativ e samples in multi-c ountry replications. 
Where the collaborations included here have made efforts 
to address these, they are summarised in Table 2 (When 
consulting this table, please note that the PI for JRP pro-
jects is not in the current authorship. Out of respec t for 
the personal and professional sacrific es that the individual 
made toward the work, and that there will be an additional 
publication with extensiv e detail from the PI, we provide 
only brief information here). Consider also that the use of 
expensiv e software may provide a barrier to participating in 
a collaborativ e projec t for institutions that do not have nec-
essary licenses. It is worth exploring if institutions are able 
to share licenses or provide projec t-based access, or con-
sidering open-sourc e alternativ es with bespok e edits made 
through expert collaborators. 
Data vs. collaborators from underrepresented      
groups  
Where funding is not a limiting factor, it is beneficial 
to approach cross-c ountry replications with the mindset 
of not simply obtaining underrepresented or non-WEIRD 
data, but rather involving collaborators (researchers, clini-
cians, trainees, etc.) from underrepresented groups and re-
gions in fundamental stages of the research process, ide-
ally starting with the planning phase. This approach can 
help to broaden perspec tives and mitigate potential power 
imbalanc es that may otherwise exist when lead researchers 
from high-resourc e institutions seek out collaborators from 
lower-resourc ed ones (Silan et al., 2021). 
Part of making a study valuable to any group of people is 
approaching the topic of replication from all angles, includ -
ing the local perspec tives of researchers from all collaborat -
ing regions. Trying to conduc t a replication in a vastly dif-
ferent culture than that of the original study may result in 
weaker replication findings, not due to issues in the meth-
ods, false positiv e findings, or a challenge to the central 
theory/phenomenon, but rather because that theory/phe-
nomenon works differently in a different context in which 
the replication is conduc ted. For example, while conspir -
acy beliefs have been found to mediate the link between 
political ideology and risk perception in Americans, the ef-
fects seem to be much weaker in India (Puthillam & Kapoor , 
2021). Conversely, a successful replication in a novel popu-
lation may or may not mean that the studied phenomenon 
occurs in the same way across countries‒it might also be the 
result of non-representativ e sampling. 
Involving local collaborators from the outset of the pro-
ject not only demonstrates consideration and respec t but 
also benefits the research by ensuring that design decisions 
are cross-culturally informed. One option is to establish an “outreach commit tee” within the research group or consor-
tium, that specialises in finding and contacting potential 
collaborators for specific tasks and studies. The PSA uses 
their communit y building and network expansion commit -
tee to find ways to engage with all members of the network 
(i.e. social events, conferences/seminars, slack threads, 
etc.) which in turn allows the commit tee to connec t on a 
natural basis. For conceptual replications, inspiration could 
also be taken from the STRATEQ-2 projec t (Dujols & IJz-
erman, 2021), where researchers started developing an in-
ternational questionnaire by first asking researchers from 
different countries around the globe to generate items rel-
evant to the phenomenon at hand to gather what fits the 
respec tive cultural context. 
Considering study value for participants and       
collaborators  
The ability to recruit participants and collaborators will 
likely depend on how valuable a study’ s topic is to the local 
population it aims to sample (Silan et al., 2021). When con-
sidering a study replication, researchers are advised to con-
sider (and ideally consult local collaborators about) how 
relevant the topic might be in different populations, as this 
might heavily influenc e participation rates or interest from 
potential collaborators. For instanc e, replicating a study 
about financial choices relating to health insuranc e costs 
might be of interest to US citizens but relativ ely inconse-
quential to someone in a country with universal healthcare 
provided by the state. Researchers can then either adapt the 
replication to better suit their target populations or, with 
the help of local collaborators, find ways to convey or aug-
ment the value and benefits (direct or indirec t) of the re-
search to potential participants. In the case of large stud-
ies requiring communit y investment or designed to impac t 
communities through intervention, it may be most appro-
priate to involve communit y leaders and local stakehold-
ers in ongoing conversations about both costs and benefits 
of the study. In determining adequate compensation, it is 
advisable to consult researchers and personnel with local 
knowledge, especially if a population may be vulnerable for 
economic or social reasons. 
Strengths-based vs. deficit-based framing of      
cross-population comparisons   
An important aspect of taking local perspec tives into ac-
count is to frame research questions in a way that will ad-
equately contextualise the study’ s results and allow room 
for the exploration of phenomena that may vary from those 
seen in the original study. One way of accomplishing this 
would be to use a strengths-based (as opposed to a deficit -
based) model when comparing the original population with 
those from multi-c ountry replications. Instead of viewing 
divergenc es in results as an indication of one population 
performing better at a task or better understanding  a theory 
than the other population, it would be more meaningful to 
investigate how differing skill sets, values, goals, and in-
centives play into how different populations interac t with 
the concepts being studied. As discussed in the previous A Roadmap to Large-Scale Multi-C ountry Replications in Psychology
Collabra: Psychology 6Downloaded from http://online.ucpress.edu/collabra/article-pdf/8/1/57538/767427/collabra_2022_8_1_57538.pdf by guest on 04 January 2023
",A_Roadmap_to_Large-Scale_Multi-Country_Replication 3.pdf
"section, certain concepts or relationships may not be sta-
tistically significant in some populations; take for instanc e 
conspiratorial blame and political ideology as laid out by 
Puthillam and Kapoor (2021)  – a researcher could either 
look at this phenomenon through a deficit -based lens (e.g. 
the comparison population has less of a grasp on conspir -
acy theories than the original sample, or they’re less ‘clued 
into’/exposed to international media), or they could use a 
strengths-based approach to explore why the comparison 
population is not incentiviz ed or motivated to link conspir -
atorial blame to their political ideologies. What aspects of 
their context (daily life challenges, governmental structure, 
etc.) might counterbalanc e their (potential) inclination to 
engage with certain conspiracy theories? 
Methodological Approaches   
Even for conceptual replications, the general methodol -
ogy is often at least partially based on the methodology of 
the original study. However, there are several methodologi -
cal aspects that require extra attention and adaptation due 
to the cross-cultural nature of multi-c ountry replications, 
and these come with some challenges to consider before 
embarking on such a study. Also, note that multi-c ountry 
collaborations focussing on existing research and materials 
can take various forms, including the validation of metrics 
in different countries and cultures, or a revalidation to see 
if an instrument measures the same construc t in different 
populations (see also Ruggeri et al., 2019). 
Pre-registering your project    
Pre-registration is always advisable in confirmator y re-
search (Munaf ò et al., 2017) but especially important in 
multi-c ountry replication studies, where adaptations are 
likely needed for different countries. Where possible, this 
may take the form of a registered report: a publishing 
model in which a journal first approves a study protocol and 
later the full report with results and conclusions, includ -
ing potential deviations from the original plan. While reg-
istered reports are no deus ex machina to solve issues with 
reproducibilit y, initial evidenc e supports that they promo te 
reproducibilit y, transparency , and self-correction across 
disciplines as intended (Chambers & Tzavella, 2021; Scheel 
et al., 2021) and that they are perceived as having greater 
research quality and rigour compared to the standard pub-
lishing model (Soderberg et al., 2021). However, consider 
also that publishing a registered report can impac t the 
timeline of your projec t, as review at this early stage can 
take a long time and may delay the start of your study. If 
this a concern—or where materials need to remain private 
before the conclusion of data collection—a time-stamped 
pre-registration (private or public) on platforms such as 
the OSF can ensure that any deviations from the planned 
methodology are traceable upon publication. 
Ethics  
Applying for ethical approval can be challenging when 
operating in multiple countries. In general, there are two options: a centralised approval from the PI’s Institutional 
Review Board (IRB) or approval from the PI’s institution 
and at least one IRB per country. It is also possible that in-
stitutions or journals require approval from each institu -
tion’s IRB. Having only one approval is obviously the eas-
ier approach but may not always be possible. IRBs may 
refuse such a wide-ranging approval because their members 
are not able to review materials in all languages involved 
or canno t assess data privacy matters for all countries. In 
our experienc e, contributors should at least sign a letter 
confirming their commitment to upholding procedures ex-
actly as outlined in the procedural plans submit ted to the 
IRB. Where approvals from multiple IRBs are needed, time-
lines should account for the potential of considerable de-
lays so that data collection can commenc e in all countries 
around the same time, as global events may otherwise in-
terfere with the validit y of data collected at different time 
points. In the experienc e of PSA, approvals can take differ-
ent forms and may take any time between just one week and 
nine months to be finalised, which might even lead to aban-
doning plans for a country. However, having obtained ap-
proval from one institution can often speed up the process 
as it may provide another IRB with extra confidenc e where 
other colleagues have already approved it. This should also 
be considered when making the choice of a private pre-reg -
istration versus registered report, as a journal may require 
all ethical approvals before consideration, which can cause 
delays in the projec t timeline. Further , you can find your-
self in a situation where one IRB requires a change in ap-
proach (e.g., dropping a culturally sensitiv e question). This 
impac t will then have to be reconsidered from a method -
ological perspec tive. 
In addition to these administrativ e aspects, there are 
specific ethical considerations to keep in mind when plan-
ning research in multiple countries. Primarily , this means 
discussing cultural aspects with the local collaborators in 
detail and also involving local stakeholders for their ap-
proval. Especially when the research requires intense time 
commitment, the consortium should consider what to pro-
vide in return. For example, linguists commonly create dic-
tionaries based on their work and share them with partic-
ipants. Further , the age of consent for participation may 
differ between countries and IRBs may require a different 
age minimum, an aspect which makes sense to be checked 
with local collaborators early. 
Translation and adaptation of materials      
Another time-c onsuming aspect of multi-c ountry stud-
ies is the translation of study materials. It may take mul-
tiple iterations and checks to confirm that materials are 
not only translated, but also that the translated text is per-
ceived as intended in the base version of the materials. 
The forward- and back-translation protocol (Brislin, 1970) 
seems to be the most commonly utilised method in psy-
chological research and requires at least one bilingual, as 
well as additional native speakers of the target language. 
Ideally , the forward- and back-translation should be com-
plemented by methods that can test the conceptual (i.e., 
construc t has a comparable meaning in all countries/cul -A Roadmap to Large-Scale Multi-C ountry Replications in Psychology
Collabra: Psychology 7Downloaded from http://online.ucpress.edu/collabra/article-pdf/8/1/57538/767427/collabra_2022_8_1_57538.pdf by guest on 04 January 2023
",A_Roadmap_to_Large-Scale_Multi-Country_Replication 3.pdf
"Table 2. Actions regarding funding and including under-represented populations by different consortia so far.             
JRP PSA Man yBabies CORE ISM 
Funding Studies  are conducted  online; PI 
pro vides access to surv ey; 
centr alised budget is pro vided to 
pay data collection services if 
not all data can be obtained b y 
collabor ators; collabor ators 
work on their own time Funding  has  been  limited  but 
does come up with a few of 
our projects for populations 
that are underrepresented 
or for tr anslators. Surv eys 
are typically run with an 
online link and funding for 
participants are most lik ely 
funded b y the lab ’s funds or 
resources. Project funding varies across the MB 
initiativ es, but se veral ha ve receiv ed 
funding through scientific agencies or 
through internal gr ants from a PI’s home 
institution. F unds ha ve been a warded to 
promote large collabor ativ e projects and 
support capacity-building b y supplying 
necessary tr aining and equipment ( e.g., 
laptops, speak ers, software, ethical 
review , and a tr aining workshop in the 
case of MB1- A, for labs that had not 
pre viously run infant looking-time 
studies). In man y cases, howe ver, 
collabor ating labs opt in to projects that 
are consistent with e xisting gr ants and 
methods in the lab, and do not require 
additional funding. Funding  has  been  very limited 
so far . Single PI used his own 
seed and internal funding to 
conduct all data collections 
online using online labour 
mark ets. Surv ey-based  online 
research; PI 
pro vides access to 
surv ey; funds are 
available to offset 
data collection 
costs and 
participant 
incentiv es 
Collabor ators 
from 
underrepresented 
groups Existing network is e xplored for 
trusted colleagues from as man y 
countries as possible; the more 
the network grows, the more 
grows the outreach and 
colleagues from more countries 
can be included (First study: 32 
collabor ators and data from 19 
countries; latest: 169 authors 
from 61 countries included) The PSA is a Distributed 
Labor atory Network which 
means we ha ve and recruit 
collabor ators from an y and 
all Geopolitical regions. Our 
network continues to grow 
and at the moment we ha ve 
over 1300 researchers 
across 83 geopolitical 
regions.   Any researcher or lab can join an MB 
project; across the 16 MB studies to date, 
there are 490 collabor ators from 45 
countries on 6 continents, including 191 
in the Americas, 230 in Europe, 21 in 
Africa, 34 in Asia, and 15 in Oceania. The student body has been 
based at Hong K ong, 
extending to collabor ations 
with early career researchers 
from around the world.  Researchers and 
participants from 
57 countries, 
including 7 African 
countries, 10 
American countries, 
18 Asian countries, 
21 European 
countries, and 
Austr alia. 
Value for 
participants and 
collabor ators Participants: Surv eys are short 
(5 minutes or less), minimal time 
investment – results are often 
shared via the same channels 
participants were recruited; 
Collabor ators: Access to 
network of collabor ators ( option 
of further connection), 
opportunity to re-use data for 
additional research ( e.g. country 
specific sub-analyses, or 
connecting data to new research 
question); for ECR – tr aining 
(adaptation of materials, work 
with Qualtrics, data collection) Participants: V alue for 
participants varies for each 
project. V alue can be 
monetary , knowledge based, 
and credit. P articipants are 
also shared results through 
ways in which are 
understandable and public.  
Collabor ators: The PSA 
allows for an y career le vel to 
join and get in volved in an 
aspect of the study to gain 
authorship, e xperience, and 
new skill sets. Collabor ators 
also gain connections and The families of infant participants are 
often compensated financially or giv en 
small gifts ( e.g., books, to ys) for 
participation. 
Collabor ators benefit from tr aining in 
experimental and analytic methods, open 
access to materials and data, authorship 
on publications, funding to attend 
professional conferences, and de veloping 
relationships with a large international 
network of colleagues. For replication and e xtension 
projects early career 
researchers are in vited to 
join as lead authors o ver 
projects started with 
students in course and thesis 
work. 
For the de veloped 
collabor ativ e resources, an y 
meaningful contribution is 
acknowledged and lik ely to 
lead to coauthorship. Participants: V alue 
for participants 
could be monetary 
or knowledge-
based. 
Collabor ators: 
Access to network 
of collabor ators in 
the same area of 
research, 
opportunity to re-
use data for 
additional research 
(e.g. country specific 
sub-analyses, or A Roadmap to Large-Scale Multi-C ountry Replications in Psychology
Collabra: Psychology 8Downloaded from http://online.ucpress.edu/collabra/article-pdf/8/1/57538/767427/collabra_2022_8_1_57538.pdf by guest on 04 January 2023
",A_Roadmap_to_Large-Scale_Multi-Country_Replication 3.pdf
"and support from country team 
members in conducting the 
study access to researchers across 
the world.  
Other people: The PSA uses 
open material pr actices and 
therefore allows e veryone 
to ha ve access to our 
research methods and data. 
Value to the field is shown 
by contributing large and 
gener alizable data sets for 
studies.  connecting data to 
new research 
question), co-
authorship on all 
subsequent papers 
emerging from the 
main dataset. 
Strengths-based 
approach JRP encour ages country-specific 
analyses, e xamples can be found 
in the appendix of Ruggeri et al. 
(2020) N/A By in volving scientists from a div erse 
range of research sites in all project 
stages, MB pro vides a platform for 
perspectiv es that are often 
underrepresented within scientific 
discourse. MB projects ha ve gener ally 
interpreted differential performance 
across participants and populations as 
evidence for the man y paths that typical 
cognitiv e de velopment ma y tak e. N/A ISM encour ages 
country-le vel 
analyses and 
conte xtualisation of 
meta-norms in 
contempor ary times 
(e.g., a follow-up 
study on CO VID-19 
related meta-
norms) A Roadmap to Large-Scale Multi-C ountry Replications in Psychology
Collabra: Psychology 9Downloaded from http://online.ucpress.edu/collabra/article-pdf/8/1/57538/767427/collabra_2022_8_1_57538.pdf by guest on 04 January 2023
",A_Roadmap_to_Large-Scale_Multi-Country_Replication 3.pdf
"tures), instrument (i.e., construc t is operationaliz ed in a 
similar way across countries/cultures), item (i.e., construc t 
can be measured with the same instrument across coun-
tries/cultures), and scalar (i.e., construc t can be assessed on 
the same metric) equivalenc e of an instrument to meaning -
fully compare different cultures/c ountries (Hui & Triandis, 
1985). 
Despite the prevalenc e of the forward- and back-trans -
lation method, evidenc e that this method yields the best 
results is lacking (Epstein et al., 2015). While some re-
searchers consider back-translation indispensable, others 
argue that it does not necessarily guarantee equivalenc e 
and linguistic appropriateness of an instrument (Behr, 
2017) and theref ore advise against it (Epstein et al., 2015). 
In addition, there is no standardised method of translating 
research material in psychology (Cha et al., 2007; Epstein 
et al., 2015). This poses an issue as the back-translation 
process requires several people with different backgrounds/
expertise levels, making it difficult to achieve a truly valid 
translation where the target language is very rare or in re-
gions where psychological research may not have a large 
presenc e. In these cases, it may be worth considering utilis-
ing a combination of translation techniques, depending on 
the resourc es and personnel available. 
The PSA uses a variation of back-translation as its of-
ficial method and also appoints language coordinators for 
each study. 
The PSA method uses two translators, and goes as fol-
lows: 
When following the back-translation method, focusing 
on the feedback of non-academic speakers of the target lan-
guage is crucial, as they are ensuring that materials are 
jargon-free, make practical/social (not just grammatical) 
sense in their cultural contexts, and are easy to follow for 
everyone. To put it bluntly: When your colleague tells you 
your questionnaire is fine, but your grandma tells you it 
does not make sense, your grandma is right.4
 Sometimes 
such feedback may require changes to very fundamental 
parts of the materials, such as items in a scale that do not 
apply or make sense in a particular culture. Another issue 
to consider is adapting materials to local dialects or ver-
sions rather than leaving them in the ‘standard’ version of 
the language (such as Austrian German vs. Standard High 
German) - an extra measure to ensure cultural validit y. Whether such an adaptation is adequate should be evalu-
ated by a local contributor . Additionally , in countries with 
multiple major languages where people may be fluent in 
more than one, discussions to decide which languages the 
materials will be translated to are helpful. 
Given the culturally dependent and, at times, subjec tive 
nature of translation, transparency in the process is para-
mount. One of the easiest ways to maintain transparency 
is to keep track of changes made to materials through the 
translation process, and comment on the rationale behind 
those changes. This can be done in an informal way at first 
(such as commenting on a Word document or using the 
Track Changes feature), and later refined into an easily di-
gestible format (such as a table with every major change to 
each material; see Supplementar y materials in Ruggeri et 
al., 2020, 2021). 
Especially when replicating older studies, adaptations 
to the base materials are sometimes needed. For example, 
studies including financial decisions need to be adapted to 
the local currency , but also to inflation, and anchored to in-
come levels of the country (for an example, see the appen -
dix of Ruggeri et al., 2020). Similarly , the Zeitgeist might 
make adaptation of materials necessary, as encountered in 
Wagenmak ers and colleagues’ (2016)  attempt at replicat -
ing experiments underlying the facial feedback hypothe-
sis, where the pictures used in the original study to evoke 
laughs were deemed not funny anymore. 
Data Collection   
The process of collecting and storing data from multiple 
countries differs across consortia, with some collecting data 
centrally (i.e., the PI saves data from multiple countries 
into their own database) and others collecting data through 
individual researchers or labs (i.e., collaborators store the 
data they collected in their countries, and then pass it on 
to the PIs). Both, national data protection laws and ethics 
commit tees, determine some aspects of how data is sup-
posed to be collected and stored. While these determinants 
provide guidanc e on establishing a data collection protocol, 
the multitude of regulations also limits how data can be 
collected and shared and may require differences in how 
data is collected and stored from country to country, which 
can cause methodological inconsistencies. 
Data  Privacy   
Data privacy should be considered in parallel to ethical 
issues before applying to an IRB. IRB applications can be 
complicated when multiple countries with different data 
protection laws are involved. It is advisable to think about 
data protection from a general ethical perspec tive first and 
try to adhere to this set of standards throughout the pro-
ject, even if the law in some countries may not require it. In 
most aspects, the EU’s General Data Protection Regulation 1. First bilingual translator translates from original lan-
guage into target language. 
2. Second bilingual translator back-translates the first 
translator’ s work into the original language. 
3. The two versions are discussed, and a third version is 
made with changes. 
4. External non-academic readers fluent in target lan-
guage read the third version and give feedback. 
5. Final version is created based on cultural feedback 
from external readers. 
The first author’ s grandmo ther agrees with this assessment. 4 A Roadmap to Large-Scale Multi-C ountry Replications in Psychology
Collabra: Psychology 10Downloaded from http://online.ucpress.edu/collabra/article-pdf/8/1/57538/767427/collabra_2022_8_1_57538.pdf by guest on 04 January 2023
",A_Roadmap_to_Large-Scale_Multi-Country_Replication 3.pdf
"(GDPR) appears to be the strictest law in that sense. For 
those coming in contact with the GDPR for the first time, 
it is important to know that it only applies if personal data 
are collected, but is not applicable if individuals are not di-
rectly identifiable from the answers in a research endeav our 
(i.e. the data is pseudonymised, see Recital 26). When col-
lecting data online, researchers should pay special atten-
tion not to collect IP addresses or geolocation of respon -
dents (as these may make participants identifiable). GDPR 
also usually does not apply to data collection in research 
contexts (Article 89), so the applicabilit y should be checked 
in advanc e – where possible with the help of a legal ad-
visor or data privacy expert, such as the institution’ s Data 
Protection Officer. As GDPR is sometimes poorly understood 
and interpreted differently by IRBs, both outside and inside 
Europe, it is advisable to plan additional time to clarify 
these issues. Independent of which data laws apply, collect-
ing non-personal and anonymised data is always the least 
likely to cause problems and avoids having to add compli-
cated and long notices that can scare people off. If personal 
identifiable data are involved, researchers may also need 
to set up Data Sharing Agreements between the consortium 
partners (these regulate how the collected data are shared 
between partners and processed), or even Data Privacy Im-
pact Assessments (a process to identify and minimise data 
protection risks which is necessary when processing sensi-
tive data on a large scale). As of 2022, this also largely still 
applies to the UK, as the Data Protection Act 2018 is the UK’s 
adaptation of GDPR. Note that, depending on the categor y 
of data concerned, countries will often have specific laws 
pertaining solely to those categories (e.g., HIPAA or FERPA 
in the US). Based on GDPR categories and the authors’ ex-
perienc es, the following categories of data always warrant 
special attention to details and protection and will there-
fore be more complicated to handle: 
Statistical Approach   
If you ask multiple data analysts what analysis to choose 
for your research projec t, chances are high you will find 
yourself with many different replies (Menkv eld et al., 2021; 
Silberzahn et al., 2018). However, there are multiple aspects 
that should always be considered when choosing the sta-
tistical approach to a multi-c ountry replication. For direct 
replications, studies should be powered to detect an effect 
at least equal to the one found in the original study, ideally 
even smaller – if there is no existing cross-cultural com-
parison, it is possible that the effect you are investigating 
may be smaller in some of the target countries. More gen-erally, due to the combination of publication bias and small 
sample sizes, reported effect sizes in the literature tend to 
be inflated (Gelman & Carlin, 2014; Lane & Dunlap, 1978). 
Due to this “winner’ s curse”, it is advisable to power repli-
cations to a smaller effect size than the one reported in 
the literature (e.g., using the small telescopes approach; Si-
monsohn, 2015). Where resourc es are constrained, sequen -
tial analyses can also be considered: This approach, com-
monly used in medical trials (where stakes for participants 
are high), uses interim analyses to observe if a sufficient 
sample has been reached while controlling for Type-1 error 
rate (Lakens, 2014). 
In addition, researchers should take into consideration 
whether it is advantageous and feasible to power the study 
for individual regions or groups. One option is to replicate 
the effect in one country only before opting for a multi-
country replication. That is specifically useful when the 
original evidenc e may be old and conduc ted on small sam-
ples. It is also an option to repeat the original analysis (of-
ten frequentist) and additionally analyse the data with an 
equivalent Bayesian analysis. In any case, the statistical ap-
proach(es) should be part of the pre-registration, and any 
additional, explorator y analyses should be labelled as such. 
To estimate the necessary sample size per country it should 
be considered whether the approach allows for a multi-le vel 
structure. If so, the number of countries and the variation 
of the effect at that level should be taken into considera -
tion. 
For conceptual replications, assessing sample size is 
more complicated. Assump tions on effect sizes should be 
as conservative as possible, yet might yield unrealistically 
large minimum sample sizes. Lakens (2021)  provides de-
tailed discussions on different approaches and highlights 
that, depending on the justification for a sample size, it 
should be considered “1) what the smallest effect size of in-
terest is, 2) which minimal effect size will be statistically 
significant, 3) which effect sizes they expect (and what they 
base these expectations on), 4) which effect sizes would be 
rejected based on a confidenc e interval around the effect 
size, 5) which ranges of effects a study has sufficient power 
to detect based on a sensitivit y power analysis, and 6) 
which effect sizes are plausible in a specific research area” 
(p. 1). When powering your study, you should also consider 
the practical implications of the effect size you are aiming 
for: while an extremely small but significant effect may pro-
vide insight, its practical relevance may be nil (Götz et al., 
2021; see also reply by Primbs et al., 2021). While we would 
clearly not advise against research for the sole purpose of 
gaining knowledge in general, practical relevance is surely a 
key consideration for such large-scale endeav ours where re-
sources could be used to investigate potentially more ben-
eficial matters. Anvari and colleagues (2021)  provide an in-
sightful discussion on this matter. 
Reporting and Disseminating Findings     
Once the study is completed, it is time for the exciting 
part of sharing the results with the world. The unique ad-
vantage in communicating results of a multi-c ountry repli-
cation lies in the opportunit y to 1) share results in every • Racial or ethnic origin 
• Health or genetic data 
• Financial data 
• Philosophical, political, or religious beliefs 
• Sexual orientation 
• Trade union membership 
• Educational data / IQ scores 
• Data from underaged individuals, especially children 
• Data from closed networks, where the combination of 
answers may reveal identities A Roadmap to Large-Scale Multi-C ountry Replications in Psychology
Collabra: Psychology 11Downloaded from http://online.ucpress.edu/collabra/article-pdf/8/1/57538/767427/collabra_2022_8_1_57538.pdf by guest on 04 January 2023
",A_Roadmap_to_Large-Scale_Multi-Country_Replication 3.pdf
"author’ s native language and 2) highlight country-specific 
findings that might fall short of attention in the shadow of 
the overall findings of the paper. It can be advantageous 
to put a dissemination plan in writing early, to avoid scat-
tered communication around results. All collaborators may 
inform their universities and institutions or faculty, who of-
ten provide the opportunit y to write a blog post or news ar-
ticle. Sharing results before peer review has become fairly 
common but also raises concerns about research being 
shared uncritically , as most people may not be able to dif-
ferentiate between peer-reviewed research and preprints. 
This concern was confirmed by Wingen et al. (2022) , who 
experimentally showed that a brief explanation can help 
clarify this issue. As such, if you opt for a preprint publica -
tion, we recommend both clearly marking your preprint as 
such, as well as adding the explanation developed by Win-
gen and colleagues. In addition, it is good practice to ensure 
that all materials are accessible. Collecting information and 
conduc ting studies is costly – it is important to make this 
data available to achieve the maximum benefit of your work 
and resourc es, so that others may answer additional re-
search questions. The PSA has even incentivised secondary 
analysis, by challenging researchers to work with one of 
their large datasets, offering monetar y rewards (Forscher et 
al., 2019). 
Concluding Remarks   
Large-scale multi-c ountry replications are not the most 
straightf orward or easiest research endeav ours. Yet they 
come with large benefits such as comparable data from dif-
ferent countries, and datasets which—if curated well—can 
be the source of future insights. While there is clearly no 
one-siz e-fits-all approach, we hope that the lessons we 
learned and summarised in this paper will be helpful at all 
steps of planning future multi-c ountry replications. How-
ever, when approaching your multi-c ountry replication, 
keep in mind to plan the protocol and responsibilities of 
collaborators well in advanc e and listen closely to your col-
laborators’ insights of their own countries, identify poten-
tial pitfalls, and make sure everyone’s safety is guaranteed. 
Likewise, share your knowledge about your own region or 
country. Your study also does not need to solve every ques-
tion there is – a simple effect or theory is much more real-
istic to test at scale and makes it easier to provide tangible 
insights from the observations. Likewise, extensions should 
be equally straightf orward. 
Working on a large scale with colleagues from many 
countries can be a challenging but enriching experienc e 
and provides collaborators with research expertise and in-
sights into how scientists operate in other parts of the 
world. These differences allow for additional perspec tives 
and a more holistic view of phenomena, but they also re-
quire clear guidelines on communication channels and re-
sponsibilities (ideally provided in a way that is easily ac-
cessible to all collaborators. ) This includes using 
communication tools that are accessible to every partner 
in the consortium. Simple and prevalent communication is 
key to the success of every projec t emplo ying Big Team Sci-
ence. However, it should also be considered that commu -nication tools may not always take into account their us-
ability for researchers with disabilities. In order to ensure 
that colleagues with disabilities can contribute without ad-
ditional hurdles, such factors should be explored before the 
start of a projec t and not be made the responsibilit y of po-
tential contributors. 
Lastly, it is our impression that despite the differences 
in approaches, all large-scale multi-c ountry endeav ours we 
have been part of have one thing in common: the motiva-
tion to conduc t such studies mostly stems from a drive to 
produc e solid research that can help impro ve people ’s lives. 
Multi-lab approaches may facilitate a mindset -shift in how 
research is conduc ted, where instead of operating in si-
los and potentially competing for publication spots in jour-
nals, a collaborativ e approach allowing for different per-
spectives within one research projec t allows for not only 
more produc tive discussions where everyone has the same 
goal, but also leads to more nuanc ed insights. Including 
students in such initiativ es can further help to support de-
velopments towards more collaborations of this kind and 
provide valuable early research experienc e and network op-
portunities. On the other hand, journals will also have to 
consider how to best provide impartial reviews as experts 
for such collaborations will become more and more con-
nected to each other and it will become increasingly diffi-
cult to find editors who are knowledgeable in multi-c oun-
try replications but without ties to author consortia that 
often include more than a hundred researchers. While we 
hope that our insights and checklist are helpful in con-
ducting your multi-c ountry replication, we also encourage 
researchers to build meaningful and lasting relationships 
with their projec t partners, moving towards a methodolog -
ically sound, more collaborativ e, and inclusiv e field. We 
would also like to encourage researchers who have created, 
or know of, additional materials for approaching this topic 
to get in touch and add resourc es or links to the accompa-
nying online repositor y (https://osf .io/xrv5p/). 
Acknowledgements  
The authors representing JRP would like to thank Kai 
Ruggeri for his leadership and extensiv e work in the pro-
jects described and for providing this opportunit y to early 
career researchers. 
Contributions  
HJ, SAV, and SJG have conceptualised and organised the 
panel discussions that form the basis of this paper. SA, LB, 
AC, GF, EGG, HK, SL, BV, and JZ were speakers at these pan-
els (moderated by HJ and SAV) and contributed their in-
sights. ALT and TLA compiled notes at these sessions that 
informed the writing of this manuscrip t. HJ structured the 
insights gained and led the writing of the first draft, sup-
ported by SAV and SJG. HJ supervised the administrativ e as-
pects of this projec t. All authors contributed to the writing 
and revision of this manuscrip t. SAV is overseeing the up-
dating of the accompanying OSF repositor y. A Roadmap to Large-Scale Multi-C ountry Replications in Psychology
Collabra: Psychology 12Downloaded from http://online.ucpress.edu/collabra/article-pdf/8/1/57538/767427/collabra_2022_8_1_57538.pdf by guest on 04 January 2023
",A_Roadmap_to_Large-Scale_Multi-Country_Replication 3.pdf
"Funding Information   
No specific funding has been received for this work. 
Competing Interests   
The authors declare no competing interests. Submit ted: August 26, 2022 PST, Accepted: November 28, 2022 
PST 
This is an open-access article distributed under the terms of the Creativ e Commons Attribution 4.0 International License 
(CCBY -4.0 ). View this license ’s legal deed at http:/ /creativ ecommons.org/licenses/b y/4.0 and legal code at http:/ /creativ ecom -
mons.org/licenses/b y/4.0/legalcode for more information. A Roadmap to Large-Scale Multi-C ountry Replications in Psychology
Collabra: Psychology 13Downloaded from http://online.ucpress.edu/collabra/article-pdf/8/1/57538/767427/collabra_2022_8_1_57538.pdf by guest on 04 January 2023
",A_Roadmap_to_Large-Scale_Multi-Country_Replication 3.pdf
"References  
Allen, L., O’Connell, A., & Kiermer , V. (2019). How can 
we ensure visibilit y and diversity in research 
contributions? How the Contributor Role Taxonomy 
(CRediT) is helping the shift from authorship to 
contributorship. Learned Publishing, 32(1), 71–74. htt
ps://doi.org/10.1002/leap.1210 
Anand-V embar , S. (chair), Alzahawi, S., Carstensen, A., 
Garcia-Gar zon, E., Kapoor , H., Lewis, S., Geiger , S. J., 
& Jarke, H. (2021, August 22). Approaches to Multi-
country Replications in Psychology and Behavioural 
Sciences – Session Two (of Two) [Panel discussion]. 
10th Junior Researcher Programme Conference, 
Online, UK. https://y outu.be/lGFi2Dp_Hp4 
Anvari, F., Kievit, R., Lakens, D., Pennington, C. R., 
Przybylski, A. K., Tiokhin, L., Wiernik, B. M., & 
Orben, A. (2021). Evaluating the practical relevance 
and significance of observed effect sizes in psychological 
research. https://doi.org/10.31234/osf .io/g3v tr 
Anvari, F., & Lakens, D. (2018). The replicabilit y crisis 
and public trust in psychological scienc e. 
Comprehensive Results in Social Psychology, 3(3), 
266–286. https://doi.org/10.1080/23743603.2019.168
4822 
Anvari, F., Olsen, J., Hung, W., & Feldman, G. (2021). 
Mispredic tion of affective outcomes due to different 
evaluation modes: Replication and extension of two 
distinc tion bias experiments by Hsee and Zhang 
(2004). Journal of Experimental  Social Psychology, 92, 
104052. https://doi.org/10.1016/j.jesp.2020.104052 
Arnett, J. J. (2008). The neglec ted 95%: Why American 
psychology needs to become less American. American 
Psychologist, 63(7), 602–614. https://doi.org/10.1037/
0003-066x.63.7 .602 
Azouaghe, S., Adetula, A., Forscher , P. S., Basnight -
Brown, D., Ouherrou, N., Charyate, A., & IJzerman, 
H. (2020). Psychology and open science in Africa: Why 
is it needed and how can we implement it? https://doi.o
rg/10.31730/osf .io/ke7ub 
Behr, D. (2017). Assessing the use of back translation: 
The shortc omings of back translation as a quality 
testing method. International  Journal of Social 
Research Methodology , 20(6), 573–584. https://doi.or
g/10.1080/13645579.2016.1252188 
Beshears, J., Gjoneska, B., Schmidt, K., Pfuhl, G., Saari, 
T., McAuliff e, W. H. B., Steltenpohl, C. N., Onie, S., 
Chartier , C. R., & Moshont z, H. (2020). Psychological 
Science Accelerator: A promising resource for clinical 
psychological science. https://doi.org/10.31234/osf .io/q
wy4k 
Brandt, M. J., IJzerman, H., Dijksterhuis, A., Farach, F. 
J., Geller , J., Giner-Sorolla, R., Grange, J. A., Perugini, 
M., Spies, J. R., & van ’t Veer, A. (2014). The 
Replication Recipe: What makes for a convincing 
replication? Journal of Experimental  Social Psychology, 
50(1), 217–224. https://doi.org/10.1016/j.jesp.2013.1
0.005 Brick, C., Fillon, A., Yeung, S., Wang, M., Lyu, H., Ho, J., 
Wong, S., & Feldman, G. (2021). Self-interest is 
overestimated: Two successful pre-registered 
replications of Miller and Ratner (1998). Collabra: 
Psychology, 7(1), 23443. https://doi.org/10.1525/c ollab
ra.23443 
Brislin, R. W. (1970). Back-translation for cross-cultural 
research. Journal of Cross-Cultural  Psychology, 1(3), 
185–216. https://doi.org/10.1177/1359104570001003
01 
Byers-Heinlein, K., Bergmann, C., Davies, C., Frank, M. 
C., Hamlin, J. K., Kline, M., Kominsky , J. F., Kosie, J. 
E., Lew-Williams, C., Liu, L., Mastroberardino, M., 
Singh, L., Waddell, C. P. G., Zettersten, M., & 
Soderstrom, M. (2020). Building a collaborativ e 
psychological scienc e: Lessons learned from 
ManyBabies 1. Canadian Psychology / Psychologie 
canadienne, 61(4), 349–363. https://doi.org/10.1037/c
ap0000216 
Byers-Heinlein, K., Tsui, A. S. M., Bergmann, C., Black, 
A. K., Brown, A., Carbajal, M. J., Durrant, S., Fennell, 
C. T., Fiévet, A.-C., Frank, M. C., Gampe, A., Gervain, 
J., Gonzale z-Gome z, N., Hamlin, J. K., Havron, N., 
Hernik, M., Kerr, S., Killam, H., Klassen, K., … 
Wermelinger , S. (2021). A multi-lab study of bilingual 
infants: Exploring the preference for infant -directed 
speech. Advances in Methods and Practices in 
Psychological Science, 4(1), 1–30. https://doi.org/10.11
77/2515245920974622 
Cha, E. S., Kim, K. H., & Erlen, J. A. (2007). Translation 
of scales in cross-cultural research: Issues and 
techniques. Journal of Advanced Nursing, 58(4), 
386–395. https://doi.org/10.1111/j.1365-2648.2007 .04
242.x 
Chambers, C. D., & Tzavella, L. (2021). The past, 
present and future of Registered Reports. Nature 
Human Behaviour, 6(1), 29–42. https://doi.org/10.103
8/s41562-021-01193-7 
Chandrashekar , S., Adelina, N., Zeng, S., Chiu, Y., 
Leung, Y., Henne, P., Cheng, B., & Feldman, G. 
(2022). Defaults versus framing: Revisiting Default 
Effect and Framing Effect with replications and 
extensions of Johnson and Goldstein (2003) and 
Johnson, Bellman, and Lohse (2002). Pre-print. http
s://osf .io/krsf2/ 
Chandrashekar , S., Yeung, S., Yau, K., Cheung, C., 
Agarwal, T. K., Wong, C., Pillai, T., Thirlw ell, T. N., 
Leung, W., Li, Y., Tse, C., Cheng, B., Chan, H., & 
Feldman, G. (2021). Agency and self-other 
asymmetries in perceived bias and shortc omings: 
Replications of the Bias Blind Spot and extensions 
linking to free will beliefs. Judgment and Decision 
Making, 16(6), 1392–1413. A Roadmap to Large-Scale Multi-C ountry Replications in Psychology
Collabra: Psychology 14Downloaded from http://online.ucpress.edu/collabra/article-pdf/8/1/57538/767427/collabra_2022_8_1_57538.pdf by guest on 04 January 2023
",A_Roadmap_to_Large-Scale_Multi-Country_Replication 3.pdf
"Chen, J., Kwan, L. C., Ma, L. Y., Choi, H. Y., Lo, Y. C., Au, 
S. Y., Tsang, C. H., Cheng, B. L., & Feldman, G. 
(2021). Retrospec tive and prospec tive hindsight bias: 
Replications and extensions of Fischhoff (1975) and 
Slovic and Fischhoff (1977). Journal of Experimental  
Social Psychology, 96, 104154. https://doi.org/10.1016/
j.jesp.2021.104154 
Collaborativ e Open-scienc e REsearch. (2022). 
Replications and extensions of classic findings in Social 
Psychology and Judgment and Decision Making. htt
p://osf .io/5z4a8 
Dujols, O., & IJzerman, H. (2021, Januar y 8). Creating a 
psychological tool applicable to individuals around the 
world (STRAEQ-2 item generation and selection phase 
[Blog post]. https://c orelab.blog/creating-a-psy cholog
ical-tool-applicable-to-individuals-around-the-w orl
d-straeq-2-items-generation-and-selec tion-phas
e/%26sa=D%26sourc e=docs%26ust=164495447044437
1%26usg=A OvVaw09P qgKtM_lPV84ML T2NHl3 
Ebersole, C. R., Klein, R. A., & Atherton, O. E. (2014). 
The Many Lab. Online Project Repositor y. https://osf .i
o/89vqh/ 
Efendić, E., Chandrashekar , S., Cheong, S., Yeung, L., 
Kim, M., Lee, C., … Feldman, G. (2022). Risky 
theref ore not beneficial: Replication and extension of 
Finucane et al. (2000)’ s Affect Heuristic experiment. 
Social Psychological and Personality Science. https://do
i.org/10.1177/19485506211056761 
Epstein, J., Osborne, R. H., Elsworth, G. R., Beaton, D. 
E., & Guillemin, F. (2015). Cross-cultural adaptation 
of the Health Education Impac t Questionnaire: 
Experimental study showed expert commit tee, not 
back-translation, added value. Journal of Clinical 
Epidemiology , 68(4), 360–369. https://doi.org/10.101
6/j.jclinepi.2013.07 .013 
Eriksson, K., Strimling, P., Gelfand, M., Wu, J., 
Abernathy , J., Akotia, C. S., Aldashe v, A., Andersson, 
P. A., Andrighet to, G., Anum, A., Arikan, G., Aycan, 
Z., Bagherian, F., Barrera, D., Basnight -Brown, D., 
Batkeyev, B., Belaus, A., Berezina, E., Björnstjerna, 
M., … Van Lange, P. A. M. (2021). Perceptions of the 
appropriate response to norm violation in 57 
societies. Nature Communications, 12(1), 1481. http
s://doi.org/10.1038/s41467 -021-21602-9 
Forscher , P. S., DeBruine, L. M., Jones, B. C., Flake, J. K., 
Coles, N. A., & Chartier , C. R. (2019, September 1). 
Introducing the PSA001 Secondary Analysis Challenge. 
https://psysciac c.org/2019/09/01/introducing-the-psa
001-sec ondary-analysis-challenge/ 
Forscher , P. S., Wagenmak ers, E.-J., Coles, N. A., Silan, 
M. A. A., Dutra, N. B., Basnight -Brown, D., & 
IJzerman, H. (2020). The Benefits, Barriers, and Risks 
of Big Team Science. Preprint. https://doi.org/10.3123
4/osf.io/2mdxh 
Gelman, A., & Carlin, J. (2014). Beyond power 
calculations: Assessing type S (sign) and type M 
(magnitude) errors. Perspectives on Psychological 
Science, 9(6), 641–651. https://doi.org/10.1177/17456
91614551642 Götz, F. M., Gosling, S. D., & Rentfro w, P. J. (2021). 
Small effects: The indispensable foundation for a 
cumulativ e psychological scienc e. Perspectives on 
Psychological Science, 17(1), 205–215. https://doi.org/
10.1177/1745691620984483 
Grahe, J., Brandt, M., Wagge, J., Legate, N., Wiggins, B., 
Christopherson, C., Weisberg, Y., Corker, K., Chartier , 
C., Fallon, M., Hildebrandt, L., Hurst, M., Lazare vic, 
L., Levitan, C., McFall, J., McLaughlin, H., Pazda, A., 
IJzerman, H., Nosek, B., … Adetula, A. (2013). 
Collaborativ e Replications and Education Project 
(CREP). Open Science Framework. https://doi.org/10.1
7605/OSF.IO/WFC6U 
Holcombe, A. O., Kovacs, M., Aust, F., & Aczel, B. 
(2020). Documenting contributions to scholarly 
articles using CRediT and tenzing. PLoS ONE, 15(12), 
1–11. https://doi.org/10.1371/journal.pone.0244611 
Hornse y, M. J., Harris, E. A., & Fielding, K. S. (2018). 
Relationships among conspiratorial beliefs, 
conservatism and climate scepticism across nations. 
Nature Climate Change, 8(7), 614–620. https://doi.or
g/10.1038/s41558-018-0157 -2 
Hui, C. H., & Triandis, H. C. (1985). Measurement in 
cross-cultural psychology: A review and comparison 
of strategies. Journal of Cross-Cultural  Psychology, 
16(2), 131–152. https://doi.org/10.1177/00220021850
16002001 
Isager , P. M., van Aert, R. C. M., Bahník, Š., Brandt, M. 
J., DeSoto, K. A., Giner-Sorolla, R., Krueger , J., 
Perugini, M., Ropovik, I., van ’t Veer, A. E., Vranka, 
M. A., & Lakens, D. (2020). Deciding what to replicate: 
A decision model for replication study selection under 
resource and knowledge constraints. Preprint. https://d
oi.org/10.31222/osf .io/2gur z 
Jakob, L., Garcia-Gar zon, E., Jarke, H., & Dablander , F. 
(2019). The Scienc e Behind the Magic? The Relation 
of the Harry Potter “Sorting Hat Quiz” to Personalit y 
and Human Values. Collabra: Psychology, 5(1), 31. htt
ps://doi.org/10.1525/c ollabra.240D 
Jarke, H. (2021, June 24). The Junior Researcher 
Programme - An initiativ e providing opportunities to 
early career researchers in psychology [Lightning 
Talk]. Annual Conference of the Society for the 
Improvement of Psychological Science (SIPS) 2021, 
Online. https://osf .io/s82qp/ 
Jarke, H. (chair), Bojanić, L., Feldman, G., Većkalo v, B., 
Zickfeld, J., Geiger , S. J., & Anand-V embar , S. (2021, 
August 22). Approaches to Multi-country Replications 
in Psychology and Behavioural Sciences – Session One 
(of Two) [Panel discussion]. 10th Junior Researcher 
Programme Conference, Online, UK. https://y outu.b
e/CHlgMCFTj9c 
Kahneman, D., & Tversky, A. (1979). Prospec t Theor y: 
An Analysis of Decision under Risk. Econometrica: 
Journal of the Econometric Society, 47(3), 263–291. htt
ps://doi.org/10.1111/j.1536-7150.2011.00774.x A Roadmap to Large-Scale Multi-C ountry Replications in Psychology
Collabra: Psychology 15Downloaded from http://online.ucpress.edu/collabra/article-pdf/8/1/57538/767427/collabra_2022_8_1_57538.pdf by guest on 04 January 2023
",A_Roadmap_to_Large-Scale_Multi-Country_Replication 3.pdf
"Klein, R. A., Ratliff , K. A., Vianello, M., Adams, R. B., 
Jr., Bahník, Š., Bernstein, M. J., Bocian, K., Brandt, M. 
J., Brooks, B., Brumbaugh, C. C., Cemalcilar , Z., 
Chandler , J., Cheong, W., Davis, W. E., Devos, T., 
Eisner , M., Frankowska, N., Furrow, D., Galliani, E. 
M., … Nosek, B. A. (2014). Investigating variation in 
replicabilit y: A “many labs” replication projec t. Social 
Psychology, 45(3), 142–152. https://doi.org/10.1027/1
864-9335/a000178 
Korbmacher , M., Kwan, C., & Feldman, G. (2022). Both 
better and worse than others depending on difficulty: 
Replication and extensions of Kruger’s (1999) above and 
below average effects. https://osf .io/7yfkc/ 
Kruger , J. (1999). Lake Wobegon be gone! The “below-
average effect” and the egocentric nature of 
comparativ e ability judgments. Journal of Personality 
and Social Psychology, 77(2), 221–232. https://doi.org/
10.1037/0022-3514.77 .2.221 
Lakens, D. (2014). Refinements and New Developments: 
Performing High-P owered Studies Efficiently With 
Sequential Analyses. European Journal of Social 
Psychology, 44(7), 701–710. https://doi.org/10.1002/ej
sp.2023 
Lakens, D. (2021). Sample Size Justification. https://doi.o
rg/10.31234/osf .io/9d3yf 
Lane, D. M., & Dunlap, W. P. (1978). Estimating effect 
size: Bias resulting from the significanc e criterion in 
editorial decisions. British Journal of Mathematical 
and Statistical Psychology, 31(2), 107–112. https://do
i.org/10.1111/j.2044-8317 .1978.tb00578.x 
ManyBabies Consortium. (2020). Quantifying sources of 
variabilit y in infancy research using the infant -
directed speech preference. Advances in Methods and 
Practices in Psychological Science, 3(1), 24–52. http
s://doi.org/10.1177/2515245919900809 
Menkv eld, A. J., Dreber , A., Holzmeister , F., Huber , J., 
Johanneson, M., Kirchler , M., Razen, M., Weitzel, U., 
Abad, D., Abudy , M. (Meni), Adrian, T., Ait-Sahalia, 
Y., Akmanso y, O., Alcock, J., Alexeev, V., Aloosh, A., 
Amato, L., Amaya, D., Angel, J. J., … Bao, L. (2021). 
Non-Standard Errors. SSRN Electronic Journal. http
s://doi.org/10.2139/ssrn.3961574 
Moshont z, H., Campbell, L., Ebersole, C. R., Ijzerman, 
H., Urry, H. L., Forscher , P. S., Grahe, J. E., McCarthy , 
R. J., Musser , E. D., Antfolk, J., Castille, C. M., Evans, 
T. R., Fiedler , S., Flake, J. K., Forero, D. A., Janssen, S. 
M. J., Keene, J. R., Protzko, J., Aczel, B., … Chartier , C. 
R. (2018). The psychological scienc e accelerator: 
Advancing psychology through a distributed 
collaborativ e network. Advances in Methods and 
Practices in Psychological Science, 1(4), 501–515. http
s://doi.org/10.1177/2515245918797607 
Munaf ò, M. R., Nosek, B. A., Bishop, D. V. M., Button, K. 
S., Chambers, C. D., Percie Du Sert, N., Simonsohn, 
U., Wagenmak ers, E.-J., Ware, J. J., & Ioannidis, J. P. 
A. (2017). A manifesto for reproducible scienc e. 
Nature Human Behaviour, 1(1), 1–9. https://doi.org/1
0.1038/s41562-016-0021 
Open Scienc e Collaboration. (2015). Estimating the 
reproducibilit y of psychological scienc e. Science, 
349(6251). https://doi.org/10.1126/scienc e.aac4716 Pashler , H., & Wagenmak ers, E. (2012). Editors’ 
introduc tion to the special section on replicabilit y in 
psychological scienc e: A crisis of confidenc e? 
Perspectives on Psychological Science, 7(6), 528–530. ht
tps://doi.org/10.1177/1745691612465253 
Primbs, M., Pennington, C. R., Lakens, D., Silan, M. A. 
A., Lieck, D. S. N., Forscher , P. S., Buchanan, E. M., & 
Westwood, S. J. (2021). Are Small Effects the 
Indispensable  Foundation for a Cumulative 
Psychological Science? A Reply to Götz et al. (2022). htt
ps://doi.org/10.31234/osf .io/6s8bj 
Psychological Scienc e Accelerator Self-Determination 
Theor y Collaboration, Legate, N., Nguyen, T., 
Weinstein, N., Moller , A., Legault, L., Vally, Z., 
Tajchman, Z., Zsido, A. N., Zrimsek, M., Chen, Z., 
Ziano, I., Gialitaki, Z., Ceary, C. D., Jang, Y., Lin, Y., 
Kunisato, Y., Yamada, Y., Xiao, Q., … Primbs, M. A. 
(2022). A global experiment on motivating social 
distancing during the COVID-19 pandemic. 
Proceedings of the National Academy of Sciences, 
119(22), 2111091119. https://doi.org/10.1073/pnas.21
11091119 
Puthillam, A., & Kapoor , H. (2021). Does conspiratorial  
blame mediate the relationship between political 
ideology and risk perception? Evidence from India and 
the US. https://doi.org/10.31234/osf .io/frgz q 
Ruggeri, K. (2020, May 19). The collaboration behind the 
replication [Blog post]. https://socialscienc es.nature.c
om/posts/the-c ollaboration-behind-the-replication 
Ruggeri, K., Alí, S., Berge, M. L., Bertoldo, G., Bjørndal, 
L. D., Cortijos-Bernabeu, A., Davison, C., Demić, E., 
Esteban-Serna, C., Friedemann, M., Gibson, S. P., 
Jarke, H., Karakashe va, R., Khorrami, P. R., Kveder, J., 
Andersen, T. L., Lofthus, I. S., McGill, L., Nieto, A. E., 
… Folke, T. (2020). Replicating patterns of Prospec t 
Theor y for decision under risk. Nature Human 
Behaviour, 4(6), 622–633. https://doi.org/10.1038/s41
562-020-0886- x 
Ruggeri, K., Bojanić, L., van Bokhorst, L., Jarke, H., 
Mareva, S., Ojinaga-Alfageme, O., Mellor , D. T., & 
Norton, S. (2019). Editorial: Advancing methods for 
psychological assessment across borders. Frontiers in 
Psychology, 10, 503. https://doi.org/10.3389/fpsy g.201
9.00503 
Ruggeri, K., Panin, A., Vdovic, M., Većkalo v, B., Abdul-
Salaam, N., Achterberg, J., Akil, C., Amatya, J., 
Amatya, K., Andersen, T. L., Aquino, S. D., 
Arunasalam, A., Ashcrof t-Jones, S., Askelund, A. D., 
Ayacaxli, N., Sheshdeh, A. B., Bailey, A., Barea 
Arroyo, P., Mejía, G. B., & García-Gar zon, E. (2022). 
The globalizabilit y of temporal discounting. Nature 
Human Behaviour, 6. https://doi.org/10.1038/s4156
2-022-01392- w 
Ruggeri, K., Većkalo v, B., Bojanić, L., Andersen, T. L., 
Ashcrof t-Jones, S., Ayacaxli, N., Barea-Arro yo, P., 
Berge, M. L., Bjørndal, L. D., Bursalıoğlu, A., Bühler , 
V., Čadek, M., Çetinçelik, M., Clay, G., Cortijos-
Bernabeu, A., Damnjano vić, K., Dugue, T. M., Esberg, 
M., Esteban-Serna, C., … Folke, T. (2021). The 
general fault in our fault lines. Nature Human 
Behaviour, 5(10), 1369–1380. https://doi.org/10.1038/
s41562-021-01092- x A Roadmap to Large-Scale Multi-C ountry Replications in Psychology
Collabra: Psychology 16Downloaded from http://online.ucpress.edu/collabra/article-pdf/8/1/57538/767427/collabra_2022_8_1_57538.pdf by guest on 04 January 2023
",A_Roadmap_to_Large-Scale_Multi-Country_Replication 3.pdf
"Scheel, A. M., Schijen, M. R. M. J., & Lakens, D. (2021). 
An excess of positiv e results: Comparing the standard 
Psychology literature with Registered Reports. 
Advances in Methods and Practices in Psychological 
Science, 4(2), 251524592110074. https://doi.org/10.11
77/25152459211007467 
Schmidt, S. (2009). Shall We Really Do It Again? The 
Powerful Concept of Replication Is Neglec ted in the 
Social Scienc es. Review of General Psychology, 13(2), 
90–100. https://doi.org/10.1037/a0015108 
Silan, M., Adetula, A., Basnight -Brown, D. M., Forscher , 
P. S., Dutra, N., & IJzerman, H. (2021, October 26). 
Psychological Scienc e Needs the Entire Globe, Part 2 
Let’s Talk About the “C” Word: Colonialism and the 
Challenges of Psychological Scienc e in the 
Developing World [Blog Post]. Aps Observer. https://w
ww.psychologicalscienc e.org/obser ver/psy chological-
scienc e-needs-the-entire-globe-part -2 
Silberzahn, R., Uhlmann, E. L., Martin, D. P., Anselmi, 
P., Aust, F., Awtrey, E., Bahník, Š., Bai, F., Bannard, 
C., Bonnier , E., Carlsson, R., Cheung, F., Christensen, 
G., Clay, R., Craig, M. A., Rosa, A. D., Dam, L., Evans, 
M. H., Cervantes, I. F., … Nosek, B. A. (2018). Many 
analysts, one data set: Making transparent how 
variations in analytic choices affect results. Advances 
in Methods and Practices in Psychological Science, 1(3), 
337–356. https://doi.org/10.1177/2515245917747646 
Simonsohn, U. (2015). Small telescopes: Detectabilit y 
and the evaluation of replication results. 
Psychological Science, 26(5), 559–569. https://doi.org/
10.1177/0956797614567341 
Soderberg, C. K., Errington, T. M., Schiav one, S. R., 
Bottesini, J., Thorn, F. S., Vazire, S., Esterling, K. M., 
& Nosek, B. A. (2021). Initial evidenc e of research 
quality of registered reports compared with the 
standard publishing model. Nature Human Behaviour, 
5(8), 990–997 . https://doi.org/10.1038/s41562-021-01
142-4 
Thalmay er, A. G., Toscanelli, C., & Arnett, J. J. (2021). 
The neglec ted 95% revisited: Is American psychology 
becoming less American? American Psychologist, 
76(1), 116–129. https://doi.org/10.1037/amp0000622 
Tiokhin, L., Hackman, J., Munira, S., Jesmin, K., & 
Hruschka, D. (2019). Generalizabilit y is not optional: 
Insights from a cross-cultural study of social 
discounting. Royal Society Open Science, 6(2), 181386. 
https://doi.org/10.1098/rsos.181386 Tsui, A. S. M., Carstensen, A., Kachergis, G., Abubakar , 
A., Asnak e, M., Barry, O., Basnight -Brown, D., Bentu, 
D., Bergmann, C., Binan Dami, E., Boll-A vetisyan, N., 
de Jongh, M., Diop, Y., Herrmann, E., Jang, C., Kizito, 
S., Lamba, T., Maliwichi-Senganimalunje, L., 
Marangu, J., … Frank, M. C. (2021). Exploring 
variation in infants’ preference for infant-directed 
speech: Evidence from a multi-site study in Africa. 
Stage 1 RR accepted at Developmental Scienc e. 
Uhlmann, E. L., Ebersole, C. R., Chartier , C. R., 
Errington, T. M., Kidwell, M. C., Lai, C. K., McCarthy , 
R. J., Riegelman, A., Silberzahn, R., & Nosek, B. A. 
(2019). Scientific utopia III: Crowdsourcing scienc e. 
Perspectives on Psychological Science, 14(5), 711–733. 
https://doi.org/10.1177/1745691619850561 
Wagenmak ers, E.-J., Beek, T., Dijkhoff , L., Gronau, Q. F., 
Acosta, A., Adams, R. B., Jr., Albohn, D. N., Allard, E. 
S., Benning, S. D., Blouin-Hudon, E.-M., Bulnes, L. C., 
Caldw ell, T. L., Calin-Jageman, R. J., Capaldi, C. A., 
Carfagno, N. S., Chasten, K. T., Cleeremans, A., 
Connell, L., DeCicco, J. M., … Zwaan, R. A. (2016). 
Registered Replication Report. Perspectives on 
Psychological Science, 11(6), 917–928. https://doi.org/
10.1177/1745691616674458 
Wiggins, B. J., & Christopherson, C. D. (2019). The 
replication crisis in psychology: An overview for 
theoretical and philosophical psychology . Journal of 
Theoretical and Philosophical Psychology, 39(4), 
202–217 . https://doi.org/10.1037/teo0000137 
Wingen, T., Berkessel, J. B., & Dohle, S. (2022). Caution, 
preprint! Brief explanations allow non-scientists to 
differentiate between preprints and peer-reviewed 
journal articles. Advances in Methods and Practices in 
Psychological Science, 5(1), 251524592110705. http
s://doi.org/10.1177/25152459211070559 
Wingen, T., Berkessel, J. B., & Englich, B. (2020). No 
replication, no trust? How low replicabilit y influenc es 
trust in psychology . Social Psychological and 
Personality Science, 11(4), 454–463. https://doi.org/1
0.1177/1948550619877412 
Ziano, I., Xiao, Q., Yeung, S. K., Wong, C. Y., Cheung, M. 
Y., Lo, C. Y. J., Yan, H. C., Narendra, G. I., Kwan, L. 
W., Chow, C. S., Man, C. Y., & Feldman, G. (2021). 
Numbing or Sensitization? Replications and 
Extensions of Fetherstonhaugh et al. (1997)’ s 
“Insensitivit y to the Value of Human Life.” Journal of 
Experimental  Social Psychology, 97, 104222. https://do
i.org/10.1016/j.jesp.2021.104222 A Roadmap to Large-Scale Multi-C ountry Replications in Psychology
Collabra: Psychology 17Downloaded from http://online.ucpress.edu/collabra/article-pdf/8/1/57538/767427/collabra_2022_8_1_57538.pdf by guest on 04 January 2023
",A_Roadmap_to_Large-Scale_Multi-Country_Replication 3.pdf
"Supplementar y Materials   
Peer Review History    
Download: https:/ /collabr a.scholasticahq.com/article/57538-a-roadmap-to-large-scale-multi-country-replications-in-
psy chology /attachment/123028.docx?auth_tok en=K vOiqA6SMDF rubX eqHxa 
Appendix 1   
Download: https:/ /collabr a.scholasticahq.com/article/57538-a-roadmap-to-large-scale-multi-country-replications-in-
psy chology /attachment/123030.docx?auth_tok en=K vOiqA6SMDF rubX eqHxa 
Appendix 2   
Download: https:/ /collabr a.scholasticahq.com/article/57538-a-roadmap-to-large-scale-multi-country-replications-in-
psy chology /attachment/123193.docx?auth_tok en=K vOiqA6SMDF rubX eqHxa A Roadmap to Large-Scale Multi-C ountry Replications in Psychology
Collabra: Psychology 18Downloaded from http://online.ucpress.edu/collabra/article-pdf/8/1/57538/767427/collabra_2022_8_1_57538.pdf by guest on 04 January 2023
",A_Roadmap_to_Large-Scale_Multi-Country_Replication 3.pdf
